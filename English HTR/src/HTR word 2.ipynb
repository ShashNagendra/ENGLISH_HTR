{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5bwwk4uxRz6A"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T06:19:48.106082Z",
     "iopub.status.busy": "2022-12-14T06:19:48.105426Z",
     "iopub.status.idle": "2022-12-14T06:19:50.981603Z",
     "shell.execute_reply": "2022-12-14T06:19:50.980658Z"
    },
    "id": "2R1hQGtZEi8Y"
   },
   "outputs": [],
   "source": [
    "# !pip uninstall -y tensorflow estimator keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T06:19:50.986279Z",
     "iopub.status.busy": "2022-12-14T06:19:50.985537Z",
     "iopub.status.idle": "2022-12-14T06:20:13.341457Z",
     "shell.execute_reply": "2022-12-14T06:20:13.340544Z"
    },
    "id": "5Xbt8BkPv8Ou"
   },
   "outputs": [],
   "source": [
    "# !pip install -U tensorflow_text tensorflow tensorflow_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T06:20:13.346822Z",
     "iopub.status.busy": "2022-12-14T06:20:13.346150Z",
     "iopub.status.idle": "2022-12-14T06:20:15.164209Z",
     "shell.execute_reply": "2022-12-14T06:20:15.163198Z"
    },
    "id": "7TGZmOuqMia9"
   },
   "outputs": [],
   "source": [
    "# !pip install einops tensorflow_hub tensorflow_text tensorflow_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-10 15:09:19.931490: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-02-10 15:09:20.019929: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-02-10 15:09:20.020094: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-02-10 15:09:20.021565: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-02-10 15:09:20.022601: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-02-10 15:09:20.022743: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-02-10 15:09:20.022868: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-02-10 15:09:20.762236: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-02-10 15:09:20.762637: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-02-10 15:09:20.762765: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-02-10 15:09:20.763058: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 9357 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2080 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  try:\n",
    "    # Currently, memory growth needs to be the same across GPUs\n",
    "    for gpu in gpus:\n",
    "      tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    logical_gpus = tf.config.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "  except RuntimeError as e:\n",
    "    # Memory growth must be set before GPUs have been initialized\n",
    "    print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "cellView": "form",
    "execution": {
     "iopub.execute_input": "2022-12-14T06:20:15.169125Z",
     "iopub.status.busy": "2022-12-14T06:20:15.168416Z",
     "iopub.status.idle": "2022-12-14T06:20:17.929094Z",
     "shell.execute_reply": "2022-12-14T06:20:17.928420Z"
    },
    "id": "U8l4RJ0XRPEm"
   },
   "outputs": [],
   "source": [
    "#@title\n",
    "import concurrent.futures\n",
    "import collections\n",
    "import dataclasses\n",
    "import hashlib\n",
    "import itertools\n",
    "import json\n",
    "import math\n",
    "import os\n",
    "import pathlib\n",
    "import random\n",
    "import re\n",
    "import string\n",
    "import time\n",
    "import urllib.request\n",
    "\n",
    "import einops\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import requests\n",
    "import tqdm\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text as text\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "\n",
    "size = dict()\n",
    "steps = dict()\n",
    "dataset = dict()\n",
    "path=\"/home/shashank/Desktop/new trials/CBAM with spatial (copy)/data/iam.hdf5\"\n",
    "with h5py.File(path, \"r\") as f:\n",
    "    for pt in ['train','test','valid']:\n",
    "        dataset[pt] = dict()\n",
    "        dataset[pt]['dt'] = np.stack([f[pt]['dt'], f[pt]['dt'], f[pt]['dt']], axis=3)\n",
    "        dataset[pt]['gt'] = np.array(f[pt]['gt'])\n",
    "\n",
    "        size[pt] = len(dataset[pt]['gt'])\n",
    "        steps[pt] = int(np.ceil(size[pt] / 16))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([b'A MOVE to stop Mr. Gaitskell from',\n",
       "       b'nominating any more Labour life Peers',\n",
       "       b'is to be made at a meeting of Labour', ...,\n",
       "       b\"about it . ' And Philip said : ' But we 've got\",\n",
       "       b\"to think about it , don't you see , because\",\n",
       "       b\"if we don't it 'll just go on and on , don't\"], dtype=object)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train']['gt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6161, 1024, 128, 3)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train']['dt'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-10 15:09:34.571005: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 2422603776 exceeds 10% of free system memory.\n",
      "2023-02-10 15:09:36.012201: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 2422603776 exceeds 10% of free system memory.\n"
     ]
    }
   ],
   "source": [
    "train_raw= tf.data.Dataset.from_tensor_slices((dataset['train']['dt'], dataset['train']['gt']))\n",
    "test_raw= tf.data.Dataset.from_tensor_slices((dataset['test']['dt'], dataset['test']['gt']))\n",
    "valid_raw= tf.data.Dataset.from_tensor_slices((dataset['valid']['dt'], dataset['valid']['gt']))\n",
    "# ds=dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds=train_raw\n",
    "test_ds=test_raw\n",
    "valid_ds=valid_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T06:21:26.360704Z",
     "iopub.status.busy": "2022-12-14T06:21:26.360422Z",
     "iopub.status.idle": "2022-12-14T06:21:26.366963Z",
     "shell.execute_reply": "2022-12-14T06:21:26.366327Z"
    },
    "id": "sAQSps5F8RQI"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorSpec(shape=(1024, 128, 3), dtype=tf.uint8, name=None),\n",
       " TensorSpec(shape=(), dtype=tf.string, name=None))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds.element_spec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8cSW4u-ORPFQ"
   },
   "source": [
    "### Image feature extractor\n",
    "\n",
    "You will use an image model (pretrained on imagenet) to extract the features from each image. The model was trained as an image classifier, but setting `include_top=False` returns the model without the final classification layer, so you can use the last layer of feature-maps:  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dojkiou9gL3R"
   },
   "source": [
    "Here's a function to load an image and resize it for the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not 224. Weights for input shape (224, 224) will be loaded as the default.\n"
     ]
    }
   ],
   "source": [
    "IMAGE_SHAPE=(1024, 128, 3)\n",
    "mobilenet = tf.keras.applications.MobileNetV3Small(\n",
    "    input_shape=IMAGE_SHAPE,\n",
    "    include_top=False,\n",
    "    include_preprocessing=False)\n",
    "mobilenet.trainable=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T06:21:26.370769Z",
     "iopub.status.busy": "2022-12-14T06:21:26.370105Z",
     "iopub.status.idle": "2022-12-14T06:21:27.881918Z",
     "shell.execute_reply": "2022-12-14T06:21:27.881104Z"
    },
    "id": "xIa0ZaP4tBez"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-10 15:09:40.791589: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:428] Loaded cuDNN version 8400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 32, 4, 576)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-10 15:09:41.769004: W tensorflow/compiler/xla/stream_executor/gpu/asm_compiler.cc:115] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "You may not need to update to CUDA 11.1; cherry-picking the ptxas binary is often sufficient.\n"
     ]
    }
   ],
   "source": [
    "for img, label in test_ds.take(1):\n",
    "    print(mobilenet(tf.expand_dims(img, axis=0)).shape)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nyqH3zFwRPFi"
   },
   "source": [
    "### Setup the text tokenizer/vectorizer\n",
    "\n",
    "You will transform the text captions into integer sequences using the [TextVectorization](https://www.tensorflow.org/api_docs/python/tf/keras/layers/TextVectorization) layer, with the following steps:\n",
    "\n",
    "* Use [adapt](https://www.tensorflow.org/api_docs/python/tf/keras/layers/TextVectorization#adapt) to iterate over all captions, split the captions into words, and compute a vocabulary of the top words.\n",
    "* Tokenize all captions by mapping each word to its index in the vocabulary. All output sequences will be padded to length 50.\n",
    "* Create word-to-index and index-to-word mappings to display results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T06:21:30.655182Z",
     "iopub.status.busy": "2022-12-14T06:21:30.654479Z",
     "iopub.status.idle": "2022-12-14T06:21:30.658979Z",
     "shell.execute_reply": "2022-12-14T06:21:30.658156Z"
    },
    "id": "NroZIzB90hD3"
   },
   "outputs": [],
   "source": [
    "def standardize(s):\n",
    "  #s = tf.strings.lower(s)\n",
    "  #s = tf.strings.regex_replace(s, f'[{re.escape(string.punctuation)}]', '')\n",
    "  s = tf.strings.join(['[START]', s, '[END]'], separator=' ')\n",
    "  return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "95"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "x=string.printable[:95]\n",
    "len(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~ \n",
      "char level Tokenizer\n",
      "[[27], [28], [29], [30], [31], [32], [33], [34], [35], [36], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [37], [38], [39], [40], [41], [42], [43], [44], [45], [46], [47], [48], [49], [50], [51], [52], [53], [54], [55], [56], [57], [58], [59], [60], [61], [62], [63], [64], [65], [66], [67], [68], [69]]\n",
      "['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', ':', ';', '<', '=', '>', '?', '@', '[', '\\\\', ']', '^', '_', '`', '{', '|', '}', '~', ' ']\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "text_data=string.printable[:95]\n",
    "print(text_data)\n",
    "\n",
    "print(\"char level Tokenizer\")\n",
    "\n",
    "char_tok = Tokenizer(char_level=True)\n",
    "char_tok.fit_on_texts(text_data)\n",
    "char_sequences = char_tok.texts_to_sequences(text_data)\n",
    "print(char_sequences)\n",
    "\n",
    "char_reconstructed_text = char_tok.sequences_to_texts(char_sequences)\n",
    "print(char_reconstructed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[59], [2], [43], [1], [69], [13], [15], [22], [5], [69], [20], [15], [69], [19], [20], [15], [16], [69], [13], [18], [50], [69], [7], [1], [9], [20], [19], [11], [5], [12], [12], [69], [6], [18], [15], [13], [43], [], [69], [2], [43], [14], [15], [13], [9], [14], [1], [20], [9], [14], [7], [69], [1], [14], [25], [69], [13], [15], [18], [5], [69], [12], [1], [2], [15], [21], [18], [69], [12], [9], [6], [5], [69], [16], [5], [5], [18], [19], [43], [], [69], [2], [43], [9], [19], [69], [20], [15], [69], [2], [5], [69], [13], [1], [4], [5], [69], [1], [20], [69], [1], [69], [13], [5], [5], [20], [9], [14], [7], [69], [15], [6], [69], [12], [1], [2], [15], [21], [18], [43], [69], [50], [50], [50], [], [69], [2], [38], [1], [2], [15], [21], [20], [69], [9], [20], [69], [50], [69], [43], [69], [1], [14], [4], [69], [16], [8], [9], [12], [9], [16], [69], [19], [1], [9], [4], [69], [52], [69], [43], [69], [2], [21], [20], [69], [23], [5], [69], [43], [22], [5], [69], [7], [15], [20], [38], [], [69], [2], [38], [20], [15], [69], [20], [8], [9], [14], [11], [69], [1], [2], [15], [21], [20], [69], [9], [20], [69], [48], [69], [4], [15], [14], [43], [20], [69], [25], [15], [21], [69], [19], [5], [5], [69], [48], [69], [2], [5], [3], [1], [21], [19], [5], [38], [], [69], [2], [38], [9], [6], [69], [23], [5], [69], [4], [15], [14], [43], [20], [69], [9], [20], [69], [43], [12], [12], [69], [10], [21], [19], [20], [69], [7], [15], [69], [15], [14], [69], [1], [14], [4], [69], [15], [14], [69], [48], [69], [4], [15], [14], [43], [20], [38], [61]]\n",
      "['[', 'b', \"'\", 'a', ' ', 'm', 'o', 'v', 'e', ' ', 't', 'o', ' ', 's', 't', 'o', 'p', ' ', 'm', 'r', '.', ' ', 'g', 'a', 'i', 't', 's', 'k', 'e', 'l', 'l', ' ', 'f', 'r', 'o', 'm', \"'\", '', ' ', 'b', \"'\", 'n', 'o', 'm', 'i', 'n', 'a', 't', 'i', 'n', 'g', ' ', 'a', 'n', 'y', ' ', 'm', 'o', 'r', 'e', ' ', 'l', 'a', 'b', 'o', 'u', 'r', ' ', 'l', 'i', 'f', 'e', ' ', 'p', 'e', 'e', 'r', 's', \"'\", '', ' ', 'b', \"'\", 'i', 's', ' ', 't', 'o', ' ', 'b', 'e', ' ', 'm', 'a', 'd', 'e', ' ', 'a', 't', ' ', 'a', ' ', 'm', 'e', 'e', 't', 'i', 'n', 'g', ' ', 'o', 'f', ' ', 'l', 'a', 'b', 'o', 'u', 'r', \"'\", ' ', '.', '.', '.', '', ' ', 'b', '\"', 'a', 'b', 'o', 'u', 't', ' ', 'i', 't', ' ', '.', ' ', \"'\", ' ', 'a', 'n', 'd', ' ', 'p', 'h', 'i', 'l', 'i', 'p', ' ', 's', 'a', 'i', 'd', ' ', ':', ' ', \"'\", ' ', 'b', 'u', 't', ' ', 'w', 'e', ' ', \"'\", 'v', 'e', ' ', 'g', 'o', 't', '\"', '', ' ', 'b', '\"', 't', 'o', ' ', 't', 'h', 'i', 'n', 'k', ' ', 'a', 'b', 'o', 'u', 't', ' ', 'i', 't', ' ', ',', ' ', 'd', 'o', 'n', \"'\", 't', ' ', 'y', 'o', 'u', ' ', 's', 'e', 'e', ' ', ',', ' ', 'b', 'e', 'c', 'a', 'u', 's', 'e', '\"', '', ' ', 'b', '\"', 'i', 'f', ' ', 'w', 'e', ' ', 'd', 'o', 'n', \"'\", 't', ' ', 'i', 't', ' ', \"'\", 'l', 'l', ' ', 'j', 'u', 's', 't', ' ', 'g', 'o', ' ', 'o', 'n', ' ', 'a', 'n', 'd', ' ', 'o', 'n', ' ', ',', ' ', 'd', 'o', 'n', \"'\", 't', '\"', ']']\n"
     ]
    }
   ],
   "source": [
    "#char_tok.fit_on_texts(\"hello\")\n",
    "char_sequences = char_tok.texts_to_sequences(str(labels))\n",
    "print(char_sequences)\n",
    "\n",
    "print(char_tok.sequences_to_texts(char_sequences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T06:21:30.662209Z",
     "iopub.status.busy": "2022-12-14T06:21:30.661976Z",
     "iopub.status.idle": "2022-12-14T06:21:30.674010Z",
     "shell.execute_reply": "2022-12-14T06:21:30.673239Z"
    },
    "id": "n9SQOXFsyS36"
   },
   "outputs": [],
   "source": [
    "# # Use the top 5000 words for a vocabulary.\n",
    "# from tf.keras.preprocessing.text import Tokenizer\n",
    "# vocabulary_size = 5000\n",
    "# tokenizer = Tokenizer(\n",
    "#     max_tokens=vocabulary_size,\n",
    "#     standardize=standardize,\n",
    "#     output_sequence_length=30,\n",
    "#     pad_to_max_tokens= True\n",
    "# #     ragged=True\n",
    "# )\n",
    "# # Learn the vocabulary from the caption data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Use the top 5000 words for a vocabulary.\n",
    "# from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "# #vocabulary_size = 98\n",
    "# tokenizer = Tokenizer(\n",
    "#     num_words=98, char_level=True\n",
    "# )\n",
    "# # Learn the vocabulary from the caption data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [29]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241m.\u001b[39mfit_on_texts(labels)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "tokenizer.fit_on_texts(labels) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels=dataset['train']['gt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([b'A MOVE to stop Mr. Gaitskell from',\n",
       "       b'nominating any more Labour life Peers',\n",
       "       b'is to be made at a meeting of Labour', ...,\n",
       "       b\"about it . ' And Philip said : ' But we 've got\",\n",
       "       b\"to think about it , don't you see , because\",\n",
       "       b\"if we don't it 'll just go on and on , don't\"], dtype=object)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels= np.concatenate(dataset['train']['gt'], dataset['test']['gt'])\n",
    "# labels= np.concatenate(labels, dataset['valid']['gt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6161,)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T06:21:30.677454Z",
     "iopub.status.busy": "2022-12-14T06:21:30.676721Z",
     "iopub.status.idle": "2022-12-14T06:21:32.338147Z",
     "shell.execute_reply": "2022-12-14T06:21:32.337359Z"
    },
    "id": "oJGE34aiRPFo"
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Tokenizer' object has no attribute 'adapt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [24]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# tokenizer.adapt(train_raw.map(lambda fp,txt: txt).unbatch().batch(1024))\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madapt\u001b[49m(labels)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Tokenizer' object has no attribute 'adapt'"
     ]
    }
   ],
   "source": [
    "# tokenizer.adapt(train_raw.map(lambda fp,txt: txt).unbatch().batch(1024))\n",
    "tokenizer.adapt(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T06:21:32.342623Z",
     "iopub.status.busy": "2022-12-14T06:21:32.341980Z",
     "iopub.status.idle": "2022-12-14T06:21:32.354940Z",
     "shell.execute_reply": "2022-12-14T06:21:32.354326Z"
    },
    "id": "oRahTDtWhJIf"
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Tokenizer' object has no attribute 'get_vocabulary'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [36]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_vocabulary\u001b[49m()[:\u001b[38;5;241m10\u001b[39m]\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Tokenizer' object has no attribute 'get_vocabulary'"
     ]
    }
   ],
   "source": [
    "tokenizer.get_vocabulary()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T06:21:32.358115Z",
     "iopub.status.busy": "2022-12-14T06:21:32.357629Z",
     "iopub.status.idle": "2022-12-14T06:21:32.395154Z",
     "shell.execute_reply": "2022-12-14T06:21:32.394516Z"
    },
    "id": "-2mGxD33JCxN"
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'Tokenizer' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [29]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m t \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43ma cat in a hat\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43ma robot dog\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m t\n",
      "\u001b[0;31mTypeError\u001b[0m: 'Tokenizer' object is not callable"
     ]
    }
   ],
   "source": [
    "t = tokenizer([['a cat in a hat'], ['a robot dog']])\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T06:21:32.398431Z",
     "iopub.status.busy": "2022-12-14T06:21:32.398157Z",
     "iopub.status.idle": "2022-12-14T06:21:32.477652Z",
     "shell.execute_reply": "2022-12-14T06:21:32.477042Z"
    },
    "id": "8Q44tNQVRPFt"
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Tokenizer' object has no attribute 'get_vocabulary'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [32]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Create mappings for words to indices and indices to words.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m word_to_index \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mStringLookup(\n\u001b[1;32m      3\u001b[0m     mask_token\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m----> 4\u001b[0m     vocabulary\u001b[38;5;241m=\u001b[39m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_vocabulary\u001b[49m())\n\u001b[1;32m      5\u001b[0m index_to_word \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mStringLookup(\n\u001b[1;32m      6\u001b[0m     mask_token\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      7\u001b[0m     vocabulary\u001b[38;5;241m=\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39mget_vocabulary(),\n\u001b[1;32m      8\u001b[0m     invert\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Tokenizer' object has no attribute 'get_vocabulary'"
     ]
    }
   ],
   "source": [
    "# Create mappings for words to indices and indices to words.\n",
    "word_to_index = tf.keras.layers.StringLookup(\n",
    "    mask_token=\"\",\n",
    "    vocabulary=tokenizer.get_vocabulary())\n",
    "index_to_word = tf.keras.layers.StringLookup(\n",
    "    mask_token=\"\",\n",
    "    vocabulary=tokenizer.get_vocabulary(),\n",
    "    invert=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T06:21:32.481676Z",
     "iopub.status.busy": "2022-12-14T06:21:32.480953Z",
     "iopub.status.idle": "2022-12-14T06:21:32.491390Z",
     "shell.execute_reply": "2022-12-14T06:21:32.490750Z"
    },
    "id": "qo-cfCX3LnHs"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'index_to_word' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [31]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m w \u001b[38;5;241m=\u001b[39m \u001b[43mindex_to_word\u001b[49m(t)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'index_to_word' is not defined"
     ]
    }
   ],
   "source": [
    "w = index_to_word(t)\n",
    "# w.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T06:21:32.494835Z",
     "iopub.status.busy": "2022-12-14T06:21:32.494362Z",
     "iopub.status.idle": "2022-12-14T06:21:32.539069Z",
     "shell.execute_reply": "2022-12-14T06:21:32.538476Z"
    },
    "id": "rrUUfGc65vAT"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'w' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [30]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m tf\u001b[38;5;241m.\u001b[39mstrings\u001b[38;5;241m.\u001b[39mreduce_join(\u001b[43mw\u001b[49m, separator\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mnumpy()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'w' is not defined"
     ]
    }
   ],
   "source": [
    "tf.strings.reduce_join(w, separator=' ', axis=-1).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uEWM9xrYcg45"
   },
   "source": [
    "### Prepare the datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6aX0Z_98S2tN"
   },
   "source": [
    "The `train_raw` and `test_raw` datasets contain 1:many `(image, captions)` pairs. \n",
    "\n",
    "This function will replicate the image so there are 1:1 images to captions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T06:21:32.549819Z",
     "iopub.status.busy": "2022-12-14T06:21:32.549230Z",
     "iopub.status.idle": "2022-12-14T06:21:33.950287Z",
     "shell.execute_reply": "2022-12-14T06:21:33.949562Z"
    },
    "id": "CZGUsuGzUfzt"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images: (32, 1024, 128, 3)\n",
      "labels: (32,)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for images, labels in test_ds.batch(32).take(1):\n",
    "  break\n",
    "\n",
    "print('images:', images.shape)\n",
    "print('labels:', labels.shape)\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8ENR_-swVhnm"
   },
   "source": [
    "To be compatible with keras training the dataset should contain `(inputs, labels)` pairs. For text generation the tokens are both an input and the labels, shifted by one step. This function will convert an `(images, texts)` pair to an `((images, input_tokens), label_tokens)` pair:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T06:21:33.953963Z",
     "iopub.status.busy": "2022-12-14T06:21:33.953470Z",
     "iopub.status.idle": "2022-12-14T06:21:33.957391Z",
     "shell.execute_reply": "2022-12-14T06:21:33.956716Z"
    },
    "id": "2DsgQ_hZT4C2"
   },
   "outputs": [],
   "source": [
    "def prepare_txt(imgs, txts):\n",
    "  tokens = tokenizer(txts)\n",
    "\n",
    "  input_tokens = tokens[..., :-1]\n",
    "  label_tokens = tokens[..., 1:]\n",
    "  return (imgs, input_tokens), label_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DA1x2j0JXX-N"
   },
   "source": [
    "This function adds operations to a dataset. The steps are:\n",
    "\n",
    "1. Load the images (and ignore images that fail to load).\n",
    "2. Replicate images to match the number of captions.\n",
    "3. Shuffle and rebatch the `image, caption` pairs.\n",
    "4. Tokenize the text, shift the tokens and add `label_tokens`.\n",
    "5. Convert the text from a `RaggedTensor` representation to padded dense `Tensor` representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T06:21:33.960764Z",
     "iopub.status.busy": "2022-12-14T06:21:33.960216Z",
     "iopub.status.idle": "2022-12-14T06:21:33.965468Z",
     "shell.execute_reply": "2022-12-14T06:21:33.964820Z"
    },
    "id": "4_Pt9zldjQ0q"
   },
   "outputs": [],
   "source": [
    "def prepare_dataset(ds, tokenizer, batch_size=32, shuffle_buffer=1000):\n",
    "  # Load the images and make batches.\n",
    "  ds = (ds\n",
    "        .shuffle(1000)\n",
    "        .apply(tf.data.experimental.ignore_errors())\n",
    "        .batch(batch_size))\n",
    "\n",
    "  def to_tensor(inputs, labels):\n",
    "    (images, in_tok), out_tok = inputs, labels\n",
    "    return (images, in_tok), out_tok\n",
    "\n",
    "  return (ds\n",
    "          .unbatch()\n",
    "          .shuffle(shuffle_buffer)\n",
    "          .batch(batch_size)\n",
    "          .map(prepare_txt, tf.data.AUTOTUNE)\n",
    "          .map(to_tensor, tf.data.AUTOTUNE)\n",
    "          )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LrQ85t1GNfpQ"
   },
   "source": [
    "You could install the feature extractor in your model and train on the datasets like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T06:21:33.968626Z",
     "iopub.status.busy": "2022-12-14T06:21:33.968216Z",
     "iopub.status.idle": "2022-12-14T06:21:35.058930Z",
     "shell.execute_reply": "2022-12-14T06:21:35.058273Z"
    },
    "id": "1KlhOG5cjQ0r"
   },
   "outputs": [],
   "source": [
    "# train_ds = prepare_dataset(train_ds, tokenizer)\n",
    "# train_ds.element_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T06:21:35.062541Z",
     "iopub.status.busy": "2022-12-14T06:21:35.061878Z",
     "iopub.status.idle": "2022-12-14T06:21:35.187059Z",
     "shell.execute_reply": "2022-12-14T06:21:35.186405Z"
    },
    "id": "d7Zy9F3zX7i2"
   },
   "outputs": [],
   "source": [
    "# test_ds = prepare_dataset(test_ds, tokenizer)\n",
    "# test_ds.element_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# valid_ds = prepare_dataset(valid_ds, tokenizer)\n",
    "# valid_ds.element_spec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XZyKygJ8S8zW"
   },
   "source": [
    "### [Optional] Cache the image features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eHKhSKhti6NS"
   },
   "source": [
    "Since the image feature extractor is not changing, and this tutorial is not using image augmentation, the image features can be cached. Same for the text tokenization. The time it takes to set up the cache is earned back on each epoch during training and validation. The code below defines two functions `save_dataset` and `load_dataset`: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /tmp/ipykernel_323915/2955139511.py:2: ignore_errors (from tensorflow.python.data.experimental.ops.error_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Dataset.ignore_errors` instead.\n"
     ]
    }
   ],
   "source": [
    "tem_ds = (train_ds\n",
    "        .apply(tf.data.experimental.ignore_errors()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorSpec(shape=(1024, 128, 3), dtype=tf.uint8, name=None),\n",
       " TensorSpec(shape=(), dtype=tf.string, name=None))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tem_ds.element_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T06:21:35.190731Z",
     "iopub.status.busy": "2022-12-14T06:21:35.190053Z",
     "iopub.status.idle": "2022-12-14T06:21:35.197647Z",
     "shell.execute_reply": "2022-12-14T06:21:35.197087Z"
    },
    "id": "9N1MX5ym6xm5"
   },
   "outputs": [],
   "source": [
    "def save_dataset(ds, save_path, image_model, tokenizer, shards=10, batch_size=32):\n",
    "  # Load the images and make batches.\n",
    "  ds = (ds\n",
    "        .apply(tf.data.experimental.ignore_errors())\n",
    "        .batch(batch_size))\n",
    "\n",
    "  # Run the feature extractor on each batch\n",
    "  # Don't do this in a .map, because tf.data runs on the CPU. \n",
    "  def gen():\n",
    "    for (images, captions) in tqdm.tqdm(ds): \n",
    "      feature_maps = image_model(images)\n",
    "      yield feature_maps, captions\n",
    "\n",
    "  # Wrap the generator in a new tf.data.Dataset.\n",
    "  new_ds = tf.data.Dataset.from_generator(\n",
    "      gen,\n",
    "      output_signature=(\n",
    "          tf.TensorSpec(shape=image_model.output_shape),\n",
    "          tf.TensorSpec(shape=(None,), dtype=tf.string)))\n",
    "\n",
    "  # Apply the tokenization \n",
    "  new_ds = (new_ds\n",
    "            .map(prepare_txt, tf.data.AUTOTUNE)\n",
    "            .unbatch()\n",
    "            .shuffle(1000))\n",
    "\n",
    "  # Save the dataset into shard files.\n",
    "  def shard_func(i, item):\n",
    "    return i % shards\n",
    "  new_ds.enumerate().save(save_path, shard_func=shard_func)\n",
    "\n",
    "def load_dataset(save_path, batch_size=32, shuffle=1000, cycle_length=2):\n",
    "  def custom_reader_func(datasets):\n",
    "    datasets = datasets.shuffle(1000)\n",
    "    return datasets.interleave(lambda x: x, cycle_length=cycle_length)\n",
    "  \n",
    "  ds = tf.data.Dataset.load(save_path, reader_func=custom_reader_func)\n",
    "\n",
    "  def drop_index(i, x):\n",
    "    return x\n",
    "\n",
    "  ds = (ds\n",
    "        .map(drop_index, tf.data.AUTOTUNE)\n",
    "        .shuffle(shuffle)\n",
    "        .padded_batch(batch_size)\n",
    "        .prefetch(tf.data.AUTOTUNE))\n",
    "  return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T06:21:35.200578Z",
     "iopub.status.busy": "2022-12-14T06:21:35.200328Z",
     "iopub.status.idle": "2022-12-14T06:21:59.861212Z",
     "shell.execute_reply": "2022-12-14T06:21:59.860424Z"
    },
    "id": "tNdzrenxB3Yy"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-06 17:47:29.941432: I tensorflow/core/grappler/optimizers/data/replicate_on_split.cc:32] Running replicate on split optimization\n",
      "0it [00:00, ?it/s]2023-02-06 17:47:29.999113: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 2422603776 exceeds 10% of free system memory.\n",
      "193it [00:11, 16.34it/s]\n"
     ]
    }
   ],
   "source": [
    "save_dataset(train_ds, 'train_cache', mobilenet, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-06 17:47:47.354049: I tensorflow/core/grappler/optimizers/data/replicate_on_split.cc:32] Running replicate on split optimization\n",
      "59it [00:03, 16.52it/s]\n"
     ]
    }
   ],
   "source": [
    "save_dataset(test_ds, 'test_cache', mobilenet, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-06 17:47:51.074863: I tensorflow/core/grappler/optimizers/data/replicate_on_split.cc:32] Running replicate on split optimization\n",
      "58it [00:03, 16.48it/s]\n"
     ]
    }
   ],
   "source": [
    "save_dataset(valid_ds, 'valid_cache', mobilenet, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "798DtfH51UI8"
   },
   "source": [
    " </section>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GI265LiDslr2"
   },
   "source": [
    "## Data ready for training\n",
    "\n",
    "After those preprocessing steps, here are the datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T06:21:59.865217Z",
     "iopub.status.busy": "2022-12-14T06:21:59.864657Z",
     "iopub.status.idle": "2022-12-14T06:21:59.962747Z",
     "shell.execute_reply": "2022-12-14T06:21:59.962022Z"
    },
    "id": "Pwic2YCjHZmV"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/shashank/anaconda3/envs/htr2/lib/python3.8/site-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
      "Instructions for updating:\n",
      "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n"
     ]
    }
   ],
   "source": [
    "train_ds = load_dataset('train_cache')\n",
    "test_ds = load_dataset('test_cache')\n",
    "valid_ds = load_dataset('valid_cache')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T06:21:59.966737Z",
     "iopub.status.busy": "2022-12-14T06:21:59.966108Z",
     "iopub.status.idle": "2022-12-14T06:21:59.970423Z",
     "shell.execute_reply": "2022-12-14T06:21:59.969839Z"
    },
    "id": "3B80JXj7HloX"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((TensorSpec(shape=(None, 32, 4, 576), dtype=tf.float32, name=None),\n",
       "  TensorSpec(shape=(None, 29), dtype=tf.int64, name=None)),\n",
       " TensorSpec(shape=(None, 29), dtype=tf.int64, name=None))"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds.element_spec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5jfb8qknlsKi"
   },
   "source": [
    "The dataset now returns `(input, label)` pairs suitable for training with keras. The `inputs` are `(images, input_tokens)` pairs. The `images` have been processed with the feature-extractor model. For each location in the `input_tokens` the model looks at the text so far and tries to predict the next which is lined up at the same location in the `labels`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T06:21:59.973709Z",
     "iopub.status.busy": "2022-12-14T06:21:59.973070Z",
     "iopub.status.idle": "2022-12-14T06:22:00.176197Z",
     "shell.execute_reply": "2022-12-14T06:22:00.175506Z"
    },
    "id": "YJBEwuXLZQdw"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 32, 4, 576)\n",
      "(32, 29)\n",
      "(32, 29)\n"
     ]
    }
   ],
   "source": [
    "for (inputs, ex_labels) in train_ds.take(1):\n",
    "    (ex_img, ex_in_tok) = inputs\n",
    "\n",
    "print(ex_img.shape)\n",
    "print(ex_in_tok.shape)\n",
    "print(ex_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "22R58DzZoF17"
   },
   "source": [
    "The input tokens and the labels are the same, just shifted by 1 step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T06:22:00.180095Z",
     "iopub.status.busy": "2022-12-14T06:22:00.179589Z",
     "iopub.status.idle": "2022-12-14T06:22:00.185425Z",
     "shell.execute_reply": "2022-12-14T06:22:00.184730Z"
    },
    "id": "V7h5UGftn1hT"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 638   19    7 3042  124    1    3    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0]\n"
     ]
    }
   ],
   "source": [
    "# print(ex_in_tok[0].numpy())\n",
    "print(ex_labels[0].numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DfICM49WFpIb"
   },
   "source": [
    "## A Transformer decoder model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ONyjuWsmZoyO"
   },
   "source": [
    "This model assumes that the pretrained image encoder is sufficient, and just focuses on building the text decoder. This tutorial uses a 2-layer Transformer-decoder.\n",
    "\n",
    "The implementations are almost identical to those in the [Transformers tutorial](https://www.tensorflow.org/text/tutorials/transformer). Refer back to it for more details.\n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "  <th>The Transformer encoder and decoder.</th>\n",
    "</tr>\n",
    "<tr>\n",
    "  <td>\n",
    "   <img width=400 src=\"https://www.tensorflow.org/images/tutorials/transformer/Transformer-1layer-words.png\"/>\n",
    "  </td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qiRXWwIKNybB"
   },
   "source": [
    "The model will be implemented in three main parts: \n",
    "\n",
    "1. Input - The token embedding and positional encoding (`SeqEmbedding`).\n",
    "1. Decoder - A stack of transformer decoder layers (`DecoderLayer`) where each contains:\n",
    "   1. A causal self attention later (`CausalSelfAttention`), where each output location can attend to the output so far.\n",
    "   1. A cross attention layer (`CrossAttention`) where each output location can attend to the input image.\n",
    "   1. A feed forward network (`FeedForward`) layer which further processes each output location independently.\n",
    "1. Output - A multiclass-classification over the output vocabulary.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_ngm3SQMCaYU"
   },
   "source": [
    "### Input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i9suaARZGPKw"
   },
   "source": [
    "The input text has already been split up into tokens and converted to sequences of IDs. \n",
    "\n",
    "Remember that unlike a CNN or RNN the Transformer's attention layers are invariant to the order of the sequence. Without some positional input, it just sees an unordered set not a sequence. So in addition to a simple vector embedding for each token ID, the embedding layer will also include an embedding for each position in the sequence.\n",
    "\n",
    "The `SeqEmbedding` layer defined below:\n",
    "\n",
    "- It looks up the embedding vector for each token.\n",
    "- It looks up an embedding vector for each sequence location.\n",
    "- It adds the two together.\n",
    "- It uses `mask_zero=True` to initialize the keras-masks for the model.\n",
    "\n",
    "Note: This implementation learns the position embeddings instead of using fixed embeddings like in the [Transformer tutorial](https://www.tensorflow.org/text/tutorials/transformer). Learning the embeddings is slightly less code, but doesn't generalize to longer sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T06:22:00.189224Z",
     "iopub.status.busy": "2022-12-14T06:22:00.188653Z",
     "iopub.status.idle": "2022-12-14T06:22:00.194218Z",
     "shell.execute_reply": "2022-12-14T06:22:00.193558Z"
    },
    "id": "P91LU2F0a9Ga"
   },
   "outputs": [],
   "source": [
    "class SeqEmbedding(tf.keras.layers.Layer):\n",
    "  def __init__(self, vocab_size, max_length, depth):\n",
    "    super().__init__()\n",
    "    self.pos_embedding = tf.keras.layers.Embedding(input_dim=max_length, output_dim=depth)\n",
    "\n",
    "    self.token_embedding = tf.keras.layers.Embedding(\n",
    "        input_dim=vocab_size,\n",
    "        output_dim=depth,\n",
    "        mask_zero=True)\n",
    "    \n",
    "    self.add = tf.keras.layers.Add()\n",
    "\n",
    "  def call(self, seq):\n",
    "    seq = self.token_embedding(seq) # (batch, seq, depth)\n",
    "\n",
    "    x = tf.range(tf.shape(seq)[1])  # (seq)\n",
    "    x = x[tf.newaxis, :]  # (1, seq)\n",
    "    x = self.pos_embedding(x)  # (1, seq, depth)\n",
    "\n",
    "    return self.add([seq,x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "II1mD-bBCdMB"
   },
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GHMLeMtKPTCW"
   },
   "source": [
    "The decoder is a standard Transformer-decoder, it contains a stack of `DecoderLayers` where each contains three sublayers: a `CausalSelfAttention`, a `CrossAttention`, and a`FeedForward`. The implementations are almost identical to the [Transformer tutorial](https://www.tensorflow.org/text/tutorials/transformer), refer to it for more details.\n",
    "\n",
    "The `CausalSelfAttention` layer is below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T06:22:00.197504Z",
     "iopub.status.busy": "2022-12-14T06:22:00.196894Z",
     "iopub.status.idle": "2022-12-14T06:22:00.201493Z",
     "shell.execute_reply": "2022-12-14T06:22:00.200856Z"
    },
    "id": "6JTLiX3lKooQ"
   },
   "outputs": [],
   "source": [
    "class CausalSelfAttention(tf.keras.layers.Layer):\n",
    "  def __init__(self, **kwargs):\n",
    "    super().__init__()\n",
    "    self.mha = tf.keras.layers.MultiHeadAttention(**kwargs)\n",
    "    # Use Add instead of + so the keras mask propagates through.\n",
    "    self.add = tf.keras.layers.Add() \n",
    "    self.layernorm = tf.keras.layers.LayerNormalization()\n",
    "  \n",
    "  def call(self, x):\n",
    "    attn = self.mha(query=x, value=x,\n",
    "                    use_causal_mask=True)\n",
    "    x = self.add([x, attn])\n",
    "    return self.layernorm(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8c66OTRwQfd8"
   },
   "source": [
    "The `CrossAttention` layer is below. Note the use of `return_attention_scores`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T06:22:00.204792Z",
     "iopub.status.busy": "2022-12-14T06:22:00.204217Z",
     "iopub.status.idle": "2022-12-14T06:22:00.208992Z",
     "shell.execute_reply": "2022-12-14T06:22:00.208405Z"
    },
    "id": "rIY6Vu2pLBAO"
   },
   "outputs": [],
   "source": [
    "class CrossAttention(tf.keras.layers.Layer):\n",
    "  def __init__(self,**kwargs):\n",
    "    super().__init__()\n",
    "    self.mha = tf.keras.layers.MultiHeadAttention(**kwargs)\n",
    "    self.add = tf.keras.layers.Add() \n",
    "    self.layernorm = tf.keras.layers.LayerNormalization()\n",
    "  \n",
    "  def call(self, x, y, **kwargs):\n",
    "    attn, attention_scores = self.mha(\n",
    "             query=x, value=y,\n",
    "             return_attention_scores=True)\n",
    "    \n",
    "    self.last_attention_scores = attention_scores\n",
    "\n",
    "    x = self.add([x, attn])\n",
    "    return self.layernorm(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8Hn5p6f-RE0C"
   },
   "source": [
    "The `FeedForward` layer is below. Remember that a `layers.Dense` layer is applied to the last axis of the input. The input will have a shape of `(batch, sequence, channels)`, so it automatically applies pointwise across the `batch` and `sequence` axes.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T06:22:00.212209Z",
     "iopub.status.busy": "2022-12-14T06:22:00.211678Z",
     "iopub.status.idle": "2022-12-14T06:22:00.216344Z",
     "shell.execute_reply": "2022-12-14T06:22:00.215737Z"
    },
    "id": "cWKrl7teOnH2"
   },
   "outputs": [],
   "source": [
    "class FeedForward(tf.keras.layers.Layer):\n",
    "  def __init__(self, units, dropout_rate=0.1):\n",
    "    super().__init__()\n",
    "    self.seq = tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(units=2*units, activation='relu'),\n",
    "        tf.keras.layers.Dense(units=units),\n",
    "        tf.keras.layers.Dropout(rate=dropout_rate),\n",
    "    ])\n",
    "\n",
    "    self.layernorm = tf.keras.layers.LayerNormalization()\n",
    "  \n",
    "  def call(self, x):\n",
    "    x = x + self.seq(x)\n",
    "    return self.layernorm(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lbXoiVNPRoJc"
   },
   "source": [
    "Next arrange these three layers into a larger `DecoderLayer`. Each decoder layer applies the three smaller layers in sequence. After each sublayer the shape of `out_seq` is `(batch, sequence, channels)`. The decoder layer also returns the `attention_scores` for later visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T06:22:00.219539Z",
     "iopub.status.busy": "2022-12-14T06:22:00.219010Z",
     "iopub.status.idle": "2022-12-14T06:22:00.223976Z",
     "shell.execute_reply": "2022-12-14T06:22:00.223359Z"
    },
    "id": "ydcW5KZZHou7"
   },
   "outputs": [],
   "source": [
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "  def __init__(self, units, num_heads=1, dropout_rate=0.1):\n",
    "    super().__init__()\n",
    "    \n",
    "    self.self_attention = CausalSelfAttention(num_heads=num_heads,\n",
    "                                              key_dim=units,\n",
    "                                              dropout=dropout_rate)\n",
    "    self.cross_attention = CrossAttention(num_heads=num_heads,\n",
    "                                          key_dim=units,\n",
    "                                          dropout=dropout_rate)\n",
    "    self.ff = FeedForward(units=units, dropout_rate=dropout_rate)\n",
    "      \n",
    "\n",
    "  def call(self, inputs, training=False):\n",
    "    in_seq, out_seq = inputs\n",
    "\n",
    "    # Text input\n",
    "    out_seq = self.self_attention(out_seq)\n",
    "\n",
    "    out_seq = self.cross_attention(out_seq, in_seq)\n",
    "    \n",
    "    self.last_attention_scores = self.cross_attention.last_attention_scores\n",
    "\n",
    "    out_seq = self.ff(out_seq)\n",
    "\n",
    "    return out_seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-lgbYrF5Csqu"
   },
   "source": [
    "### Output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VcnKZkrklAQf"
   },
   "source": [
    "At minimum the output layer needs a `layers.Dense` layer to generate logit-predictions for each token at each location."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6WQD87efena5"
   },
   "source": [
    "But there are a few other features you can add to make this work a little better:\n",
    "\n",
    "1. **Handle bad tokens**: The model will be generating text. It should\n",
    "   never generate a pad, unknown, or start token (`''`, `'[UNK]'`, \n",
    "   `'[START]'`). So set the bias for these to a large negative value.\n",
    "\n",
    "   > Note: You'll need to ignore these tokens in the loss function as well. \n",
    "\n",
    "2. **Smart initialization**: The default initialization of a dense layer will\n",
    "  give a model that initially predicts each token with almost uniform\n",
    "  likelihood. The actual token distribution is far from uniform. The\n",
    "  optimal value for the initial bias of the output layer is the log of the\n",
    "  probability of each token. So include an `adapt` method to count the tokens\n",
    "  and set the optimal initial bias. This reduces the initial loss from the\n",
    "  entropy of the uniform distribution (`log(vocabulary_size)`) to the marginal\n",
    "  entropy of the distribution (`-p*log(p)`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T06:22:00.227371Z",
     "iopub.status.busy": "2022-12-14T06:22:00.226844Z",
     "iopub.status.idle": "2022-12-14T06:22:00.233951Z",
     "shell.execute_reply": "2022-12-14T06:22:00.233320Z"
    },
    "id": "CeWw2SFDHUfo"
   },
   "outputs": [],
   "source": [
    "#@title\n",
    "class TokenOutput(tf.keras.layers.Layer):\n",
    "  def __init__(self, tokenizer, banned_tokens=('', '[UNK]', '[START]'), **kwargs):\n",
    "    super().__init__()\n",
    "    \n",
    "    self.dense = tf.keras.layers.Dense(\n",
    "        units=tokenizer.vocabulary_size(), **kwargs)\n",
    "    self.tokenizer = tokenizer\n",
    "    self.banned_tokens = banned_tokens\n",
    "\n",
    "    self.bias = None\n",
    "\n",
    "  def adapt(self, ds):\n",
    "    counts = collections.Counter()\n",
    "    vocab_dict = {name: id \n",
    "                  for id, name in enumerate(self.tokenizer.get_vocabulary())}\n",
    "\n",
    "    for tokens in tqdm.tqdm(ds):\n",
    "      counts.update(tokens.numpy().flatten())\n",
    "\n",
    "    counts_arr = np.zeros(shape=(self.tokenizer.vocabulary_size(),))\n",
    "    counts_arr[np.array(list(counts.keys()), dtype=np.int32)] = list(counts.values())\n",
    "\n",
    "    counts_arr = counts_arr[:]\n",
    "    for token in self.banned_tokens:\n",
    "      counts_arr[vocab_dict[token]] = 0\n",
    "\n",
    "    total = counts_arr.sum()\n",
    "    p = counts_arr/total\n",
    "    p[counts_arr==0] = 1.0\n",
    "    log_p = np.log(p)  # log(1) == 0\n",
    "\n",
    "    entropy = -(log_p*p).sum()\n",
    "\n",
    "    print()\n",
    "    print(f\"Uniform entropy: {np.log(self.tokenizer.vocabulary_size()):0.2f}\")\n",
    "    print(f\"Marginal entropy: {entropy:0.2f}\")\n",
    "\n",
    "    self.bias = log_p\n",
    "    self.bias[counts_arr==0] = -1e9\n",
    "\n",
    "  def call(self, x):\n",
    "    x = self.dense(x)\n",
    "    # TODO(b/250038731): Fix this.\n",
    "    # An Add layer doesn't work because of the different shapes.\n",
    "    # This clears the mask, that's okay because it prevents keras from rescaling\n",
    "    # the losses.\n",
    "    return x + self.bias\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xzQHqANd1A6Q"
   },
   "source": [
    "The smart initialization will significantly reduce the initial loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T06:22:00.237234Z",
     "iopub.status.busy": "2022-12-14T06:22:00.236671Z",
     "iopub.status.idle": "2022-12-14T06:22:02.999761Z",
     "shell.execute_reply": "2022-12-14T06:22:02.999045Z"
    },
    "id": "GGnOQyc501B2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 193/193 [00:00<00:00, 214.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Uniform entropy: 8.52\n",
      "Marginal entropy: 6.12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "output_layer = TokenOutput(tokenizer, banned_tokens=('', '[UNK]', '[START]'))\n",
    "# This might run a little faster if the dataset didn't also have to load the image data.\n",
    "output_layer.adapt(train_ds.map(lambda inputs, labels: labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3gq-ICN7bD-u"
   },
   "source": [
    "### Build the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gou4fPH_SWgH"
   },
   "source": [
    "To build the model, you need to combine several parts:\n",
    "\n",
    "1. The image `feature_extractor` and the text `tokenizer` and.\n",
    "1. The `seq_embedding` layer, to convert batches of token-IDs to \n",
    "   vectors `(batch, sequence, channels)`.\n",
    "3. The stack of `DecoderLayers` layers that will process the text and image data.\n",
    "4. The `output_layer` which returns a pointwise prediction of what the next word should be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T06:22:03.003339Z",
     "iopub.status.busy": "2022-12-14T06:22:03.003064Z",
     "iopub.status.idle": "2022-12-14T06:22:03.009620Z",
     "shell.execute_reply": "2022-12-14T06:22:03.008880Z"
    },
    "id": "bHCISYehH1f6"
   },
   "outputs": [],
   "source": [
    "class Captioner(tf.keras.Model):\n",
    "  @classmethod\n",
    "  def add_method(cls, fun):\n",
    "    setattr(cls, fun.__name__, fun)\n",
    "    return fun\n",
    "\n",
    "  def __init__(self, tokenizer, feature_extractor, output_layer, num_layers=1,\n",
    "               units=256, max_length=50, num_heads=1, dropout_rate=0.1):\n",
    "    super().__init__()\n",
    "    self.feature_extractor = feature_extractor\n",
    "    self.tokenizer = tokenizer\n",
    "    self.word_to_index = tf.keras.layers.StringLookup(\n",
    "        mask_token=\"\",\n",
    "        vocabulary=tokenizer.get_vocabulary())\n",
    "    self.index_to_word = tf.keras.layers.StringLookup(\n",
    "        mask_token=\"\",\n",
    "        vocabulary=tokenizer.get_vocabulary(),\n",
    "        invert=True) \n",
    "\n",
    "    self.seq_embedding = SeqEmbedding(\n",
    "        vocab_size=tokenizer.vocabulary_size(),\n",
    "        depth=units,\n",
    "        max_length=max_length)\n",
    "\n",
    "    self.decoder_layers = [\n",
    "        DecoderLayer(units, num_heads=num_heads, dropout_rate=dropout_rate)\n",
    "        for n in range(num_layers)]\n",
    "\n",
    "    self.output_layer = output_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YW390dOz9T-x"
   },
   "source": [
    "When you call the model, for training, it receives an `image, txt` pair. To make this function more usable, be flexible about the input:\n",
    "\n",
    "* If the image has 3 channels run it through the feature_extractor. Otherwise assume that it has been already. Similarly\n",
    "* If the text has dtype `tf.string` run it through the tokenizer.\n",
    "\n",
    "After that running the model is only a few steps:\n",
    "\n",
    "1. Flatten the extracted image features, so they can be input to the decoder layers.\n",
    "2. Look up the token embeddings.\n",
    "3. Run the stack of `DecoderLayer`s, on the image features and text embeddings.\n",
    "4. Run the output layer to predict the next token at each position.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T06:22:03.013025Z",
     "iopub.status.busy": "2022-12-14T06:22:03.012423Z",
     "iopub.status.idle": "2022-12-14T06:22:03.017326Z",
     "shell.execute_reply": "2022-12-14T06:22:03.016622Z"
    },
    "id": "lPdb7I4h9Ulo"
   },
   "outputs": [],
   "source": [
    "  @Captioner.add_method\n",
    "  def call(self, inputs):\n",
    "    image, txt = inputs\n",
    "\n",
    "    if image.shape[-1] == 3:\n",
    "      # Apply the feature-extractor, if you get an RGB image.\n",
    "      image = self.feature_extractor(image)\n",
    "    \n",
    "    # Flatten the feature map\n",
    "    image = einops.rearrange(image, 'b h w c -> b (h w) c')\n",
    "\n",
    "\n",
    "    if txt.dtype == tf.string:\n",
    "      # Apply the tokenizer if you get string inputs.\n",
    "      txt = tokenizer(txt)\n",
    "\n",
    "    txt = self.seq_embedding(txt)\n",
    "\n",
    "    # Look at the image\n",
    "    for dec_layer in self.decoder_layers:\n",
    "      txt = dec_layer(inputs=(image, txt))\n",
    "      \n",
    "    txt = self.output_layer(txt)\n",
    "\n",
    "    return txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T06:22:03.020421Z",
     "iopub.status.busy": "2022-12-14T06:22:03.019922Z",
     "iopub.status.idle": "2022-12-14T06:22:03.126998Z",
     "shell.execute_reply": "2022-12-14T06:22:03.126342Z"
    },
    "id": "kmM7aZQsLiyU"
   },
   "outputs": [],
   "source": [
    "model = Captioner(tokenizer, feature_extractor=mobilenet, output_layer=output_layer,\n",
    "                  units=256, dropout_rate=0.5, num_layers=2, num_heads=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xGvOcLQKghXN"
   },
   "source": [
    "### Generate captions\n",
    "\n",
    "Before getting into training, write a bit of code to generate captions. You'll use this to see how training is progressing.\n",
    "\n",
    "Start by downloading a test image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T06:22:03.130545Z",
     "iopub.status.busy": "2022-12-14T06:22:03.129981Z",
     "iopub.status.idle": "2022-12-14T06:22:03.264289Z",
     "shell.execute_reply": "2022-12-14T06:22:03.263588Z"
    },
    "id": "cwFcdMqC-jE2"
   },
   "outputs": [],
   "source": [
    "for img, label in test_ds.take(1):\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[b'[UNK]' b'[UNK]' b'said' b'[UNK]' b'smiling' b'from' b'the' b'bottom'\n",
      " b'step' b'[END]' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b''\n",
      " b'' b'' b'' b'' b''], shape=(29,), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "w = index_to_word(label[0])\n",
    "print(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(32, 4, 576), dtype=float32, numpy=\n",
       "array([[[  -0.    ,   -0.    ,   -0.    , ...,   -0.    ,   -0.    ,\n",
       "           -0.    ],\n",
       "        [  -0.    ,   -0.    ,   -0.    , ...,   -0.    ,   -0.    ,\n",
       "           -0.    ],\n",
       "        [  -0.    ,   -0.    ,   -0.    , ...,   -0.    ,   -0.    ,\n",
       "           -0.    ],\n",
       "        [  -0.    ,   -0.    ,   -0.    , ...,   -0.    ,   -0.    ,\n",
       "           -0.    ]],\n",
       "\n",
       "       [[  -0.    ,   -0.    ,   -0.    , ...,   -0.    ,   -0.    ,\n",
       "           -0.    ],\n",
       "        [  -0.    , 7949.9277,   -0.    , ...,   -0.    ,   -0.    ,\n",
       "           -0.    ],\n",
       "        [  -0.    ,   -0.    ,   -0.    , ...,   -0.    ,   -0.    ,\n",
       "           -0.    ],\n",
       "        [  -0.    , 5098.855 ,   -0.    , ...,   -0.    ,   -0.    ,\n",
       "           -0.    ]],\n",
       "\n",
       "       [[  -0.    , 1254.3976,   -0.    , ...,   -0.    ,   -0.    ,\n",
       "           -0.    ],\n",
       "        [  -0.    , 2324.9558,   -0.    , ...,   -0.    ,   -0.    ,\n",
       "           -0.    ],\n",
       "        [  -0.    ,   -0.    ,   -0.    , ...,   -0.    ,   -0.    ,\n",
       "           -0.    ],\n",
       "        [ 870.4131, 5140.9136,   -0.    , ...,   -0.    ,   -0.    ,\n",
       "           -0.    ]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[  -0.    ,   -0.    ,   -0.    , ...,   -0.    ,   -0.    ,\n",
       "           -0.    ],\n",
       "        [  -0.    ,  912.2126,   -0.    , ...,   -0.    ,   -0.    ,\n",
       "           -0.    ],\n",
       "        [  -0.    ,   -0.    ,   -0.    , ...,   -0.    ,   -0.    ,\n",
       "           -0.    ],\n",
       "        [2918.3018, 4624.579 ,   -0.    , ...,   -0.    ,   -0.    ,\n",
       "           -0.    ]],\n",
       "\n",
       "       [[  -0.    ,   -0.    ,   -0.    , ...,   -0.    ,   -0.    ,\n",
       "           -0.    ],\n",
       "        [  -0.    ,   -0.    ,   -0.    , ...,   -0.    ,   -0.    ,\n",
       "           -0.    ],\n",
       "        [  -0.    ,   -0.    ,   -0.    , ...,   -0.    ,   -0.    ,\n",
       "           -0.    ],\n",
       "        [  -0.    ,  457.7157,   -0.    , ...,   -0.    ,   -0.    ,\n",
       "           -0.    ]],\n",
       "\n",
       "       [[  -0.    ,   -0.    ,   -0.    , ...,   -0.    ,   -0.    ,\n",
       "           -0.    ],\n",
       "        [  -0.    ,   -0.    ,   -0.    , ...,   -0.    ,   -0.    ,\n",
       "           -0.    ],\n",
       "        [  -0.    ,   -0.    ,   -0.    , ...,   -0.    ,   -0.    ,\n",
       "           -0.    ],\n",
       "        [  -0.    ,   -0.    ,   -0.    , ...,   -0.    ,   -0.    ,\n",
       "           -0.    ]]], dtype=float32)>"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IRBIiTkubmxA"
   },
   "source": [
    "To caption an image with this model:\n",
    "\n",
    "- Extract the `img_features`\n",
    "- Initialize the list of output tokens with a `[START]` token.\n",
    "- Pass `img_features` and `tokens` into the model.\n",
    "  - It returns a list of logits.\n",
    "  - Choose the next token based on those logits.  \n",
    "  - Add it to the list of tokens, and continue the loop.\n",
    "  - If it generates an `'[END]'` token, break out of the loop.\n",
    "\n",
    "So add a \"simple\" method to do just that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T06:22:03.267487Z",
     "iopub.status.busy": "2022-12-14T06:22:03.267222Z",
     "iopub.status.idle": "2022-12-14T06:22:03.273062Z",
     "shell.execute_reply": "2022-12-14T06:22:03.272456Z"
    },
    "id": "Nf1Jie9ef_Cg"
   },
   "outputs": [],
   "source": [
    "@Captioner.add_method\n",
    "def simple_gen(self, image, temperature=1):\n",
    "  initial = self.word_to_index([['[START]']]) # (batch, sequence)\n",
    "  img_features = tf.expand_dims(image, axis=0)\n",
    "#   img_features = self.feature_extractor(image[tf.newaxis, ...])\n",
    "\n",
    "  tokens = initial # (batch, sequence)\n",
    "  for n in range(50):\n",
    "    preds = self((img_features, tokens)).numpy()  # (batch, sequence, vocab)\n",
    "    preds = preds[:,-1, :]  #(batch, vocab)\n",
    "    if temperature==0:\n",
    "        next = tf.argmax(preds, axis=-1)[:, tf.newaxis]  # (batch, 1)\n",
    "    else:\n",
    "        next = tf.random.categorical(preds/temperature, num_samples=1)  # (batch, 1)\n",
    "    tokens = tf.concat([tokens, next], axis=1) # (batch, sequence) \n",
    "\n",
    "    if next[0] == self.word_to_index('[END]'):\n",
    "      break\n",
    "  words = index_to_word(tokens[0, 1:-1])\n",
    "  result = tf.strings.reduce_join(words, axis=-1, separator=' ')\n",
    "  return result.numpy().decode()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TxN2NPX2zB8y"
   },
   "source": [
    "Here are some generated captions for that image, the model's untrained, so they don't make much sense yet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T06:22:03.276367Z",
     "iopub.status.busy": "2022-12-14T06:22:03.275837Z",
     "iopub.status.idle": "2022-12-14T06:22:05.806911Z",
     "shell.execute_reply": "2022-12-14T06:22:05.806094Z"
    },
    "id": "sPm96CccvHnq"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "the the a the the the\n",
      "set by again colonial sedate fear like width parks spite you a\n"
     ]
    }
   ],
   "source": [
    "for t in (0.0, 0.5, 1.0):\n",
    "  result = model.simple_gen(img[0][0], temperature=t)\n",
    "  print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JefwCRZ8z-Ah"
   },
   "source": [
    "The temperature parameter allows you to interpolate between 3 modes:\n",
    "\n",
    "1. Greedy decoding (`temperature=0.0`) - Chooses the most likely next token at each step.\n",
    "2. Random sampling according to the logits (`temperature=1.0`).\n",
    "3. Uniform random sampling (`temperature >> 1.0`). \n",
    "\n",
    "Since the model is untrained, and it used the frequency-based initialization, the \"greedy\" output (first) usually only contains the most common tokens: `['a', '.', '[END]']`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r0FpTvaPkqON"
   },
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IKcwZdqObK-U"
   },
   "source": [
    "To train the model you'll need several additional components:\n",
    "\n",
    "- The Loss and metrics\n",
    "- The Optimizer\n",
    "- Optional Callbacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g5IW2mWa2sAG"
   },
   "source": [
    "### Losses and metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XbpbDQTw1lOW"
   },
   "source": [
    "Here's an implementation of a masked loss and accuracy:\n",
    "\n",
    "When calculating the mask for the loss, note the `loss < 1e8`. This term discards the artificial, impossibly high losses for the `banned_tokens`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T06:22:05.810811Z",
     "iopub.status.busy": "2022-12-14T06:22:05.810544Z",
     "iopub.status.idle": "2022-12-14T06:22:05.815828Z",
     "shell.execute_reply": "2022-12-14T06:22:05.815226Z"
    },
    "id": "s24im3FqxAfT"
   },
   "outputs": [],
   "source": [
    "def masked_loss(labels, preds):  \n",
    "  loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels, preds)\n",
    "\n",
    "  mask = (labels != 0) & (loss < 1e8) \n",
    "  mask = tf.cast(mask, loss.dtype)\n",
    "\n",
    "  loss = loss*mask\n",
    "  loss = tf.reduce_sum(loss)/tf.reduce_sum(mask)\n",
    "  return loss\n",
    "\n",
    "def masked_acc(labels, preds):\n",
    "  mask = tf.cast(labels!=0, tf.float32)\n",
    "  preds = tf.argmax(preds, axis=-1)\n",
    "  labels = tf.cast(labels, tf.int64)\n",
    "  match = tf.cast(preds == labels, mask.dtype)\n",
    "  acc = tf.reduce_sum(match*mask)/tf.reduce_sum(mask)\n",
    "  return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zOhjHqgv3F2e"
   },
   "source": [
    "### Callbacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3dyQN9UfJYEd"
   },
   "source": [
    "For feedback during training setup a `keras.callbacks.Callback` to generate some captions for the surfer image at the end of each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T06:22:05.819452Z",
     "iopub.status.busy": "2022-12-14T06:22:05.818884Z",
     "iopub.status.idle": "2022-12-14T06:22:05.823422Z",
     "shell.execute_reply": "2022-12-14T06:22:05.822773Z"
    },
    "id": "IKDwbZOCZ-AP"
   },
   "outputs": [],
   "source": [
    "class GenerateText(tf.keras.callbacks.Callback):\n",
    "  def __init__(self):\n",
    "    self.image=img[0][0]\n",
    "    \n",
    "\n",
    "  def on_epoch_end(self, epochs=None, logs=None):\n",
    "    print()\n",
    "    print()\n",
    "    for t in (0.0, 0.5, 1.0):\n",
    "      result = self.model.simple_gen(self.image, temperature=t)\n",
    "      print(result)\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1yNA3_RAsdl0"
   },
   "source": [
    "It generates three output strings, like the earlier example, like before the first is \"greedy\", choosing the argmax of the logits at each step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T06:22:05.826873Z",
     "iopub.status.busy": "2022-12-14T06:22:05.826250Z",
     "iopub.status.idle": "2022-12-14T06:22:06.569844Z",
     "shell.execute_reply": "2022-12-14T06:22:06.569031Z"
    },
    "id": "IGVLpzo13rcA"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "one way edward\n",
      "\n"
     ]
    }
   ],
   "source": [
    "g = GenerateText()\n",
    "g.model = model\n",
    "g.on_epoch_end(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MAxp4KZRKDk9"
   },
   "source": [
    "Also use `callbacks.EarlyStopping` to terminate training when the model starts to overfit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T06:22:06.574003Z",
     "iopub.status.busy": "2022-12-14T06:22:06.573386Z",
     "iopub.status.idle": "2022-12-14T06:22:06.581462Z",
     "shell.execute_reply": "2022-12-14T06:22:06.580830Z"
    },
    "id": "MjzrwGZp23xx"
   },
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    GenerateText(),\n",
    "    tf.keras.callbacks.EarlyStopping(\n",
    "        patience=5, restore_best_weights=True)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZBaJhQpcG8u0"
   },
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WBXG0dCDKO55"
   },
   "source": [
    "Configure and execute the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T06:22:06.585144Z",
     "iopub.status.busy": "2022-12-14T06:22:06.584521Z",
     "iopub.status.idle": "2022-12-14T06:22:06.610757Z",
     "shell.execute_reply": "2022-12-14T06:22:06.610166Z"
    },
    "id": "2OR5ZpAII__u"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
    "           loss=masked_loss,\n",
    "           metrics=[masked_acc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ro955bQ2KR0X"
   },
   "source": [
    "For more frequent reporting, use the `Dataset.repeat()` method, and set the `steps_per_epoch` and `validation_steps` arguments to `Model.fit`. \n",
    "\n",
    "With this setup on `Flickr8k` a full pass over the dataset is 900+ batches, but below the reporting-epochs are 100 steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T06:22:06.614610Z",
     "iopub.status.busy": "2022-12-14T06:22:06.613905Z",
     "iopub.status.idle": "2022-12-14T06:25:36.881556Z",
     "shell.execute_reply": "2022-12-14T06:25:36.880775Z"
    },
    "id": "3aB0baOVMZe9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-06 17:57:46.056495: I tensorflow/compiler/xla/service/service.cc:173] XLA service 0x55615eff28b0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2023-02-06 17:57:46.056513: I tensorflow/compiler/xla/service/service.cc:181]   StreamExecutor device (0): NVIDIA GeForce RTX 2080 Ti, Compute Capability 7.5\n",
      "2023-02-06 17:57:46.084298: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2023-02-06 17:57:46.323183: W tensorflow/compiler/xla/stream_executor/gpu/asm_compiler.cc:115] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "You may not need to update to CUDA 11.1; cherry-picking the ptxas binary is often sufficient.\n",
      "2023-02-06 17:57:46.358118: I tensorflow/compiler/jit/xla_compilation_cache.cc:477] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2023-02-06 17:57:46.418999: W tensorflow/compiler/xla/stream_executor/gpu/asm_compiler.cc:115] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "You may not need to update to CUDA 11.1; cherry-picking the ptxas binary is often sufficient.\n",
      "2023-02-06 17:57:46.507457: W tensorflow/compiler/xla/stream_executor/gpu/asm_compiler.cc:115] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "You may not need to update to CUDA 11.1; cherry-picking the ptxas binary is often sufficient.\n",
      "2023-02-06 17:57:46.713655: W tensorflow/compiler/xla/stream_executor/gpu/asm_compiler.cc:115] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "You may not need to update to CUDA 11.1; cherry-picking the ptxas binary is often sufficient.\n",
      "2023-02-06 17:57:46.801362: W tensorflow/compiler/xla/stream_executor/gpu/asm_compiler.cc:115] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "You may not need to update to CUDA 11.1; cherry-picking the ptxas binary is often sufficient.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "  1/100 [..............................] - ETA: 13:12 - loss: 6.3798 - masked_acc: 0.1136"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-06 17:57:49.548087: W tensorflow/compiler/xla/stream_executor/gpu/asm_compiler.cc:115] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "You may not need to update to CUDA 11.1; cherry-picking the ptxas binary is often sufficient.\n",
      "2023-02-06 17:57:49.654136: W tensorflow/compiler/xla/stream_executor/gpu/asm_compiler.cc:115] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "You may not need to update to CUDA 11.1; cherry-picking the ptxas binary is often sufficient.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  3/100 [..............................] - ETA: 12s - loss: 6.2430 - masked_acc: 0.1174 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-06 17:57:49.791489: W tensorflow/compiler/xla/stream_executor/gpu/asm_compiler.cc:115] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "You may not need to update to CUDA 11.1; cherry-picking the ptxas binary is often sufficient.\n",
      "2023-02-06 17:57:49.921590: W tensorflow/compiler/xla/stream_executor/gpu/asm_compiler.cc:115] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "You may not need to update to CUDA 11.1; cherry-picking the ptxas binary is often sufficient.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  6/100 [>.............................] - ETA: 10s - loss: 6.2145 - masked_acc: 0.1143"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-06 17:57:50.081423: W tensorflow/compiler/xla/stream_executor/gpu/asm_compiler.cc:115] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "You may not need to update to CUDA 11.1; cherry-picking the ptxas binary is often sufficient.\n",
      "2023-02-06 17:57:50.212341: W tensorflow/compiler/xla/stream_executor/gpu/asm_compiler.cc:115] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "You may not need to update to CUDA 11.1; cherry-picking the ptxas binary is often sufficient.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  8/100 [=>............................] - ETA: 10s - loss: 6.1984 - masked_acc: 0.1138"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-06 17:57:50.340393: W tensorflow/compiler/xla/stream_executor/gpu/asm_compiler.cc:115] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "You may not need to update to CUDA 11.1; cherry-picking the ptxas binary is often sufficient.\n",
      "2023-02-06 17:57:50.469905: W tensorflow/compiler/xla/stream_executor/gpu/asm_compiler.cc:115] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "You may not need to update to CUDA 11.1; cherry-picking the ptxas binary is often sufficient.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 10/100 [==>...........................] - ETA: 10s - loss: 6.1691 - masked_acc: 0.1131"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-06 17:57:50.597591: W tensorflow/compiler/xla/stream_executor/gpu/asm_compiler.cc:115] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "You may not need to update to CUDA 11.1; cherry-picking the ptxas binary is often sufficient.\n",
      "2023-02-06 17:57:50.733022: W tensorflow/compiler/xla/stream_executor/gpu/asm_compiler.cc:115] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "You may not need to update to CUDA 11.1; cherry-picking the ptxas binary is often sufficient.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      " 13/100 [==>...........................] - ETA: 9s - loss: 6.1754 - masked_acc: 0.1125 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-06 17:57:50.918754: W tensorflow/compiler/xla/stream_executor/gpu/asm_compiler.cc:115] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "You may not need to update to CUDA 11.1; cherry-picking the ptxas binary is often sufficient.\n",
      "2023-02-06 17:57:51.078114: W tensorflow/compiler/xla/stream_executor/gpu/asm_compiler.cc:115] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "You may not need to update to CUDA 11.1; cherry-picking the ptxas binary is often sufficient.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 18/100 [====>.........................] - ETA: 8s - loss: 6.1841 - masked_acc: 0.1134"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-06 17:57:51.214252: W tensorflow/compiler/xla/stream_executor/gpu/asm_compiler.cc:115] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "You may not need to update to CUDA 11.1; cherry-picking the ptxas binary is often sufficient.\n",
      "2023-02-06 17:57:51.363371: W tensorflow/compiler/xla/stream_executor/gpu/asm_compiler.cc:115] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "You may not need to update to CUDA 11.1; cherry-picking the ptxas binary is often sufficient.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 20/100 [=====>........................] - ETA: 8s - loss: 6.1770 - masked_acc: 0.1141"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-06 17:57:51.495137: W tensorflow/compiler/xla/stream_executor/gpu/asm_compiler.cc:115] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "You may not need to update to CUDA 11.1; cherry-picking the ptxas binary is often sufficient.\n",
      "2023-02-06 17:57:51.619841: W tensorflow/compiler/xla/stream_executor/gpu/asm_compiler.cc:115] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "You may not need to update to CUDA 11.1; cherry-picking the ptxas binary is often sufficient.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 24/100 [======>.......................] - ETA: 7s - loss: 6.1619 - masked_acc: 0.1150"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-06 17:57:51.775809: W tensorflow/compiler/xla/stream_executor/gpu/asm_compiler.cc:115] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "You may not need to update to CUDA 11.1; cherry-picking the ptxas binary is often sufficient.\n",
      "2023-02-06 17:57:51.923501: W tensorflow/compiler/xla/stream_executor/gpu/asm_compiler.cc:115] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "You may not need to update to CUDA 11.1; cherry-picking the ptxas binary is often sufficient.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 26/100 [======>.......................] - ETA: 7s - loss: 6.1719 - masked_acc: 0.1152"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-06 17:57:52.077003: W tensorflow/compiler/xla/stream_executor/gpu/asm_compiler.cc:115] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "You may not need to update to CUDA 11.1; cherry-picking the ptxas binary is often sufficient.\n",
      "2023-02-06 17:57:52.209550: W tensorflow/compiler/xla/stream_executor/gpu/asm_compiler.cc:115] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "You may not need to update to CUDA 11.1; cherry-picking the ptxas binary is often sufficient.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 29/100 [=======>......................] - ETA: 7s - loss: 6.1788 - masked_acc: 0.1152"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-06 17:57:52.333542: W tensorflow/compiler/xla/stream_executor/gpu/asm_compiler.cc:115] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "You may not need to update to CUDA 11.1; cherry-picking the ptxas binary is often sufficient.\n",
      "2023-02-06 17:57:52.479648: W tensorflow/compiler/xla/stream_executor/gpu/asm_compiler.cc:115] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "You may not need to update to CUDA 11.1; cherry-picking the ptxas binary is often sufficient.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 33/100 [========>.....................] - ETA: 6s - loss: 6.1773 - masked_acc: 0.1152"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-06 17:57:52.649823: W tensorflow/compiler/xla/stream_executor/gpu/asm_compiler.cc:115] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "You may not need to update to CUDA 11.1; cherry-picking the ptxas binary is often sufficient.\n",
      "2023-02-06 17:57:52.776256: W tensorflow/compiler/xla/stream_executor/gpu/asm_compiler.cc:115] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "You may not need to update to CUDA 11.1; cherry-picking the ptxas binary is often sufficient.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 46/100 [============>.................] - ETA: 4s - loss: 6.1735 - masked_acc: 0.1150"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-06 17:57:53.163345: W tensorflow/compiler/xla/stream_executor/gpu/asm_compiler.cc:115] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "You may not need to update to CUDA 11.1; cherry-picking the ptxas binary is often sufficient.\n",
      "2023-02-06 17:57:53.297629: W tensorflow/compiler/xla/stream_executor/gpu/asm_compiler.cc:115] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "You may not need to update to CUDA 11.1; cherry-picking the ptxas binary is often sufficient.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 56/100 [===============>..............] - ETA: 3s - loss: 6.1702 - masked_acc: 0.1148"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-06 17:57:53.484613: W tensorflow/compiler/xla/stream_executor/gpu/asm_compiler.cc:115] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "You may not need to update to CUDA 11.1; cherry-picking the ptxas binary is often sufficient.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 71/100 [====================>.........] - ETA: 1s - loss: 6.1774 - masked_acc: 0.1148"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-06 17:57:53.907139: W tensorflow/compiler/xla/stream_executor/gpu/asm_compiler.cc:115] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "You may not need to update to CUDA 11.1; cherry-picking the ptxas binary is often sufficient.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 99/100 [============================>.] - ETA: 0s - loss: 6.1760 - masked_acc: 0.1148"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-06 17:57:54.788945: W tensorflow/compiler/xla/stream_executor/gpu/asm_compiler.cc:115] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "You may not need to update to CUDA 11.1; cherry-picking the ptxas binary is often sufficient.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "of\n",
      "\n",
      "100/100 [==============================] - 15s 67ms/step - loss: 6.1748 - masked_acc: 0.1147 - val_loss: 5.7214 - val_masked_acc: 0.1119\n",
      "Epoch 2/100\n",
      " 16/100 [===>..........................] - ETA: 2s - loss: 6.1712 - masked_acc: 0.1152"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-06 17:57:56.573117: W tensorflow/compiler/xla/stream_executor/gpu/asm_compiler.cc:115] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "You may not need to update to CUDA 11.1; cherry-picking the ptxas binary is often sufficient.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 30/100 [========>.....................] - ETA: 1s - loss: 6.1534 - masked_acc: 0.1151"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-06 17:57:56.970999: W tensorflow/compiler/xla/stream_executor/gpu/asm_compiler.cc:115] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "You may not need to update to CUDA 11.1; cherry-picking the ptxas binary is often sufficient.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 40/100 [===========>..................] - ETA: 1s - loss: 6.1580 - masked_acc: 0.1146"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-06 17:57:57.300818: W tensorflow/compiler/xla/stream_executor/gpu/asm_compiler.cc:115] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "You may not need to update to CUDA 11.1; cherry-picking the ptxas binary is often sufficient.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 48/100 [=============>................] - ETA: 1s - loss: 6.1654 - masked_acc: 0.1147"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-06 17:57:57.569864: W tensorflow/compiler/xla/stream_executor/gpu/asm_compiler.cc:115] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "You may not need to update to CUDA 11.1; cherry-picking the ptxas binary is often sufficient.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 85/100 [========================>.....] - ETA: 0s - loss: 6.1580 - masked_acc: 0.1141"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-06 17:57:58.437652: W tensorflow/compiler/xla/stream_executor/gpu/asm_compiler.cc:115] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "You may not need to update to CUDA 11.1; cherry-picking the ptxas binary is often sufficient.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 91/100 [==========================>...] - ETA: 0s - loss: 6.1606 - masked_acc: 0.1139"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-06 17:57:58.766488: W tensorflow/compiler/xla/stream_executor/gpu/asm_compiler.cc:115] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "You may not need to update to CUDA 11.1; cherry-picking the ptxas binary is often sufficient.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 96/100 [===========================>..] - ETA: 0s - loss: 6.1618 - masked_acc: 0.1140"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-06 17:57:58.973277: W tensorflow/compiler/xla/stream_executor/gpu/asm_compiler.cc:115] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "You may not need to update to CUDA 11.1; cherry-picking the ptxas binary is often sufficient.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      " 98/100 [============================>.] - ETA: 0s - loss: 6.1592 - masked_acc: 0.1138"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-06 17:57:59.185505: W tensorflow/compiler/xla/stream_executor/gpu/asm_compiler.cc:115] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "You may not need to update to CUDA 11.1; cherry-picking the ptxas binary is often sufficient.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "but understanding by master the effective a national which in\n",
      "\n",
      "100/100 [==============================] - 4s 40ms/step - loss: 6.1602 - masked_acc: 0.1139 - val_loss: 5.6848 - val_masked_acc: 0.1132\n",
      "Epoch 3/100\n",
      " 25/100 [======>.......................] - ETA: 1s - loss: 6.1430 - masked_acc: 0.1165"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-06 17:58:00.798069: W tensorflow/compiler/xla/stream_executor/gpu/asm_compiler.cc:115] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "You may not need to update to CUDA 11.1; cherry-picking the ptxas binary is often sufficient.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 33/100 [========>.....................] - ETA: 1s - loss: 6.1391 - masked_acc: 0.1163"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-06 17:58:01.069918: W tensorflow/compiler/xla/stream_executor/gpu/asm_compiler.cc:115] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "You may not need to update to CUDA 11.1; cherry-picking the ptxas binary is often sufficient.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 54/100 [===============>..............] - ETA: 1s - loss: 6.1342 - masked_acc: 0.1153"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-06 17:58:01.613488: W tensorflow/compiler/xla/stream_executor/gpu/asm_compiler.cc:115] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "You may not need to update to CUDA 11.1; cherry-picking the ptxas binary is often sufficient.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 99/100 [============================>.] - ETA: 0s - loss: 6.1521 - masked_acc: 0.1143"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-06 17:58:02.772437: W tensorflow/compiler/xla/stream_executor/gpu/asm_compiler.cc:115] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "You may not need to update to CUDA 11.1; cherry-picking the ptxas binary is often sufficient.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "the lloyd he normal performance may\n",
      "\n",
      "100/100 [==============================] - 3s 34ms/step - loss: 6.1547 - masked_acc: 0.1143 - val_loss: 5.6369 - val_masked_acc: 0.1114\n",
      "Epoch 4/100\n",
      " 42/100 [===========>..................] - ETA: 1s - loss: 6.1413 - masked_acc: 0.1156"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-06 17:58:04.508291: W tensorflow/compiler/xla/stream_executor/gpu/asm_compiler.cc:115] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "You may not need to update to CUDA 11.1; cherry-picking the ptxas binary is often sufficient.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 55/100 [===============>..............] - ETA: 1s - loss: 6.1360 - masked_acc: 0.1153"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-06 17:58:04.906531: W tensorflow/compiler/xla/stream_executor/gpu/asm_compiler.cc:115] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "You may not need to update to CUDA 11.1; cherry-picking the ptxas binary is often sufficient.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 80/100 [=======================>......] - ETA: 0s - loss: 6.1289 - masked_acc: 0.1148"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-06 17:58:05.653737: W tensorflow/compiler/xla/stream_executor/gpu/asm_compiler.cc:115] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "You may not need to update to CUDA 11.1; cherry-picking the ptxas binary is often sufficient.\n",
      "2023-02-06 17:58:05.820795: W tensorflow/compiler/xla/stream_executor/gpu/asm_compiler.cc:115] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "You may not need to update to CUDA 11.1; cherry-picking the ptxas binary is often sufficient.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 91/100 [==========================>...] - ETA: 0s - loss: 6.1377 - masked_acc: 0.1144"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-06 17:58:06.041213: W tensorflow/compiler/xla/stream_executor/gpu/asm_compiler.cc:115] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "You may not need to update to CUDA 11.1; cherry-picking the ptxas binary is often sufficient.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - ETA: 0s - loss: 6.1305 - masked_acc: 0.1146\n",
      "\n",
      "\n",
      "and\n",
      "by homoeopathy fast disaster\n",
      "\n",
      "100/100 [==============================] - 3s 35ms/step - loss: 6.1305 - masked_acc: 0.1146 - val_loss: 5.6614 - val_masked_acc: 0.1108\n",
      "Epoch 5/100\n",
      " 77/100 [======================>.......] - ETA: 0s - loss: 6.1281 - masked_acc: 0.1159"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-06 17:58:08.779062: W tensorflow/compiler/xla/stream_executor/gpu/asm_compiler.cc:115] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "You may not need to update to CUDA 11.1; cherry-picking the ptxas binary is often sufficient.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 98/100 [============================>.] - ETA: 0s - loss: 6.1382 - masked_acc: 0.1161\n",
      "\n",
      "\n",
      "\n",
      "the a\n",
      "\n",
      "100/100 [==============================] - 3s 30ms/step - loss: 6.1383 - masked_acc: 0.1162 - val_loss: 5.6748 - val_masked_acc: 0.1120\n",
      "Epoch 6/100\n",
      " 54/100 [===============>..............] - ETA: 1s - loss: 6.1256 - masked_acc: 0.1144"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-06 17:58:11.250799: W tensorflow/compiler/xla/stream_executor/gpu/asm_compiler.cc:115] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "You may not need to update to CUDA 11.1; cherry-picking the ptxas binary is often sufficient.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 85/100 [========================>.....] - ETA: 0s - loss: 6.1315 - masked_acc: 0.1147"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-06 17:58:12.030928: W tensorflow/compiler/xla/stream_executor/gpu/asm_compiler.cc:115] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "You may not need to update to CUDA 11.1; cherry-picking the ptxas binary is often sufficient.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - ETA: 0s - loss: 6.1334 - masked_acc: 0.1152\n",
      "\n",
      "\n",
      "the\n",
      "but is production point press dingy standards with de foreign\n",
      "\n",
      "100/100 [==============================] - 3s 34ms/step - loss: 6.1334 - masked_acc: 0.1152 - val_loss: 5.6381 - val_masked_acc: 0.1109\n",
      "Epoch 7/100\n",
      " 64/100 [==================>...........] - ETA: 0s - loss: 6.1313 - masked_acc: 0.1163"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-06 17:58:14.782003: W tensorflow/compiler/xla/stream_executor/gpu/asm_compiler.cc:115] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "You may not need to update to CUDA 11.1; cherry-picking the ptxas binary is often sufficient.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - ETA: 0s - loss: 6.1244 - masked_acc: 0.1158\n",
      "\n",
      "\n",
      "\n",
      "in of her\n",
      "\n",
      "100/100 [==============================] - 3s 28ms/step - loss: 6.1244 - masked_acc: 0.1158 - val_loss: 5.6813 - val_masked_acc: 0.1124\n",
      "Epoch 8/100\n",
      " 78/100 [======================>.......] - ETA: 0s - loss: 6.0882 - masked_acc: 0.1151"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-06 17:58:17.933515: W tensorflow/compiler/xla/stream_executor/gpu/asm_compiler.cc:115] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "You may not need to update to CUDA 11.1; cherry-picking the ptxas binary is often sufficient.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 99/100 [============================>.] - ETA: 0s - loss: 6.0967 - masked_acc: 0.1155\n",
      "\n",
      "\n",
      "\n",
      "delay is known for dropped diplomatist get government government sixtyfour assumption project the and is to are\n",
      "\n",
      "100/100 [==============================] - 4s 36ms/step - loss: 6.0951 - masked_acc: 0.1155 - val_loss: 5.7056 - val_masked_acc: 0.1119\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    train_ds.repeat(),\n",
    "    steps_per_epoch=100,\n",
    "    validation_data=valid_ds.repeat(),\n",
    "    validation_steps=20,\n",
    "    epochs=100,\n",
    "    callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P634LfVgw-eV"
   },
   "source": [
    "Plot the loss and accuracy over the training run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T06:25:36.885901Z",
     "iopub.status.busy": "2022-12-14T06:25:36.885233Z",
     "iopub.status.idle": "2022-12-14T06:25:37.060469Z",
     "shell.execute_reply": "2022-12-14T06:25:37.059838Z"
    },
    "id": "6Wn8KSkUw916"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7efa9f74b580>"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['loss'], label='loss')\n",
    "plt.plot(history.history['val_loss'], label='val_loss')\n",
    "plt.ylim([0, max(plt.ylim())])\n",
    "plt.xlabel('Epoch #')\n",
    "plt.ylabel('CE/token')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T06:25:37.063774Z",
     "iopub.status.busy": "2022-12-14T06:25:37.063531Z",
     "iopub.status.idle": "2022-12-14T06:25:37.223243Z",
     "shell.execute_reply": "2022-12-14T06:25:37.222629Z"
    },
    "id": "yZQ78b2Kxw-T"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7efa9d692af0>"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['masked_acc'], label='accuracy')\n",
    "plt.plot(history.history['val_masked_acc'], label='val_accuracy')\n",
    "plt.ylim([0, max(plt.ylim())])\n",
    "plt.xlabel('Epoch #')\n",
    "plt.ylabel('CE/token')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T06:25:41.387953Z",
     "iopub.status.busy": "2022-12-14T06:25:41.387392Z",
     "iopub.status.idle": "2022-12-14T06:25:43.254154Z",
     "shell.execute_reply": "2022-12-14T06:25:43.253412Z"
    },
    "id": "9Psd1quzaAWg"
   },
   "outputs": [],
   "source": [
    "image_url = 'https://tensorflow.org/images/bedroom_hrnet_tutorial.jpg'\n",
    "image_path = tf.keras.utils.get_file(origin=image_url)\n",
    "image = load_image(image_path)\n",
    "\n",
    "run_and_show_attention(model, image)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "image_captioning.ipynb",
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
