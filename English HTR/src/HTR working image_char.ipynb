{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5bwwk4uxRz6A"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T06:19:48.106082Z",
     "iopub.status.busy": "2022-12-14T06:19:48.105426Z",
     "iopub.status.idle": "2022-12-14T06:19:50.981603Z",
     "shell.execute_reply": "2022-12-14T06:19:50.980658Z"
    },
    "id": "2R1hQGtZEi8Y"
   },
   "outputs": [],
   "source": [
    "# !pip uninstall -y tensorflow estimator keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T06:19:50.986279Z",
     "iopub.status.busy": "2022-12-14T06:19:50.985537Z",
     "iopub.status.idle": "2022-12-14T06:20:13.341457Z",
     "shell.execute_reply": "2022-12-14T06:20:13.340544Z"
    },
    "id": "5Xbt8BkPv8Ou"
   },
   "outputs": [],
   "source": [
    "# !pip install -U tensorflow_text tensorflow tensorflow_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T06:20:13.346822Z",
     "iopub.status.busy": "2022-12-14T06:20:13.346150Z",
     "iopub.status.idle": "2022-12-14T06:20:15.164209Z",
     "shell.execute_reply": "2022-12-14T06:20:15.163198Z"
    },
    "id": "7TGZmOuqMia9"
   },
   "outputs": [],
   "source": [
    "# !pip install einops tensorflow_hub tensorflow_text tensorflow_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-09 17:39:09.980715: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-02-09 17:39:10.812604: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-02-09 17:39:10.812671: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-02-09 17:39:10.812678: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-09 17:39:11.600232: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-02-09 17:39:11.637807: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-02-09 17:39:11.638025: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-02-09 17:39:11.638693: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-02-09 17:39:11.639459: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-02-09 17:39:11.639700: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-02-09 17:39:11.639871: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-02-09 17:39:12.168975: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-02-09 17:39:12.169143: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-02-09 17:39:12.169272: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-02-09 17:39:12.169376: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4382 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2080 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  try:\n",
    "    # Currently, memory growth needs to be the same across GPUs\n",
    "    for gpu in gpus:\n",
    "      tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    logical_gpus = tf.config.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "  except RuntimeError as e:\n",
    "    # Memory growth must be set before GPUs have been initialized\n",
    "    print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "form",
    "execution": {
     "iopub.execute_input": "2022-12-14T06:20:15.169125Z",
     "iopub.status.busy": "2022-12-14T06:20:15.168416Z",
     "iopub.status.idle": "2022-12-14T06:20:17.929094Z",
     "shell.execute_reply": "2022-12-14T06:20:17.928420Z"
    },
    "id": "U8l4RJ0XRPEm"
   },
   "outputs": [],
   "source": [
    "#@title\n",
    "import concurrent.futures\n",
    "import collections\n",
    "import dataclasses\n",
    "import hashlib\n",
    "import itertools\n",
    "import json\n",
    "import math\n",
    "import os\n",
    "import pathlib\n",
    "import random\n",
    "import re\n",
    "import string\n",
    "import time\n",
    "import urllib.request\n",
    "\n",
    "import einops\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import requests\n",
    "import tqdm\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text as text\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "\n",
    "size = dict()\n",
    "steps = dict()\n",
    "dataset = dict()\n",
    "path=\"/home/shashank/Desktop/new trials/CBAM with spatial (copy)/data/iam.hdf5\"\n",
    "with h5py.File(path, \"r\") as f:\n",
    "    for pt in ['train','test','valid']:\n",
    "        dataset[pt] = dict()\n",
    "        dataset[pt]['dt'] = np.stack([f[pt]['dt'], f[pt]['dt'], f[pt]['dt']], axis=3)\n",
    "        dataset[pt]['gt'] = np.array(f[pt]['gt'])\n",
    "\n",
    "        size[pt] = len(dataset[pt]['gt'])\n",
    "        steps[pt] = int(np.ceil(size[pt] / 16))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([b'A MOVE to stop Mr. Gaitskell from',\n",
       "       b'nominating any more Labour life Peers',\n",
       "       b'is to be made at a meeting of Labour', ...,\n",
       "       b\"about it . ' And Philip said : ' But we 've got\",\n",
       "       b\"to think about it , don't you see , because\",\n",
       "       b\"if we don't it 'll just go on and on , don't\"], dtype=object)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train']['gt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6161, 1024, 128, 3)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train']['dt'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-09 17:39:26.201969: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 2422603776 exceeds 10% of free system memory.\n",
      "2023-02-09 17:39:27.652713: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 2422603776 exceeds 10% of free system memory.\n"
     ]
    }
   ],
   "source": [
    "train_raw= tf.data.Dataset.from_tensor_slices((dataset['train']['dt'], dataset['train']['gt']))\n",
    "test_raw= tf.data.Dataset.from_tensor_slices((dataset['test']['dt'], dataset['test']['gt']))\n",
    "valid_raw= tf.data.Dataset.from_tensor_slices((dataset['valid']['dt'], dataset['valid']['gt']))\n",
    "# ds=dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds=train_raw\n",
    "test_ds=test_raw\n",
    "valid_ds=valid_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T06:21:26.360704Z",
     "iopub.status.busy": "2022-12-14T06:21:26.360422Z",
     "iopub.status.idle": "2022-12-14T06:21:26.366963Z",
     "shell.execute_reply": "2022-12-14T06:21:26.366327Z"
    },
    "id": "sAQSps5F8RQI"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorSpec(shape=(1024, 128, 3), dtype=tf.uint8, name=None),\n",
       " TensorSpec(shape=(), dtype=tf.string, name=None))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds.element_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TakeDataset element_spec=(TensorSpec(shape=(1024, 128, 3), dtype=tf.uint8, name=None), TensorSpec(shape=(), dtype=tf.string, name=None))>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8cSW4u-ORPFQ"
   },
   "source": [
    "### Image feature extractor\n",
    "\n",
    "You will use an image model (pretrained on imagenet) to extract the features from each image. The model was trained as an image classifier, but setting `include_top=False` returns the model without the final classification layer, so you can use the last layer of feature-maps:  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dojkiou9gL3R"
   },
   "source": [
    "Here's a function to load an image and resize it for the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not 224. Weights for input shape (224, 224) will be loaded as the default.\n"
     ]
    }
   ],
   "source": [
    "IMAGE_SHAPE=(1024, 128, 3)\n",
    "mobilenet = tf.keras.applications.MobileNetV3Small(\n",
    "    input_shape=IMAGE_SHAPE,\n",
    "    include_top=False,\n",
    "    include_preprocessing=False)\n",
    "mobilenet.trainable=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T06:21:26.370769Z",
     "iopub.status.busy": "2022-12-14T06:21:26.370105Z",
     "iopub.status.idle": "2022-12-14T06:21:27.881918Z",
     "shell.execute_reply": "2022-12-14T06:21:27.881104Z"
    },
    "id": "xIa0ZaP4tBez"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-09 17:39:31.995562: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:428] Loaded cuDNN version 8400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 32, 4, 576)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-09 17:39:32.500356: W tensorflow/compiler/xla/stream_executor/gpu/asm_compiler.cc:115] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "You may not need to update to CUDA 11.1; cherry-picking the ptxas binary is often sufficient.\n"
     ]
    }
   ],
   "source": [
    "for img, label in test_ds.take(1):\n",
    "    print(mobilenet(tf.expand_dims(img, axis=0)).shape)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nyqH3zFwRPFi"
   },
   "source": [
    "### Setup the text tokenizer/vectorizer\n",
    "\n",
    "You will transform the text captions into integer sequences using the [TextVectorization](https://www.tensorflow.org/api_docs/python/tf/keras/layers/TextVectorization) layer, with the following steps:\n",
    "\n",
    "* Use [adapt](https://www.tensorflow.org/api_docs/python/tf/keras/layers/TextVectorization#adapt) to iterate over all captions, split the captions into words, and compute a vocabulary of the top words.\n",
    "* Tokenize all captions by mapping each word to its index in the vocabulary. All output sequences will be padded to length 50.\n",
    "* Create word-to-index and index-to-word mappings to display results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_text_length=128\n",
    "charset = string.printable[:95]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "from itertools import groupby\n",
    "class Tokenizer():\n",
    "    \"\"\"Manager tokens functions and charset/dictionary properties\"\"\"\n",
    "\n",
    "    def __init__(self, chars, max_text_length=128):\n",
    "        self.PAD_TK, self.UNK_TK = \"¶\", \"¤\"\n",
    "        self.chars = (self.PAD_TK + self.UNK_TK + chars)\n",
    "\n",
    "        self.PAD = self.chars.find(self.PAD_TK)\n",
    "        self.UNK = self.chars.find(self.UNK_TK)\n",
    "        \n",
    "        self.START = len(self.chars)+1\n",
    "        self.END = len(self.chars)+2\n",
    "\n",
    "        self.vocab_size = len(self.chars)\n",
    "        self.maxlen = max_text_length\n",
    "\n",
    "    def encode(self, text):\n",
    "        \"\"\"Encode text to vector\"\"\"\n",
    "\n",
    "        if isinstance(text, bytes):\n",
    "            text = text.decode()\n",
    "\n",
    "        text = unicodedata.normalize(\"NFKD\", text).encode(\"ASCII\", \"ignore\").decode(\"ASCII\")\n",
    "        text = \" \".join(text.split())\n",
    "\n",
    "        groups = [\"\".join(group) for _, group in groupby(text)]\n",
    "        text = \"\".join([self.UNK_TK.join(list(x)) if len(x) > 1 else x for x in groups])\n",
    "        encoded = []\n",
    "\n",
    "        for item in text:\n",
    "            index = self.chars.find(item)\n",
    "            index = self.UNK if index == -1 else index\n",
    "            encoded.append(index)\n",
    "        \n",
    "        encoded = [self.START]+encoded+[self.END]\n",
    "        encoded = np.asarray(encoded)\n",
    "        \n",
    "        encoded = np.pad(encoded, (0,self.maxlen-len(encoded)), mode='constant')\n",
    "        \n",
    "        return encoded\n",
    "\n",
    "    def decode(self, text):\n",
    "        \"\"\"Decode vector to text\"\"\"\n",
    "\n",
    "        decoded = \"\".join([self.chars[int(x)] for x in text if x > -1])\n",
    "        decoded = self.remove_tokens(decoded)\n",
    "\n",
    "        return decoded\n",
    "\n",
    "    def remove_tokens(self, text):\n",
    "        \"\"\"Remove tokens (PAD) from text\"\"\"\n",
    "\n",
    "        return text.replace(self.PAD_TK, \"\").replace(self.UNK_TK, \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyTokenizer(tf.keras.layers.Layer):\n",
    "    def __init__(self, charset):\n",
    "        super(MyTokenizer, self).__init__()\n",
    "        self.tokenizer = Tokenizer(charset)\n",
    "\n",
    "    def call(self, inputs):\n",
    "#         inputs = inputs.numpy()\n",
    "        tokens=[]\n",
    "        for line in inputs:\n",
    "            tokens.append(self.tokenizer.encode(line))\n",
    "        return tf.constant(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = MyTokenizer(charset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Exception encountered when calling layer 'my_tokenizer_1' (type MyTokenizer).\n\nnormalize() argument 2 must be str, not tensorflow.python.framework.ops.EagerTensor\n\nCall arguments received by layer 'my_tokenizer_1' (type MyTokenizer):\n  • inputs=tf.Tensor(shape=(1,), dtype=string)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [86]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconstant\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43ma cat in a hat\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/htr2/lib/python3.8/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "Input \u001b[0;32mIn [84]\u001b[0m, in \u001b[0;36mMyTokenizer.call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m      8\u001b[0m tokens\u001b[38;5;241m=\u001b[39m[]\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m inputs:\n\u001b[0;32m---> 10\u001b[0m     tokens\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mline\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mconstant(tokens)\n",
      "Input \u001b[0;32mIn [60]\u001b[0m, in \u001b[0;36mTokenizer.encode\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(text, \u001b[38;5;28mbytes\u001b[39m):\n\u001b[1;32m     23\u001b[0m     text \u001b[38;5;241m=\u001b[39m text\u001b[38;5;241m.\u001b[39mdecode()\n\u001b[0;32m---> 25\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[43municodedata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormalize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mNFKD\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mencode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mASCII\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mASCII\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     26\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(text\u001b[38;5;241m.\u001b[39msplit())\n\u001b[1;32m     28\u001b[0m groups \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(group) \u001b[38;5;28;01mfor\u001b[39;00m _, group \u001b[38;5;129;01min\u001b[39;00m groupby(text)]\n",
      "\u001b[0;31mTypeError\u001b[0m: Exception encountered when calling layer 'my_tokenizer_1' (type MyTokenizer).\n\nnormalize() argument 2 must be str, not tensorflow.python.framework.ops.EagerTensor\n\nCall arguments received by layer 'my_tokenizer_1' (type MyTokenizer):\n  • inputs=tf.Tensor(shape=(1,), dtype=string)"
     ]
    }
   ],
   "source": [
    "tokenizer(tf.constant(['a cat in a hat',]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels=dataset['train']['gt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([b'A MOVE to stop Mr. Gaitskell from',\n",
       "       b'nominating any more Labour life Peers',\n",
       "       b'is to be made at a meeting of Labour', ...,\n",
       "       b\"about it . ' And Philip said : ' But we 've got\",\n",
       "       b\"to think about it , don't you see , because\",\n",
       "       b\"if we don't it 'll just go on and on , don't\"], dtype=object)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels= np.concatenate(dataset['train']['gt'], dataset['test']['gt'])\n",
    "# labels= np.concatenate(labels, dataset['valid']['gt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6161,)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T06:21:30.677454Z",
     "iopub.status.busy": "2022-12-14T06:21:30.676721Z",
     "iopub.status.idle": "2022-12-14T06:21:32.338147Z",
     "shell.execute_reply": "2022-12-14T06:21:32.337359Z"
    },
    "id": "oJGE34aiRPFo"
   },
   "outputs": [],
   "source": [
    "# tokenizer.adapt(train_raw.map(lambda fp,txt: txt).unbatch().batch(1024))\n",
    "# tokenizer.adapt(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T06:21:32.342623Z",
     "iopub.status.busy": "2022-12-14T06:21:32.341980Z",
     "iopub.status.idle": "2022-12-14T06:21:32.354940Z",
     "shell.execute_reply": "2022-12-14T06:21:32.354326Z"
    },
    "id": "oRahTDtWhJIf"
   },
   "outputs": [],
   "source": [
    "# tokenizer.get_vocabulary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T06:21:32.358115Z",
     "iopub.status.busy": "2022-12-14T06:21:32.357629Z",
     "iopub.status.idle": "2022-12-14T06:21:32.395154Z",
     "shell.execute_reply": "2022-12-14T06:21:32.394516Z"
    },
    "id": "-2mGxD33JCxN"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 128), dtype=int32, numpy=\n",
       "array([[98, 12, 96, 14, 12, 31, 96, 20, 25, 96, 12, 96, 19, 12, 31, 99,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [98, 12, 96, 29, 26, 13, 26, 31, 96, 15, 26, 18, 99,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]],\n",
       "      dtype=int32)>"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = tokenizer(tf.constant(['a cat in a hat', 'a robot dog']))\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T06:21:32.398431Z",
     "iopub.status.busy": "2022-12-14T06:21:32.398157Z",
     "iopub.status.idle": "2022-12-14T06:21:32.477652Z",
     "shell.execute_reply": "2022-12-14T06:21:32.477042Z"
    },
    "id": "8Q44tNQVRPFt"
   },
   "outputs": [],
   "source": [
    "# # Create mappings for words to indices and indices to words.\n",
    "# word_to_index = tf.keras.layers.StringLookup(\n",
    "#     mask_token=\"\",\n",
    "#     vocabulary=tokenizer.get_vocabulary())\n",
    "# index_to_word = tf.keras.layers.StringLookup(\n",
    "#     mask_token=\"\",\n",
    "#     vocabulary=tokenizer.get_vocabulary(),\n",
    "#     invert=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T06:21:32.481676Z",
     "iopub.status.busy": "2022-12-14T06:21:32.480953Z",
     "iopub.status.idle": "2022-12-14T06:21:32.491390Z",
     "shell.execute_reply": "2022-12-14T06:21:32.490750Z"
    },
    "id": "qo-cfCX3LnHs"
   },
   "outputs": [],
   "source": [
    "# w = index_to_word(t)\n",
    "# w.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T06:21:32.494835Z",
     "iopub.status.busy": "2022-12-14T06:21:32.494362Z",
     "iopub.status.idle": "2022-12-14T06:21:32.539069Z",
     "shell.execute_reply": "2022-12-14T06:21:32.538476Z"
    },
    "id": "rrUUfGc65vAT"
   },
   "outputs": [],
   "source": [
    "# tf.strings.reduce_join(w, separator=' ', axis=-1).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uEWM9xrYcg45"
   },
   "source": [
    "### Prepare the datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6aX0Z_98S2tN"
   },
   "source": [
    "The `train_raw` and `test_raw` datasets contain 1:many `(image, captions)` pairs. \n",
    "\n",
    "This function will replicate the image so there are 1:1 images to captions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T06:21:32.549819Z",
     "iopub.status.busy": "2022-12-14T06:21:32.549230Z",
     "iopub.status.idle": "2022-12-14T06:21:33.950287Z",
     "shell.execute_reply": "2022-12-14T06:21:33.949562Z"
    },
    "id": "CZGUsuGzUfzt"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images: (32, 1024, 128, 3)\n",
      "labels: (32,)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for images, labels in test_ds.batch(32).take(1):\n",
    "  break\n",
    "\n",
    "print('images:', images.shape)\n",
    "print('labels:', labels.shape)\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8ENR_-swVhnm"
   },
   "source": [
    "To be compatible with keras training the dataset should contain `(inputs, labels)` pairs. For text generation the tokens are both an input and the labels, shifted by one step. This function will convert an `(images, texts)` pair to an `((images, input_tokens), label_tokens)` pair:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T06:21:33.953963Z",
     "iopub.status.busy": "2022-12-14T06:21:33.953470Z",
     "iopub.status.idle": "2022-12-14T06:21:33.957391Z",
     "shell.execute_reply": "2022-12-14T06:21:33.956716Z"
    },
    "id": "2DsgQ_hZT4C2"
   },
   "outputs": [],
   "source": [
    "def prepare_txt(imgs, txts):\n",
    "  print(txts)\n",
    "  tokens = tokenizer(txts)\n",
    "\n",
    "  input_tokens = tokens[..., :-1]\n",
    "  label_tokens = tokens[..., 1:]\n",
    "  return (imgs, input_tokens), label_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DA1x2j0JXX-N"
   },
   "source": [
    "This function adds operations to a dataset. The steps are:\n",
    "\n",
    "1. Load the images (and ignore images that fail to load).\n",
    "2. Replicate images to match the number of captions.\n",
    "3. Shuffle and rebatch the `image, caption` pairs.\n",
    "4. Tokenize the text, shift the tokens and add `label_tokens`.\n",
    "5. Convert the text from a `RaggedTensor` representation to padded dense `Tensor` representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T06:21:33.960764Z",
     "iopub.status.busy": "2022-12-14T06:21:33.960216Z",
     "iopub.status.idle": "2022-12-14T06:21:33.965468Z",
     "shell.execute_reply": "2022-12-14T06:21:33.964820Z"
    },
    "id": "4_Pt9zldjQ0q"
   },
   "outputs": [],
   "source": [
    "def prepare_dataset(ds, tokenizer, batch_size=32, shuffle_buffer=1000):\n",
    "  # Load the images and make batches.\n",
    "  ds = (ds\n",
    "        .shuffle(1000)\n",
    "        .apply(tf.data.experimental.ignore_errors())\n",
    "        .batch(batch_size))\n",
    "\n",
    "  def to_tensor(inputs, labels):\n",
    "    (images, in_tok), out_tok = inputs, labels\n",
    "    return (images, in_tok), out_tok\n",
    "\n",
    "  return (ds\n",
    "          .unbatch()\n",
    "          .shuffle(shuffle_buffer)\n",
    "          .batch(batch_size)\n",
    "          .map(prepare_txt, tf.data.AUTOTUNE)\n",
    "          .map(to_tensor, tf.data.AUTOTUNE)\n",
    "          )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LrQ85t1GNfpQ"
   },
   "source": [
    "You could install the feature extractor in your model and train on the datasets like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T06:21:33.968626Z",
     "iopub.status.busy": "2022-12-14T06:21:33.968216Z",
     "iopub.status.idle": "2022-12-14T06:21:35.058930Z",
     "shell.execute_reply": "2022-12-14T06:21:35.058273Z"
    },
    "id": "1KlhOG5cjQ0r"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"args_1:0\", shape=(None,), dtype=string)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "in user code:\n\n    File \"/tmp/ipykernel_747509/4005401010.py\", line 3, in prepare_txt  *\n        tokens = tokenizer(txts)\n    File \"/home/shashank/anaconda3/envs/htr2/lib/python3.8/site-packages/keras/utils/traceback_utils.py\", line 70, in error_handler  **\n        raise e.with_traceback(filtered_tb) from None\n    File \"/tmp/__autograph_generated_filey8hq9rzq.py\", line 10, in tf__call\n        inputs = ag__.converted_call(ag__.ld(inputs).numpy, (), None, fscope)\n\n    AttributeError: Exception encountered when calling layer 'my_tokenizer' (type MyTokenizer).\n    \n    in user code:\n    \n        File \"/tmp/ipykernel_747509/883610391.py\", line 7, in call  *\n            inputs = inputs.numpy()\n    \n        AttributeError: 'Tensor' object has no attribute 'numpy'\n    \n    \n    Call arguments received by layer 'my_tokenizer' (type MyTokenizer):\n      • inputs=tf.Tensor(shape=(None,), dtype=string)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [83]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m temp \u001b[38;5;241m=\u001b[39m \u001b[43mprepare_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_ds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m temp\u001b[38;5;241m.\u001b[39melement_spec\n",
      "Input \u001b[0;32mIn [82]\u001b[0m, in \u001b[0;36mprepare_dataset\u001b[0;34m(ds, tokenizer, batch_size, shuffle_buffer)\u001b[0m\n\u001b[1;32m      9\u001b[0m   (images, in_tok), out_tok \u001b[38;5;241m=\u001b[39m inputs, labels\n\u001b[1;32m     10\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m (images, in_tok), out_tok\n\u001b[0;32m---> 12\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[43mds\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munbatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshuffle\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshuffle_buffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprepare_txt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mAUTOTUNE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m         \u001b[38;5;241m.\u001b[39mmap(to_tensor, tf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mAUTOTUNE)\n\u001b[1;32m     18\u001b[0m         )\n",
      "File \u001b[0;32m~/anaconda3/envs/htr2/lib/python3.8/site-packages/tensorflow/python/data/ops/dataset_ops.py:2296\u001b[0m, in \u001b[0;36mDatasetV2.map\u001b[0;34m(self, map_func, num_parallel_calls, deterministic, name)\u001b[0m\n\u001b[1;32m   2294\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m MapDataset(\u001b[38;5;28mself\u001b[39m, map_func, preserve_cardinality\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, name\u001b[38;5;241m=\u001b[39mname)\n\u001b[1;32m   2295\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2296\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mParallelMapDataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2297\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2298\u001b[0m \u001b[43m      \u001b[49m\u001b[43mmap_func\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2299\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_parallel_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2300\u001b[0m \u001b[43m      \u001b[49m\u001b[43mdeterministic\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2301\u001b[0m \u001b[43m      \u001b[49m\u001b[43mpreserve_cardinality\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   2302\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/htr2/lib/python3.8/site-packages/tensorflow/python/data/ops/dataset_ops.py:5540\u001b[0m, in \u001b[0;36mParallelMapDataset.__init__\u001b[0;34m(self, input_dataset, map_func, num_parallel_calls, deterministic, use_inter_op_parallelism, preserve_cardinality, use_legacy_function, name)\u001b[0m\n\u001b[1;32m   5538\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_input_dataset \u001b[38;5;241m=\u001b[39m input_dataset\n\u001b[1;32m   5539\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_use_inter_op_parallelism \u001b[38;5;241m=\u001b[39m use_inter_op_parallelism\n\u001b[0;32m-> 5540\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_map_func \u001b[38;5;241m=\u001b[39m \u001b[43mstructured_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mStructuredFunctionWrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   5541\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmap_func\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5542\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_transformation_name\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5543\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5544\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_legacy_function\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_legacy_function\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   5545\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m deterministic \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   5546\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_deterministic \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdefault\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/htr2/lib/python3.8/site-packages/tensorflow/python/data/ops/structured_function.py:263\u001b[0m, in \u001b[0;36mStructuredFunctionWrapper.__init__\u001b[0;34m(self, func, transformation_name, dataset, input_classes, input_shapes, input_types, input_structure, add_to_graph, use_legacy_function, defun_kwargs)\u001b[0m\n\u001b[1;32m    256\u001b[0m       warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    257\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEven though the `tf.config.experimental_run_functions_eagerly` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    258\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moption is set, this option does not apply to tf.data functions. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    259\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTo force eager execution of tf.data functions, please use \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    260\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`tf.data.experimental.enable_debug_mode()`.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    261\u001b[0m     fn_factory \u001b[38;5;241m=\u001b[39m trace_tf_function(defun_kwargs)\n\u001b[0;32m--> 263\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_function \u001b[38;5;241m=\u001b[39m \u001b[43mfn_factory\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# There is no graph to add in eager mode.\u001b[39;00m\n\u001b[1;32m    265\u001b[0m add_to_graph \u001b[38;5;241m&\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m context\u001b[38;5;241m.\u001b[39mexecuting_eagerly()\n",
      "File \u001b[0;32m~/anaconda3/envs/htr2/lib/python3.8/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:226\u001b[0m, in \u001b[0;36mTracingCompiler.get_concrete_function\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_concrete_function\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    218\u001b[0m   \u001b[38;5;124;03m\"\"\"Returns a `ConcreteFunction` specialized to inputs and execution context.\u001b[39;00m\n\u001b[1;32m    219\u001b[0m \n\u001b[1;32m    220\u001b[0m \u001b[38;5;124;03m  Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    224\u001b[0m \u001b[38;5;124;03m      `tf.Tensor` or `tf.TensorSpec`.\u001b[39;00m\n\u001b[1;32m    225\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> 226\u001b[0m   concrete_function \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_concrete_function_garbage_collected\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    227\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    228\u001b[0m   concrete_function\u001b[38;5;241m.\u001b[39m_garbage_collector\u001b[38;5;241m.\u001b[39mrelease()  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m    229\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m concrete_function\n",
      "File \u001b[0;32m~/anaconda3/envs/htr2/lib/python3.8/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:192\u001b[0m, in \u001b[0;36mTracingCompiler._get_concrete_function_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    189\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_function_spec\u001b[38;5;241m.\u001b[39mvalidate_inputs_with_signature(args, kwargs)\n\u001b[1;32m    191\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m--> 192\u001b[0m   concrete_function, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_maybe_define_concrete_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    193\u001b[0m   seen_names \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n\u001b[1;32m    194\u001b[0m   captured \u001b[38;5;241m=\u001b[39m object_identity\u001b[38;5;241m.\u001b[39mObjectIdentitySet(\n\u001b[1;32m    195\u001b[0m       concrete_function\u001b[38;5;241m.\u001b[39mgraph\u001b[38;5;241m.\u001b[39minternal_captures)\n",
      "File \u001b[0;32m~/anaconda3/envs/htr2/lib/python3.8/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:157\u001b[0m, in \u001b[0;36mTracingCompiler._maybe_define_concrete_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m    154\u001b[0m   args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_signature\n\u001b[1;32m    155\u001b[0m   kwargs \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m--> 157\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_maybe_define_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/htr2/lib/python3.8/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:360\u001b[0m, in \u001b[0;36mTracingCompiler._maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m    357\u001b[0m   \u001b[38;5;66;03m# Only get placeholders for arguments, not captures\u001b[39;00m\n\u001b[1;32m    358\u001b[0m   args, kwargs \u001b[38;5;241m=\u001b[39m generalized_func_key\u001b[38;5;241m.\u001b[39m_placeholder_value()  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m--> 360\u001b[0m concrete_function \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_concrete_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    362\u001b[0m graph_capture_container \u001b[38;5;241m=\u001b[39m concrete_function\u001b[38;5;241m.\u001b[39mgraph\u001b[38;5;241m.\u001b[39m_capture_func_lib  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m    363\u001b[0m \u001b[38;5;66;03m# Maintain the list of all captures\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/htr2/lib/python3.8/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:284\u001b[0m, in \u001b[0;36mTracingCompiler._create_concrete_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m    279\u001b[0m missing_arg_names \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    280\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (arg, i) \u001b[38;5;28;01mfor\u001b[39;00m i, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(missing_arg_names)\n\u001b[1;32m    281\u001b[0m ]\n\u001b[1;32m    282\u001b[0m arg_names \u001b[38;5;241m=\u001b[39m base_arg_names \u001b[38;5;241m+\u001b[39m missing_arg_names\n\u001b[1;32m    283\u001b[0m concrete_function \u001b[38;5;241m=\u001b[39m monomorphic_function\u001b[38;5;241m.\u001b[39mConcreteFunction(\n\u001b[0;32m--> 284\u001b[0m     \u001b[43mfunc_graph_module\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc_graph_from_py_func\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    285\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    286\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_python_function\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    287\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    288\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    289\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m        \u001b[49m\u001b[43mautograph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_autograph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m        \u001b[49m\u001b[43mautograph_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_autograph_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m        \u001b[49m\u001b[43marg_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43marg_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapture_by_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_capture_by_value\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m    294\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_function_attributes,\n\u001b[1;32m    295\u001b[0m     spec\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_spec,\n\u001b[1;32m    296\u001b[0m     \u001b[38;5;66;03m# Tell the ConcreteFunction to clean up its graph once it goes out of\u001b[39;00m\n\u001b[1;32m    297\u001b[0m     \u001b[38;5;66;03m# scope. This is not the default behavior since it gets used in some\u001b[39;00m\n\u001b[1;32m    298\u001b[0m     \u001b[38;5;66;03m# places (like Keras) where the FuncGraph lives longer than the\u001b[39;00m\n\u001b[1;32m    299\u001b[0m     \u001b[38;5;66;03m# ConcreteFunction.\u001b[39;00m\n\u001b[1;32m    300\u001b[0m     shared_func_graph\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    301\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m concrete_function\n",
      "File \u001b[0;32m~/anaconda3/envs/htr2/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py:1283\u001b[0m, in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, acd_record_initial_resource_uses)\u001b[0m\n\u001b[1;32m   1280\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1281\u001b[0m   _, original_func \u001b[38;5;241m=\u001b[39m tf_decorator\u001b[38;5;241m.\u001b[39munwrap(python_func)\n\u001b[0;32m-> 1283\u001b[0m func_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mpython_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfunc_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfunc_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1285\u001b[0m \u001b[38;5;66;03m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[39;00m\n\u001b[1;32m   1286\u001b[0m \u001b[38;5;66;03m# TensorArrays and `None`s.\u001b[39;00m\n\u001b[1;32m   1287\u001b[0m func_outputs \u001b[38;5;241m=\u001b[39m variable_utils\u001b[38;5;241m.\u001b[39mconvert_variables_to_tensors(func_outputs)\n",
      "File \u001b[0;32m~/anaconda3/envs/htr2/lib/python3.8/site-packages/tensorflow/python/data/ops/structured_function.py:240\u001b[0m, in \u001b[0;36mStructuredFunctionWrapper.__init__.<locals>.trace_tf_function.<locals>.wrapped_fn\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    234\u001b[0m \u001b[38;5;129m@eager_function\u001b[39m\u001b[38;5;241m.\u001b[39mdefun_with_attributes(\n\u001b[1;32m    235\u001b[0m     input_signature\u001b[38;5;241m=\u001b[39mstructure\u001b[38;5;241m.\u001b[39mget_flat_tensor_specs(\n\u001b[1;32m    236\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_input_structure),\n\u001b[1;32m    237\u001b[0m     autograph\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    238\u001b[0m     attributes\u001b[38;5;241m=\u001b[39mdefun_kwargs)\n\u001b[1;32m    239\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped_fn\u001b[39m(\u001b[38;5;241m*\u001b[39margs):  \u001b[38;5;66;03m# pylint: disable=missing-docstring\u001b[39;00m\n\u001b[0;32m--> 240\u001b[0m   ret \u001b[38;5;241m=\u001b[39m \u001b[43mwrapper_helper\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    241\u001b[0m   ret \u001b[38;5;241m=\u001b[39m structure\u001b[38;5;241m.\u001b[39mto_tensor_list(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_structure, ret)\n\u001b[1;32m    242\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m [ops\u001b[38;5;241m.\u001b[39mconvert_to_tensor(t) \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m ret]\n",
      "File \u001b[0;32m~/anaconda3/envs/htr2/lib/python3.8/site-packages/tensorflow/python/data/ops/structured_function.py:171\u001b[0m, in \u001b[0;36mStructuredFunctionWrapper.__init__.<locals>.wrapper_helper\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _should_unpack(nested_args):\n\u001b[1;32m    170\u001b[0m   nested_args \u001b[38;5;241m=\u001b[39m (nested_args,)\n\u001b[0;32m--> 171\u001b[0m ret \u001b[38;5;241m=\u001b[39m \u001b[43mautograph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtf_convert\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mag_ctx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mnested_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    172\u001b[0m ret \u001b[38;5;241m=\u001b[39m variable_utils\u001b[38;5;241m.\u001b[39mconvert_variables_to_tensors(ret)\n\u001b[1;32m    173\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _should_pack(ret):\n",
      "File \u001b[0;32m~/anaconda3/envs/htr2/lib/python3.8/site-packages/tensorflow/python/autograph/impl/api.py:692\u001b[0m, in \u001b[0;36mconvert.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint:disable=broad-except\u001b[39;00m\n\u001b[1;32m    691\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(e, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mag_error_metadata\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m--> 692\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mag_error_metadata\u001b[38;5;241m.\u001b[39mto_exception(e)\n\u001b[1;32m    693\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    694\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/htr2/lib/python3.8/site-packages/tensorflow/python/autograph/impl/api.py:689\u001b[0m, in \u001b[0;36mconvert.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    687\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    688\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m conversion_ctx:\n\u001b[0;32m--> 689\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconverted_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    690\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint:disable=broad-except\u001b[39;00m\n\u001b[1;32m    691\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(e, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mag_error_metadata\u001b[39m\u001b[38;5;124m'\u001b[39m):\n",
      "File \u001b[0;32m~/anaconda3/envs/htr2/lib/python3.8/site-packages/tensorflow/python/autograph/impl/api.py:439\u001b[0m, in \u001b[0;36mconverted_call\u001b[0;34m(f, args, kwargs, caller_fn_scope, options)\u001b[0m\n\u001b[1;32m    437\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    438\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 439\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mconverted_f\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43meffective_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    440\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    441\u001b[0m     result \u001b[38;5;241m=\u001b[39m converted_f(\u001b[38;5;241m*\u001b[39meffective_args)\n",
      "File \u001b[0;32m/tmp/__autograph_generated_filelwoim25c.py:11\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__prepare_txt\u001b[0;34m(imgs, txts)\u001b[0m\n\u001b[1;32m      9\u001b[0m retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mUndefinedReturnValue()\n\u001b[1;32m     10\u001b[0m ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mprint\u001b[39m)(ag__\u001b[38;5;241m.\u001b[39mld(txts))\n\u001b[0;32m---> 11\u001b[0m tokens \u001b[38;5;241m=\u001b[39m \u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconverted_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtxts\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfscope\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m input_tokens \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mld(tokens)[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, :(\u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m)]\n\u001b[1;32m     13\u001b[0m label_tokens \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mld(tokens)[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, \u001b[38;5;241m1\u001b[39m:]\n",
      "File \u001b[0;32m~/anaconda3/envs/htr2/lib/python3.8/site-packages/tensorflow/python/autograph/impl/api.py:377\u001b[0m, in \u001b[0;36mconverted_call\u001b[0;34m(f, args, kwargs, caller_fn_scope, options)\u001b[0m\n\u001b[1;32m    374\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m _call_unconverted(f, args, kwargs, options)\n\u001b[1;32m    376\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m options\u001b[38;5;241m.\u001b[39muser_requested \u001b[38;5;129;01mand\u001b[39;00m conversion\u001b[38;5;241m.\u001b[39mis_allowlisted(f):\n\u001b[0;32m--> 377\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_call_unconverted\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    379\u001b[0m \u001b[38;5;66;03m# internal_convert_user_code is for example turned off when issuing a dynamic\u001b[39;00m\n\u001b[1;32m    380\u001b[0m \u001b[38;5;66;03m# call conversion from generated code while in nonrecursive mode. In that\u001b[39;00m\n\u001b[1;32m    381\u001b[0m \u001b[38;5;66;03m# case we evidently don't want to recurse, but we still have to convert\u001b[39;00m\n\u001b[1;32m    382\u001b[0m \u001b[38;5;66;03m# things like builtins.\u001b[39;00m\n\u001b[1;32m    383\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m options\u001b[38;5;241m.\u001b[39minternal_convert_user_code:\n",
      "File \u001b[0;32m~/anaconda3/envs/htr2/lib/python3.8/site-packages/tensorflow/python/autograph/impl/api.py:459\u001b[0m, in \u001b[0;36m_call_unconverted\u001b[0;34m(f, args, kwargs, options, update_cache)\u001b[0m\n\u001b[1;32m    457\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    458\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 459\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/htr2/lib/python3.8/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/tmp/__autograph_generated_filey8hq9rzq.py:10\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m      8\u001b[0m do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m      9\u001b[0m retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mUndefinedReturnValue()\n\u001b[0;32m---> 10\u001b[0m inputs \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(inputs)\u001b[38;5;241m.\u001b[39mnumpy, (), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[1;32m     11\u001b[0m tokens \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_state\u001b[39m():\n",
      "\u001b[0;31mAttributeError\u001b[0m: in user code:\n\n    File \"/tmp/ipykernel_747509/4005401010.py\", line 3, in prepare_txt  *\n        tokens = tokenizer(txts)\n    File \"/home/shashank/anaconda3/envs/htr2/lib/python3.8/site-packages/keras/utils/traceback_utils.py\", line 70, in error_handler  **\n        raise e.with_traceback(filtered_tb) from None\n    File \"/tmp/__autograph_generated_filey8hq9rzq.py\", line 10, in tf__call\n        inputs = ag__.converted_call(ag__.ld(inputs).numpy, (), None, fscope)\n\n    AttributeError: Exception encountered when calling layer 'my_tokenizer' (type MyTokenizer).\n    \n    in user code:\n    \n        File \"/tmp/ipykernel_747509/883610391.py\", line 7, in call  *\n            inputs = inputs.numpy()\n    \n        AttributeError: 'Tensor' object has no attribute 'numpy'\n    \n    \n    Call arguments received by layer 'my_tokenizer' (type MyTokenizer):\n      • inputs=tf.Tensor(shape=(None,), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "temp = prepare_dataset(train_ds, tokenizer)\n",
    "temp.element_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T06:21:35.062541Z",
     "iopub.status.busy": "2022-12-14T06:21:35.061878Z",
     "iopub.status.idle": "2022-12-14T06:21:35.187059Z",
     "shell.execute_reply": "2022-12-14T06:21:35.186405Z"
    },
    "id": "d7Zy9F3zX7i2"
   },
   "outputs": [],
   "source": [
    "# test_ds = prepare_dataset(test_ds, tokenizer)\n",
    "# test_ds.element_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# valid_ds = prepare_dataset(valid_ds, tokenizer)\n",
    "# valid_ds.element_spec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XZyKygJ8S8zW"
   },
   "source": [
    "### [Optional] Cache the image features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eHKhSKhti6NS"
   },
   "source": [
    "Since the image feature extractor is not changing, and this tutorial is not using image augmentation, the image features can be cached. Same for the text tokenization. The time it takes to set up the cache is earned back on each epoch during training and validation. The code below defines two functions `save_dataset` and `load_dataset`: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /tmp/ipykernel_744818/2955139511.py:2: ignore_errors (from tensorflow.python.data.experimental.ops.error_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Dataset.ignore_errors` instead.\n"
     ]
    }
   ],
   "source": [
    "tem_ds = (train_ds\n",
    "        .apply(tf.data.experimental.ignore_errors()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorSpec(shape=(1024, 128, 3), dtype=tf.uint8, name=None),\n",
       " TensorSpec(shape=(), dtype=tf.string, name=None))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tem_ds.element_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T06:21:35.190731Z",
     "iopub.status.busy": "2022-12-14T06:21:35.190053Z",
     "iopub.status.idle": "2022-12-14T06:21:35.197647Z",
     "shell.execute_reply": "2022-12-14T06:21:35.197087Z"
    },
    "id": "9N1MX5ym6xm5"
   },
   "outputs": [],
   "source": [
    "def save_dataset(ds, save_path, image_model, tokenizer, shards=10, batch_size=32):\n",
    "  # Load the images and make batches.\n",
    "  ds = (ds\n",
    "        .apply(tf.data.experimental.ignore_errors())\n",
    "        .batch(batch_size))\n",
    "\n",
    "  # Run the feature extractor on each batch\n",
    "  # Don't do this in a .map, because tf.data runs on the CPU. \n",
    "  def gen():\n",
    "    for (images, captions) in tqdm.tqdm(ds): \n",
    "      feature_maps = image_model(images)\n",
    "      yield feature_maps, captions\n",
    "\n",
    "  # Wrap the generator in a new tf.data.Dataset.\n",
    "  new_ds = tf.data.Dataset.from_generator(\n",
    "      gen,\n",
    "      output_signature=(\n",
    "          tf.TensorSpec(shape=image_model.output_shape),\n",
    "          tf.TensorSpec(shape=(None,), dtype=tf.string)))\n",
    "\n",
    "  # Apply the tokenization \n",
    "  new_ds = (new_ds\n",
    "            .map(prepare_txt, tf.data.AUTOTUNE)\n",
    "            .unbatch()\n",
    "            .shuffle(1000))\n",
    "\n",
    "  # Save the dataset into shard files.\n",
    "  def shard_func(i, item):\n",
    "    return i % shards\n",
    "  new_ds.enumerate().save(save_path, shard_func=shard_func)\n",
    "\n",
    "def load_dataset(save_path, batch_size=32, shuffle=1000, cycle_length=2):\n",
    "  def custom_reader_func(datasets):\n",
    "    datasets = datasets.shuffle(1000)\n",
    "    return datasets.interleave(lambda x: x, cycle_length=cycle_length)\n",
    "  \n",
    "  ds = tf.data.Dataset.load(save_path, reader_func=custom_reader_func)\n",
    "\n",
    "  def drop_index(i, x):\n",
    "    return x\n",
    "\n",
    "  ds = (ds\n",
    "        .map(drop_index, tf.data.AUTOTUNE)\n",
    "        .shuffle(shuffle)\n",
    "        .padded_batch(batch_size)\n",
    "        .prefetch(tf.data.AUTOTUNE))\n",
    "  return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T06:21:35.200578Z",
     "iopub.status.busy": "2022-12-14T06:21:35.200328Z",
     "iopub.status.idle": "2022-12-14T06:21:59.861212Z",
     "shell.execute_reply": "2022-12-14T06:21:59.860424Z"
    },
    "id": "tNdzrenxB3Yy"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-09 16:24:17.718468: I tensorflow/core/grappler/optimizers/data/replicate_on_split.cc:32] Running replicate on split optimization\n",
      "193it [00:12, 15.45it/s]\n"
     ]
    }
   ],
   "source": [
    "save_dataset(train_ds, 'train_cache', mobilenet, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-09 16:24:30.407996: I tensorflow/core/grappler/optimizers/data/replicate_on_split.cc:32] Running replicate on split optimization\n",
      "59it [00:03, 16.18it/s]\n"
     ]
    }
   ],
   "source": [
    "save_dataset(test_ds, 'test_cache', mobilenet, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-09 16:24:34.299782: I tensorflow/core/grappler/optimizers/data/replicate_on_split.cc:32] Running replicate on split optimization\n",
      "58it [00:03, 15.77it/s]\n"
     ]
    }
   ],
   "source": [
    "save_dataset(valid_ds, 'valid_cache', mobilenet, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "798DtfH51UI8"
   },
   "source": [
    " </section>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GI265LiDslr2"
   },
   "source": [
    "## Data ready for training\n",
    "\n",
    "After those preprocessing steps, here are the datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T06:21:59.865217Z",
     "iopub.status.busy": "2022-12-14T06:21:59.864657Z",
     "iopub.status.idle": "2022-12-14T06:21:59.962747Z",
     "shell.execute_reply": "2022-12-14T06:21:59.962022Z"
    },
    "id": "Pwic2YCjHZmV"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/shashank/anaconda3/envs/htr2/lib/python3.8/site-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
      "Instructions for updating:\n",
      "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n"
     ]
    }
   ],
   "source": [
    "train_ds = load_dataset('train_cache')\n",
    "test_ds = load_dataset('test_cache')\n",
    "valid_ds = load_dataset('valid_cache')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T06:21:59.966737Z",
     "iopub.status.busy": "2022-12-14T06:21:59.966108Z",
     "iopub.status.idle": "2022-12-14T06:21:59.970423Z",
     "shell.execute_reply": "2022-12-14T06:21:59.969839Z"
    },
    "id": "3B80JXj7HloX"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((TensorSpec(shape=(None, 32, 4, 576), dtype=tf.float32, name=None),\n",
       "  TensorSpec(shape=(None, 29), dtype=tf.int64, name=None)),\n",
       " TensorSpec(shape=(None, 29), dtype=tf.int64, name=None))"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds.element_spec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5jfb8qknlsKi"
   },
   "source": [
    "The dataset now returns `(input, label)` pairs suitable for training with keras. The `inputs` are `(images, input_tokens)` pairs. The `images` have been processed with the feature-extractor model. For each location in the `input_tokens` the model looks at the text so far and tries to predict the next which is lined up at the same location in the `labels`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T06:21:59.973709Z",
     "iopub.status.busy": "2022-12-14T06:21:59.973070Z",
     "iopub.status.idle": "2022-12-14T06:22:00.176197Z",
     "shell.execute_reply": "2022-12-14T06:22:00.175506Z"
    },
    "id": "YJBEwuXLZQdw"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 32, 4, 576)\n",
      "(32, 29)\n",
      "(32, 29)\n"
     ]
    }
   ],
   "source": [
    "for (inputs, ex_labels) in train_ds.take(1):\n",
    "    (ex_img, ex_in_tok) = inputs\n",
    "\n",
    "print(ex_img.shape)\n",
    "print(ex_in_tok.shape)\n",
    "print(ex_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "22R58DzZoF17"
   },
   "source": [
    "The input tokens and the labels are the same, just shifted by 1 step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T06:22:00.180095Z",
     "iopub.status.busy": "2022-12-14T06:22:00.179589Z",
     "iopub.status.idle": "2022-12-14T06:22:00.185425Z",
     "shell.execute_reply": "2022-12-14T06:22:00.184730Z"
    },
    "id": "V7h5UGftn1hT"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1  1 12  1  1  1  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0]\n"
     ]
    }
   ],
   "source": [
    "# print(ex_in_tok[0].numpy())\n",
    "print(ex_labels[0].numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DfICM49WFpIb"
   },
   "source": [
    "## A Transformer decoder model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ONyjuWsmZoyO"
   },
   "source": [
    "This model assumes that the pretrained image encoder is sufficient, and just focuses on building the text decoder. This tutorial uses a 2-layer Transformer-decoder.\n",
    "\n",
    "The implementations are almost identical to those in the [Transformers tutorial](https://www.tensorflow.org/text/tutorials/transformer). Refer back to it for more details.\n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "  <th>The Transformer encoder and decoder.</th>\n",
    "</tr>\n",
    "<tr>\n",
    "  <td>\n",
    "   <img width=400 src=\"https://www.tensorflow.org/images/tutorials/transformer/Transformer-1layer-words.png\"/>\n",
    "  </td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qiRXWwIKNybB"
   },
   "source": [
    "The model will be implemented in three main parts: \n",
    "\n",
    "1. Input - The token embedding and positional encoding (`SeqEmbedding`).\n",
    "1. Decoder - A stack of transformer decoder layers (`DecoderLayer`) where each contains:\n",
    "   1. A causal self attention later (`CausalSelfAttention`), where each output location can attend to the output so far.\n",
    "   1. A cross attention layer (`CrossAttention`) where each output location can attend to the input image.\n",
    "   1. A feed forward network (`FeedForward`) layer which further processes each output location independently.\n",
    "1. Output - A multiclass-classification over the output vocabulary.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_ngm3SQMCaYU"
   },
   "source": [
    "### Input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i9suaARZGPKw"
   },
   "source": [
    "The input text has already been split up into tokens and converted to sequences of IDs. \n",
    "\n",
    "Remember that unlike a CNN or RNN the Transformer's attention layers are invariant to the order of the sequence. Without some positional input, it just sees an unordered set not a sequence. So in addition to a simple vector embedding for each token ID, the embedding layer will also include an embedding for each position in the sequence.\n",
    "\n",
    "The `SeqEmbedding` layer defined below:\n",
    "\n",
    "- It looks up the embedding vector for each token.\n",
    "- It looks up an embedding vector for each sequence location.\n",
    "- It adds the two together.\n",
    "- It uses `mask_zero=True` to initialize the keras-masks for the model.\n",
    "\n",
    "Note: This implementation learns the position embeddings instead of using fixed embeddings like in the [Transformer tutorial](https://www.tensorflow.org/text/tutorials/transformer). Learning the embeddings is slightly less code, but doesn't generalize to longer sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T06:22:00.189224Z",
     "iopub.status.busy": "2022-12-14T06:22:00.188653Z",
     "iopub.status.idle": "2022-12-14T06:22:00.194218Z",
     "shell.execute_reply": "2022-12-14T06:22:00.193558Z"
    },
    "id": "P91LU2F0a9Ga"
   },
   "outputs": [],
   "source": [
    "class SeqEmbedding(tf.keras.layers.Layer):\n",
    "  def __init__(self, vocab_size, max_length, depth):\n",
    "    super().__init__()\n",
    "    self.pos_embedding = tf.keras.layers.Embedding(input_dim=max_length, output_dim=depth)\n",
    "\n",
    "    self.token_embedding = tf.keras.layers.Embedding(\n",
    "        input_dim=vocab_size,\n",
    "        output_dim=depth,\n",
    "        mask_zero=True)\n",
    "    \n",
    "    self.add = tf.keras.layers.Add()\n",
    "\n",
    "  def call(self, seq):\n",
    "    seq = self.token_embedding(seq) # (batch, seq, depth)\n",
    "\n",
    "    x = tf.range(tf.shape(seq)[1])  # (seq)\n",
    "    x = x[tf.newaxis, :]  # (1, seq)\n",
    "    x = self.pos_embedding(x)  # (1, seq, depth)\n",
    "\n",
    "    return self.add([seq,x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "II1mD-bBCdMB"
   },
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GHMLeMtKPTCW"
   },
   "source": [
    "The decoder is a standard Transformer-decoder, it contains a stack of `DecoderLayers` where each contains three sublayers: a `CausalSelfAttention`, a `CrossAttention`, and a`FeedForward`. The implementations are almost identical to the [Transformer tutorial](https://www.tensorflow.org/text/tutorials/transformer), refer to it for more details.\n",
    "\n",
    "The `CausalSelfAttention` layer is below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T06:22:00.197504Z",
     "iopub.status.busy": "2022-12-14T06:22:00.196894Z",
     "iopub.status.idle": "2022-12-14T06:22:00.201493Z",
     "shell.execute_reply": "2022-12-14T06:22:00.200856Z"
    },
    "id": "6JTLiX3lKooQ"
   },
   "outputs": [],
   "source": [
    "class CausalSelfAttention(tf.keras.layers.Layer):\n",
    "  def __init__(self, **kwargs):\n",
    "    super().__init__()\n",
    "    self.mha = tf.keras.layers.MultiHeadAttention(**kwargs)\n",
    "    # Use Add instead of + so the keras mask propagates through.\n",
    "    self.add = tf.keras.layers.Add() \n",
    "    self.layernorm = tf.keras.layers.LayerNormalization()\n",
    "  \n",
    "  def call(self, x):\n",
    "    attn = self.mha(query=x, value=x,\n",
    "                    use_causal_mask=True)\n",
    "    x = self.add([x, attn])\n",
    "    return self.layernorm(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8c66OTRwQfd8"
   },
   "source": [
    "The `CrossAttention` layer is below. Note the use of `return_attention_scores`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T06:22:00.204792Z",
     "iopub.status.busy": "2022-12-14T06:22:00.204217Z",
     "iopub.status.idle": "2022-12-14T06:22:00.208992Z",
     "shell.execute_reply": "2022-12-14T06:22:00.208405Z"
    },
    "id": "rIY6Vu2pLBAO"
   },
   "outputs": [],
   "source": [
    "class CrossAttention(tf.keras.layers.Layer):\n",
    "  def __init__(self,**kwargs):\n",
    "    super().__init__()\n",
    "    self.mha = tf.keras.layers.MultiHeadAttention(**kwargs)\n",
    "    self.add = tf.keras.layers.Add() \n",
    "    self.layernorm = tf.keras.layers.LayerNormalization()\n",
    "  \n",
    "  def call(self, x, y, **kwargs):\n",
    "    attn, attention_scores = self.mha(\n",
    "             query=x, value=y,\n",
    "             return_attention_scores=True)\n",
    "    \n",
    "    self.last_attention_scores = attention_scores\n",
    "\n",
    "    x = self.add([x, attn])\n",
    "    return self.layernorm(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8Hn5p6f-RE0C"
   },
   "source": [
    "The `FeedForward` layer is below. Remember that a `layers.Dense` layer is applied to the last axis of the input. The input will have a shape of `(batch, sequence, channels)`, so it automatically applies pointwise across the `batch` and `sequence` axes.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T06:22:00.212209Z",
     "iopub.status.busy": "2022-12-14T06:22:00.211678Z",
     "iopub.status.idle": "2022-12-14T06:22:00.216344Z",
     "shell.execute_reply": "2022-12-14T06:22:00.215737Z"
    },
    "id": "cWKrl7teOnH2"
   },
   "outputs": [],
   "source": [
    "class FeedForward(tf.keras.layers.Layer):\n",
    "  def __init__(self, units, dropout_rate=0.1):\n",
    "    super().__init__()\n",
    "    self.seq = tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(units=2*units, activation='relu'),\n",
    "        tf.keras.layers.Dense(units=units),\n",
    "        tf.keras.layers.Dropout(rate=dropout_rate),\n",
    "    ])\n",
    "\n",
    "    self.layernorm = tf.keras.layers.LayerNormalization()\n",
    "  \n",
    "  def call(self, x):\n",
    "    x = x + self.seq(x)\n",
    "    return self.layernorm(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lbXoiVNPRoJc"
   },
   "source": [
    "Next arrange these three layers into a larger `DecoderLayer`. Each decoder layer applies the three smaller layers in sequence. After each sublayer the shape of `out_seq` is `(batch, sequence, channels)`. The decoder layer also returns the `attention_scores` for later visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T06:22:00.219539Z",
     "iopub.status.busy": "2022-12-14T06:22:00.219010Z",
     "iopub.status.idle": "2022-12-14T06:22:00.223976Z",
     "shell.execute_reply": "2022-12-14T06:22:00.223359Z"
    },
    "id": "ydcW5KZZHou7"
   },
   "outputs": [],
   "source": [
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "  def __init__(self, units, num_heads=1, dropout_rate=0.1):\n",
    "    super().__init__()\n",
    "    \n",
    "    self.self_attention = CausalSelfAttention(num_heads=num_heads,\n",
    "                                              key_dim=units,\n",
    "                                              dropout=dropout_rate)\n",
    "    self.cross_attention = CrossAttention(num_heads=num_heads,\n",
    "                                          key_dim=units,\n",
    "                                          dropout=dropout_rate)\n",
    "    self.ff = FeedForward(units=units, dropout_rate=dropout_rate)\n",
    "      \n",
    "\n",
    "  def call(self, inputs, training=False):\n",
    "    in_seq, out_seq = inputs\n",
    "\n",
    "    # Text input\n",
    "    out_seq = self.self_attention(out_seq)\n",
    "\n",
    "    out_seq = self.cross_attention(out_seq, in_seq)\n",
    "    \n",
    "    self.last_attention_scores = self.cross_attention.last_attention_scores\n",
    "\n",
    "    out_seq = self.ff(out_seq)\n",
    "\n",
    "    return out_seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-lgbYrF5Csqu"
   },
   "source": [
    "### Output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VcnKZkrklAQf"
   },
   "source": [
    "At minimum the output layer needs a `layers.Dense` layer to generate logit-predictions for each token at each location."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6WQD87efena5"
   },
   "source": [
    "But there are a few other features you can add to make this work a little better:\n",
    "\n",
    "1. **Handle bad tokens**: The model will be generating text. It should\n",
    "   never generate a pad, unknown, or start token (`''`, `'[UNK]'`, \n",
    "   `'[START]'`). So set the bias for these to a large negative value.\n",
    "\n",
    "   > Note: You'll need to ignore these tokens in the loss function as well. \n",
    "\n",
    "2. **Smart initialization**: The default initialization of a dense layer will\n",
    "  give a model that initially predicts each token with almost uniform\n",
    "  likelihood. The actual token distribution is far from uniform. The\n",
    "  optimal value for the initial bias of the output layer is the log of the\n",
    "  probability of each token. So include an `adapt` method to count the tokens\n",
    "  and set the optimal initial bias. This reduces the initial loss from the\n",
    "  entropy of the uniform distribution (`log(vocabulary_size)`) to the marginal\n",
    "  entropy of the distribution (`-p*log(p)`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T06:22:00.227371Z",
     "iopub.status.busy": "2022-12-14T06:22:00.226844Z",
     "iopub.status.idle": "2022-12-14T06:22:00.233951Z",
     "shell.execute_reply": "2022-12-14T06:22:00.233320Z"
    },
    "id": "CeWw2SFDHUfo"
   },
   "outputs": [],
   "source": [
    "#@title\n",
    "class TokenOutput(tf.keras.layers.Layer):\n",
    "  def __init__(self, tokenizer, banned_tokens=('', '[UNK]', '[START]'), **kwargs):\n",
    "    super().__init__()\n",
    "    \n",
    "    self.dense = tf.keras.layers.Dense(\n",
    "        units=tokenizer.vocabulary_size(), **kwargs)\n",
    "    self.tokenizer = tokenizer\n",
    "    self.banned_tokens = banned_tokens\n",
    "\n",
    "    self.bias = None\n",
    "\n",
    "  def adapt(self, ds):\n",
    "    counts = collections.Counter()\n",
    "    vocab_dict = {name: id \n",
    "                  for id, name in enumerate(self.tokenizer.get_vocabulary())}\n",
    "\n",
    "    for tokens in tqdm.tqdm(ds):\n",
    "      counts.update(tokens.numpy().flatten())\n",
    "\n",
    "    counts_arr = np.zeros(shape=(self.tokenizer.vocabulary_size(),))\n",
    "    counts_arr[np.array(list(counts.keys()), dtype=np.int32)] = list(counts.values())\n",
    "\n",
    "    counts_arr = counts_arr[:]\n",
    "    for token in self.banned_tokens:\n",
    "      counts_arr[vocab_dict[token]] = 0\n",
    "\n",
    "    total = counts_arr.sum()\n",
    "    p = counts_arr/total\n",
    "    p[counts_arr==0] = 1.0\n",
    "    log_p = np.log(p)  # log(1) == 0\n",
    "\n",
    "    entropy = -(log_p*p).sum()\n",
    "\n",
    "    print()\n",
    "    print(f\"Uniform entropy: {np.log(self.tokenizer.vocabulary_size()):0.2f}\")\n",
    "    print(f\"Marginal entropy: {entropy:0.2f}\")\n",
    "\n",
    "    self.bias = log_p\n",
    "    self.bias[counts_arr==0] = -1e9\n",
    "\n",
    "  def call(self, x):\n",
    "    x = self.dense(x)\n",
    "    # TODO(b/250038731): Fix this.\n",
    "    # An Add layer doesn't work because of the different shapes.\n",
    "    # This clears the mask, that's okay because it prevents keras from rescaling\n",
    "    # the losses.\n",
    "    return x + self.bias\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xzQHqANd1A6Q"
   },
   "source": [
    "The smart initialization will significantly reduce the initial loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T06:22:00.237234Z",
     "iopub.status.busy": "2022-12-14T06:22:00.236671Z",
     "iopub.status.idle": "2022-12-14T06:22:02.999761Z",
     "shell.execute_reply": "2022-12-14T06:22:02.999045Z"
    },
    "id": "GGnOQyc501B2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 193/193 [00:00<00:00, 211.06it/s]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'[START]'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Input \u001b[0;32mIn [50]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m output_layer \u001b[38;5;241m=\u001b[39m TokenOutput(tokenizer, banned_tokens\u001b[38;5;241m=\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m[UNK]\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m[START]\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# This might run a little faster if the dataset didn't also have to load the image data.\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[43moutput_layer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madapt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_ds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [49]\u001b[0m, in \u001b[0;36mTokenOutput.adapt\u001b[0;34m(self, ds)\u001b[0m\n\u001b[1;32m     24\u001b[0m counts_arr \u001b[38;5;241m=\u001b[39m counts_arr[:]\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbanned_tokens:\n\u001b[0;32m---> 26\u001b[0m   counts_arr[\u001b[43mvocab_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m]\u001b[49m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     28\u001b[0m total \u001b[38;5;241m=\u001b[39m counts_arr\u001b[38;5;241m.\u001b[39msum()\n\u001b[1;32m     29\u001b[0m p \u001b[38;5;241m=\u001b[39m counts_arr\u001b[38;5;241m/\u001b[39mtotal\n",
      "\u001b[0;31mKeyError\u001b[0m: '[START]'"
     ]
    }
   ],
   "source": [
    "output_layer = TokenOutput(tokenizer, banned_tokens=('', '[UNK]', '[START]'))\n",
    "# This might run a little faster if the dataset didn't also have to load the image data.\n",
    "output_layer.adapt(train_ds.map(lambda inputs, labels: labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3gq-ICN7bD-u"
   },
   "source": [
    "### Build the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gou4fPH_SWgH"
   },
   "source": [
    "To build the model, you need to combine several parts:\n",
    "\n",
    "1. The image `feature_extractor` and the text `tokenizer` and.\n",
    "1. The `seq_embedding` layer, to convert batches of token-IDs to \n",
    "   vectors `(batch, sequence, channels)`.\n",
    "3. The stack of `DecoderLayers` layers that will process the text and image data.\n",
    "4. The `output_layer` which returns a pointwise prediction of what the next word should be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T06:22:03.003339Z",
     "iopub.status.busy": "2022-12-14T06:22:03.003064Z",
     "iopub.status.idle": "2022-12-14T06:22:03.009620Z",
     "shell.execute_reply": "2022-12-14T06:22:03.008880Z"
    },
    "id": "bHCISYehH1f6"
   },
   "outputs": [],
   "source": [
    "class Captioner(tf.keras.Model):\n",
    "  @classmethod\n",
    "  def add_method(cls, fun):\n",
    "    setattr(cls, fun.__name__, fun)\n",
    "    return fun\n",
    "\n",
    "  def __init__(self, tokenizer, feature_extractor, output_layer, num_layers=1,\n",
    "               units=256, max_length=50, num_heads=1, dropout_rate=0.1):\n",
    "    super().__init__()\n",
    "    self.feature_extractor = feature_extractor\n",
    "    self.tokenizer = tokenizer\n",
    "    self.word_to_index = tf.keras.layers.StringLookup(\n",
    "        mask_token=\"\",\n",
    "        vocabulary=tokenizer.get_vocabulary())\n",
    "    self.index_to_word = tf.keras.layers.StringLookup(\n",
    "        mask_token=\"\",\n",
    "        vocabulary=tokenizer.get_vocabulary(),\n",
    "        invert=True) \n",
    "\n",
    "    self.seq_embedding = SeqEmbedding(\n",
    "        vocab_size=tokenizer.vocabulary_size(),\n",
    "        depth=units,\n",
    "        max_length=max_length)\n",
    "\n",
    "    self.decoder_layers = [\n",
    "        DecoderLayer(units, num_heads=num_heads, dropout_rate=dropout_rate)\n",
    "        for n in range(num_layers)]\n",
    "\n",
    "    self.output_layer = output_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YW390dOz9T-x"
   },
   "source": [
    "When you call the model, for training, it receives an `image, txt` pair. To make this function more usable, be flexible about the input:\n",
    "\n",
    "* If the image has 3 channels run it through the feature_extractor. Otherwise assume that it has been already. Similarly\n",
    "* If the text has dtype `tf.string` run it through the tokenizer.\n",
    "\n",
    "After that running the model is only a few steps:\n",
    "\n",
    "1. Flatten the extracted image features, so they can be input to the decoder layers.\n",
    "2. Look up the token embeddings.\n",
    "3. Run the stack of `DecoderLayer`s, on the image features and text embeddings.\n",
    "4. Run the output layer to predict the next token at each position.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T06:22:03.013025Z",
     "iopub.status.busy": "2022-12-14T06:22:03.012423Z",
     "iopub.status.idle": "2022-12-14T06:22:03.017326Z",
     "shell.execute_reply": "2022-12-14T06:22:03.016622Z"
    },
    "id": "lPdb7I4h9Ulo"
   },
   "outputs": [],
   "source": [
    "  @Captioner.add_method\n",
    "  def call(self, inputs):\n",
    "    image, txt = inputs\n",
    "\n",
    "    if image.shape[-1] == 3:\n",
    "      # Apply the feature-extractor, if you get an RGB image.\n",
    "      image = self.feature_extractor(image)\n",
    "    \n",
    "    # Flatten the feature map\n",
    "    image = einops.rearrange(image, 'b h w c -> b (h w) c')\n",
    "\n",
    "\n",
    "    if txt.dtype == tf.string:\n",
    "      # Apply the tokenizer if you get string inputs.\n",
    "      txt = tokenizer(txt)\n",
    "\n",
    "    txt = self.seq_embedding(txt)\n",
    "\n",
    "    # Look at the image\n",
    "    for dec_layer in self.decoder_layers:\n",
    "      txt = dec_layer(inputs=(image, txt))\n",
    "      \n",
    "    txt = self.output_layer(txt)\n",
    "\n",
    "    return txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T06:22:03.020421Z",
     "iopub.status.busy": "2022-12-14T06:22:03.019922Z",
     "iopub.status.idle": "2022-12-14T06:22:03.126998Z",
     "shell.execute_reply": "2022-12-14T06:22:03.126342Z"
    },
    "id": "kmM7aZQsLiyU"
   },
   "outputs": [],
   "source": [
    "model = Captioner(tokenizer, feature_extractor=mobilenet, output_layer=output_layer,\n",
    "                  units=256, dropout_rate=0.5, num_layers=2, num_heads=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xGvOcLQKghXN"
   },
   "source": [
    "### Generate captions\n",
    "\n",
    "Before getting into training, write a bit of code to generate captions. You'll use this to see how training is progressing.\n",
    "\n",
    "Start by downloading a test image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T06:22:03.130545Z",
     "iopub.status.busy": "2022-12-14T06:22:03.129981Z",
     "iopub.status.idle": "2022-12-14T06:22:03.264289Z",
     "shell.execute_reply": "2022-12-14T06:22:03.263588Z"
    },
    "id": "cwFcdMqC-jE2"
   },
   "outputs": [],
   "source": [
    "for img, label in test_ds.take(1):\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[b'[UNK]' b'd' b'[UNK]' b'[UNK]' b'[UNK]' b'[UNK]' b'[UNK]' b'[UNK]' b''\n",
      " b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b''\n",
      " b'' b''], shape=(29,), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "w = index_to_word(label[0])\n",
    "print(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(32, 4, 576), dtype=float32, numpy=\n",
       "array([[[ 3847.1816 ,    -0.     ,    -0.     , ...,    -0.     ,\n",
       "            -0.     ,    -0.     ],\n",
       "        [ 1530.8416 ,  6505.8774 ,    -0.     , ...,    -0.     ,\n",
       "          3934.6052 ,    -0.     ],\n",
       "        [   -0.     ,    -0.     ,    -0.     , ...,    -0.     ,\n",
       "            -0.     ,    -0.     ],\n",
       "        [   -0.     ,  3497.5974 ,    -0.     , ...,    -0.     ,\n",
       "            -0.     ,    -0.     ]],\n",
       "\n",
       "       [[   -0.     ,    -0.     ,    -0.     , ...,    -0.     ,\n",
       "            -0.     ,    -0.     ],\n",
       "        [ 1654.1566 ,  2590.0369 ,    -0.     , ...,    -0.     ,\n",
       "            -0.     ,    -0.     ],\n",
       "        [   -0.     ,    -0.     ,    -0.     , ...,    -0.     ,\n",
       "            -0.     ,    -0.     ],\n",
       "        [   -0.     , 13461.704  ,    -0.     , ...,    -0.     ,\n",
       "            -0.     ,    -0.     ]],\n",
       "\n",
       "       [[   -0.     ,    -0.     ,    -0.     , ...,    -0.     ,\n",
       "            -0.     ,    -0.     ],\n",
       "        [ 6421.4375 ,    -0.     ,    -0.     , ...,    -0.     ,\n",
       "            -0.     ,    -0.     ],\n",
       "        [   -0.     ,    -0.     ,    -0.     , ...,    -0.     ,\n",
       "            -0.     ,    -0.     ],\n",
       "        [   -0.     , 14318.027  ,    -0.     , ...,    -0.     ,\n",
       "          5596.9478 ,    -0.     ]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[   -0.     ,    -0.     ,    -0.     , ...,    -0.     ,\n",
       "            -0.     ,    -0.     ],\n",
       "        [   -0.     ,  1068.5531 ,    -0.     , ...,    -0.     ,\n",
       "            -0.     ,    -0.     ],\n",
       "        [   -0.     ,    -0.     ,    -0.     , ...,    -0.     ,\n",
       "            -0.     ,    -0.     ],\n",
       "        [ 1233.7482 , 12330.929  ,    -0.     , ...,    -0.     ,\n",
       "          3163.929  ,    -0.     ]],\n",
       "\n",
       "       [[   -0.     ,    -0.     ,    -0.     , ...,    -0.     ,\n",
       "            -0.     ,    -0.     ],\n",
       "        [   -0.     ,  2021.2152 ,    -0.     , ...,    -0.     ,\n",
       "            -0.     ,    -0.     ],\n",
       "        [   -0.     ,    -0.     ,    -0.     , ...,    -0.     ,\n",
       "            -0.     ,    -0.     ],\n",
       "        [   -0.     ,   153.72717,    -0.     , ...,    -0.     ,\n",
       "            -0.     ,    -0.     ]],\n",
       "\n",
       "       [[   -0.     ,    -0.     ,    -0.     , ...,    -0.     ,\n",
       "            -0.     ,    -0.     ],\n",
       "        [   -0.     ,    -0.     ,    -0.     , ...,    -0.     ,\n",
       "            -0.     ,    -0.     ],\n",
       "        [   -0.     ,    -0.     ,    -0.     , ...,    -0.     ,\n",
       "            -0.     ,    -0.     ],\n",
       "        [   -0.     ,    -0.     ,    -0.     , ...,    -0.     ,\n",
       "            -0.     ,    -0.     ]]], dtype=float32)>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IRBIiTkubmxA"
   },
   "source": [
    "To caption an image with this model:\n",
    "\n",
    "- Extract the `img_features`\n",
    "- Initialize the list of output tokens with a `[START]` token.\n",
    "- Pass `img_features` and `tokens` into the model.\n",
    "  - It returns a list of logits.\n",
    "  - Choose the next token based on those logits.  \n",
    "  - Add it to the list of tokens, and continue the loop.\n",
    "  - If it generates an `'[END]'` token, break out of the loop.\n",
    "\n",
    "So add a \"simple\" method to do just that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T06:22:03.267487Z",
     "iopub.status.busy": "2022-12-14T06:22:03.267222Z",
     "iopub.status.idle": "2022-12-14T06:22:03.273062Z",
     "shell.execute_reply": "2022-12-14T06:22:03.272456Z"
    },
    "id": "Nf1Jie9ef_Cg"
   },
   "outputs": [],
   "source": [
    "@Captioner.add_method\n",
    "def simple_gen(self, image, temperature=1):\n",
    "  initial = self.word_to_index([['[START]']]) # (batch, sequence)\n",
    "  img_features = tf.expand_dims(image, axis=0)\n",
    "#   img_features = self.feature_extractor(image[tf.newaxis, ...])\n",
    "\n",
    "  tokens = initial # (batch, sequence)\n",
    "  for n in range(50):\n",
    "    preds = self((img_features, tokens)).numpy()  # (batch, sequence, vocab)\n",
    "    preds = preds[:,-1, :]  #(batch, vocab)\n",
    "    if temperature==0:\n",
    "        next = tf.argmax(preds, axis=-1)[:, tf.newaxis]  # (batch, 1)\n",
    "    else:\n",
    "        next = tf.random.categorical(preds/temperature, num_samples=1)  # (batch, 1)\n",
    "    tokens = tf.concat([tokens, next], axis=1) # (batch, sequence) \n",
    "\n",
    "    if next[0] == self.word_to_index('[END]'):\n",
    "      break\n",
    "  words = index_to_word(tokens[0, 1:-1])\n",
    "  result = tf.strings.reduce_join(words, axis=-1, separator=' ')\n",
    "  return result.numpy().decode()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TxN2NPX2zB8y"
   },
   "source": [
    "Here are some generated captions for that image, the model's untrained, so they don't make much sense yet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T06:22:03.276367Z",
     "iopub.status.busy": "2022-12-14T06:22:03.275837Z",
     "iopub.status.idle": "2022-12-14T06:22:05.806911Z",
     "shell.execute_reply": "2022-12-14T06:22:05.806094Z"
    },
    "id": "sPm96CccvHnq"
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Exception encountered when calling layer 'token_output' (type TokenOutput).\n\nAttempt to convert a value (None) with an unsupported type (<class 'NoneType'>) to a Tensor.\n\nCall arguments received by layer 'token_output' (type TokenOutput):\n  • x=tf.Tensor(shape=(1, 1, 256), dtype=float32)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [58]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m0.0\u001b[39m, \u001b[38;5;241m0.5\u001b[39m, \u001b[38;5;241m1.0\u001b[39m):\n\u001b[0;32m----> 2\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msimple_gen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m   \u001b[38;5;28mprint\u001b[39m(result)\n",
      "Input \u001b[0;32mIn [57]\u001b[0m, in \u001b[0;36msimple_gen\u001b[0;34m(self, image, temperature)\u001b[0m\n\u001b[1;32m      7\u001b[0m tokens \u001b[38;5;241m=\u001b[39m initial \u001b[38;5;66;03m# (batch, sequence)\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m n \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m50\u001b[39m):\n\u001b[0;32m----> 9\u001b[0m   preds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mnumpy()  \u001b[38;5;66;03m# (batch, sequence, vocab)\u001b[39;00m\n\u001b[1;32m     10\u001b[0m   preds \u001b[38;5;241m=\u001b[39m preds[:,\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :]  \u001b[38;5;66;03m#(batch, vocab)\u001b[39;00m\n\u001b[1;32m     11\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m temperature\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/envs/htr2/lib/python3.8/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "Input \u001b[0;32mIn [52]\u001b[0m, in \u001b[0;36mcall\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m dec_layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder_layers:\n\u001b[1;32m     21\u001b[0m   txt \u001b[38;5;241m=\u001b[39m dec_layer(inputs\u001b[38;5;241m=\u001b[39m(image, txt))\n\u001b[0;32m---> 23\u001b[0m txt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtxt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m txt\n",
      "Input \u001b[0;32mIn [49]\u001b[0m, in \u001b[0;36mTokenOutput.call\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     43\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdense(x)\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# TODO(b/250038731): Fix this.\u001b[39;00m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;66;03m# An Add layer doesn't work because of the different shapes.\u001b[39;00m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# This clears the mask, that's okay because it prevents keras from rescaling\u001b[39;00m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;66;03m# the losses.\u001b[39;00m\n\u001b[0;32m---> 48\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\n",
      "\u001b[0;31mValueError\u001b[0m: Exception encountered when calling layer 'token_output' (type TokenOutput).\n\nAttempt to convert a value (None) with an unsupported type (<class 'NoneType'>) to a Tensor.\n\nCall arguments received by layer 'token_output' (type TokenOutput):\n  • x=tf.Tensor(shape=(1, 1, 256), dtype=float32)"
     ]
    }
   ],
   "source": [
    "for t in (0.0, 0.5, 1.0):\n",
    "  result = model.simple_gen(img[0][0], temperature=t)\n",
    "  print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JefwCRZ8z-Ah"
   },
   "source": [
    "The temperature parameter allows you to interpolate between 3 modes:\n",
    "\n",
    "1. Greedy decoding (`temperature=0.0`) - Chooses the most likely next token at each step.\n",
    "2. Random sampling according to the logits (`temperature=1.0`).\n",
    "3. Uniform random sampling (`temperature >> 1.0`). \n",
    "\n",
    "Since the model is untrained, and it used the frequency-based initialization, the \"greedy\" output (first) usually only contains the most common tokens: `['a', '.', '[END]']`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r0FpTvaPkqON"
   },
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IKcwZdqObK-U"
   },
   "source": [
    "To train the model you'll need several additional components:\n",
    "\n",
    "- The Loss and metrics\n",
    "- The Optimizer\n",
    "- Optional Callbacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g5IW2mWa2sAG"
   },
   "source": [
    "### Losses and metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XbpbDQTw1lOW"
   },
   "source": [
    "Here's an implementation of a masked loss and accuracy:\n",
    "\n",
    "When calculating the mask for the loss, note the `loss < 1e8`. This term discards the artificial, impossibly high losses for the `banned_tokens`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T06:22:05.810811Z",
     "iopub.status.busy": "2022-12-14T06:22:05.810544Z",
     "iopub.status.idle": "2022-12-14T06:22:05.815828Z",
     "shell.execute_reply": "2022-12-14T06:22:05.815226Z"
    },
    "id": "s24im3FqxAfT"
   },
   "outputs": [],
   "source": [
    "def masked_loss(labels, preds):  \n",
    "  loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels, preds)\n",
    "\n",
    "  mask = (labels != 0) & (loss < 1e8) \n",
    "  mask = tf.cast(mask, loss.dtype)\n",
    "\n",
    "  loss = loss*mask\n",
    "  loss = tf.reduce_sum(loss)/tf.reduce_sum(mask)\n",
    "  return loss\n",
    "\n",
    "def masked_acc(labels, preds):\n",
    "  mask = tf.cast(labels!=0, tf.float32)\n",
    "  preds = tf.argmax(preds, axis=-1)\n",
    "  labels = tf.cast(labels, tf.int64)\n",
    "  match = tf.cast(preds == labels, mask.dtype)\n",
    "  acc = tf.reduce_sum(match*mask)/tf.reduce_sum(mask)\n",
    "  return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zOhjHqgv3F2e"
   },
   "source": [
    "### Callbacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3dyQN9UfJYEd"
   },
   "source": [
    "For feedback during training setup a `keras.callbacks.Callback` to generate some captions for the surfer image at the end of each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T06:22:05.819452Z",
     "iopub.status.busy": "2022-12-14T06:22:05.818884Z",
     "iopub.status.idle": "2022-12-14T06:22:05.823422Z",
     "shell.execute_reply": "2022-12-14T06:22:05.822773Z"
    },
    "id": "IKDwbZOCZ-AP"
   },
   "outputs": [],
   "source": [
    "class GenerateText(tf.keras.callbacks.Callback):\n",
    "  def __init__(self):\n",
    "    self.image=img[0][0]\n",
    "    \n",
    "\n",
    "  def on_epoch_end(self, epochs=None, logs=None):\n",
    "    print()\n",
    "    print()\n",
    "    for t in (0.0, 0.5, 1.0):\n",
    "      result = self.model.simple_gen(self.image, temperature=t)\n",
    "      print(result)\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1yNA3_RAsdl0"
   },
   "source": [
    "It generates three output strings, like the earlier example, like before the first is \"greedy\", choosing the argmax of the logits at each step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T06:22:05.826873Z",
     "iopub.status.busy": "2022-12-14T06:22:05.826250Z",
     "iopub.status.idle": "2022-12-14T06:22:06.569844Z",
     "shell.execute_reply": "2022-12-14T06:22:06.569031Z"
    },
    "id": "IGVLpzo13rcA"
   },
   "outputs": [],
   "source": [
    "g = GenerateText()\n",
    "g.model = model\n",
    "g.on_epoch_end(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MAxp4KZRKDk9"
   },
   "source": [
    "Also use `callbacks.EarlyStopping` to terminate training when the model starts to overfit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T06:22:06.574003Z",
     "iopub.status.busy": "2022-12-14T06:22:06.573386Z",
     "iopub.status.idle": "2022-12-14T06:22:06.581462Z",
     "shell.execute_reply": "2022-12-14T06:22:06.580830Z"
    },
    "id": "MjzrwGZp23xx"
   },
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    GenerateText(),\n",
    "    tf.keras.callbacks.EarlyStopping(\n",
    "        patience=5, restore_best_weights=True)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZBaJhQpcG8u0"
   },
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WBXG0dCDKO55"
   },
   "source": [
    "Configure and execute the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T06:22:06.585144Z",
     "iopub.status.busy": "2022-12-14T06:22:06.584521Z",
     "iopub.status.idle": "2022-12-14T06:22:06.610757Z",
     "shell.execute_reply": "2022-12-14T06:22:06.610166Z"
    },
    "id": "2OR5ZpAII__u"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
    "           loss=masked_loss,\n",
    "           metrics=[masked_acc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ro955bQ2KR0X"
   },
   "source": [
    "For more frequent reporting, use the `Dataset.repeat()` method, and set the `steps_per_epoch` and `validation_steps` arguments to `Model.fit`. \n",
    "\n",
    "With this setup on `Flickr8k` a full pass over the dataset is 900+ batches, but below the reporting-epochs are 100 steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T06:22:06.614610Z",
     "iopub.status.busy": "2022-12-14T06:22:06.613905Z",
     "iopub.status.idle": "2022-12-14T06:25:36.881556Z",
     "shell.execute_reply": "2022-12-14T06:25:36.880775Z"
    },
    "id": "3aB0baOVMZe9"
   },
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    train_ds.repeat(),\n",
    "    steps_per_epoch=100,\n",
    "    validation_data=valid_ds.repeat(),\n",
    "    validation_steps=20,\n",
    "    epochs=100,\n",
    "    callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P634LfVgw-eV"
   },
   "source": [
    "Plot the loss and accuracy over the training run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T06:25:36.885901Z",
     "iopub.status.busy": "2022-12-14T06:25:36.885233Z",
     "iopub.status.idle": "2022-12-14T06:25:37.060469Z",
     "shell.execute_reply": "2022-12-14T06:25:37.059838Z"
    },
    "id": "6Wn8KSkUw916"
   },
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'], label='loss')\n",
    "plt.plot(history.history['val_loss'], label='val_loss')\n",
    "plt.ylim([0, max(plt.ylim())])\n",
    "plt.xlabel('Epoch #')\n",
    "plt.ylabel('CE/token')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T06:25:37.063774Z",
     "iopub.status.busy": "2022-12-14T06:25:37.063531Z",
     "iopub.status.idle": "2022-12-14T06:25:37.223243Z",
     "shell.execute_reply": "2022-12-14T06:25:37.222629Z"
    },
    "id": "yZQ78b2Kxw-T"
   },
   "outputs": [],
   "source": [
    "plt.plot(history.history['masked_acc'], label='accuracy')\n",
    "plt.plot(history.history['val_masked_acc'], label='val_accuracy')\n",
    "plt.ylim([0, max(plt.ylim())])\n",
    "plt.xlabel('Epoch #')\n",
    "plt.ylabel('CE/token')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T06:25:41.387953Z",
     "iopub.status.busy": "2022-12-14T06:25:41.387392Z",
     "iopub.status.idle": "2022-12-14T06:25:43.254154Z",
     "shell.execute_reply": "2022-12-14T06:25:43.253412Z"
    },
    "id": "9Psd1quzaAWg"
   },
   "outputs": [],
   "source": [
    "image_url = 'https://tensorflow.org/images/bedroom_hrnet_tutorial.jpg'\n",
    "image_path = tf.keras.utils.get_file(origin=image_url)\n",
    "image = load_image(image_path)\n",
    "\n",
    "run_and_show_attention(model, image)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "image_captioning.ipynb",
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
