{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e1e0137",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: kaldiio in /home/project/anaconda3/envs/htr2/lib/python3.8/site-packages (2.17.2)\n",
      "Requirement already satisfied: numpy in /home/project/anaconda3/envs/htr2/lib/python3.8/site-packages (from kaldiio) (1.22.3)\n",
      "Requirement already satisfied: stn in /home/project/anaconda3/envs/htr2/lib/python3.8/site-packages (1.0.1)\n",
      "Requirement already satisfied: numpy in /home/project/anaconda3/envs/htr2/lib/python3.8/site-packages (from stn) (1.22.3)\n",
      "Requirement already satisfied: rapidfuzz in /home/project/anaconda3/envs/htr2/lib/python3.8/site-packages (2.3.0)\n",
      "Requirement already satisfied: jarowinkler<2.0.0,>=1.2.0 in /home/project/anaconda3/envs/htr2/lib/python3.8/site-packages (from rapidfuzz) (1.2.0)\n",
      "Requirement already satisfied: seaborn in /home/project/anaconda3/envs/htr2/lib/python3.8/site-packages (0.11.2)\n",
      "Requirement already satisfied: pandas>=0.23 in /home/project/anaconda3/envs/htr2/lib/python3.8/site-packages (from seaborn) (1.4.3)\n",
      "Requirement already satisfied: matplotlib>=2.2 in /home/project/anaconda3/envs/htr2/lib/python3.8/site-packages (from seaborn) (3.5.2)\n",
      "Requirement already satisfied: numpy>=1.15 in /home/project/anaconda3/envs/htr2/lib/python3.8/site-packages (from seaborn) (1.22.3)\n",
      "Requirement already satisfied: scipy>=1.0 in /home/project/anaconda3/envs/htr2/lib/python3.8/site-packages (from seaborn) (1.8.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/project/anaconda3/envs/htr2/lib/python3.8/site-packages (from matplotlib>=2.2->seaborn) (21.3)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /home/project/anaconda3/envs/htr2/lib/python3.8/site-packages (from matplotlib>=2.2->seaborn) (3.0.8)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/project/anaconda3/envs/htr2/lib/python3.8/site-packages (from matplotlib>=2.2->seaborn) (4.34.4)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/project/anaconda3/envs/htr2/lib/python3.8/site-packages (from matplotlib>=2.2->seaborn) (1.4.4)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/project/anaconda3/envs/htr2/lib/python3.8/site-packages (from matplotlib>=2.2->seaborn) (0.11.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /home/project/anaconda3/envs/htr2/lib/python3.8/site-packages (from matplotlib>=2.2->seaborn) (9.1.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/project/anaconda3/envs/htr2/lib/python3.8/site-packages (from matplotlib>=2.2->seaborn) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/project/anaconda3/envs/htr2/lib/python3.8/site-packages (from pandas>=0.23->seaborn) (2022.1)\n",
      "Requirement already satisfied: six>=1.5 in /home/project/anaconda3/envs/htr2/lib/python3.8/site-packages (from python-dateutil>=2.7->matplotlib>=2.2->seaborn) (1.16.0)\n",
      "Requirement already satisfied: tensorflow_addons in /home/project/anaconda3/envs/htr2/lib/python3.8/site-packages (0.17.1)\n",
      "Requirement already satisfied: typeguard>=2.7 in /home/project/anaconda3/envs/htr2/lib/python3.8/site-packages (from tensorflow_addons) (2.13.3)\n",
      "Requirement already satisfied: packaging in /home/project/anaconda3/envs/htr2/lib/python3.8/site-packages (from tensorflow_addons) (21.3)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/project/anaconda3/envs/htr2/lib/python3.8/site-packages (from packaging->tensorflow_addons) (3.0.8)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-30 13:30:27.083064: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-09-30 13:30:27.175830: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-09-30 13:30:27.690626: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-09-30 13:30:27.690669: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-09-30 13:30:27.690673: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "2023-09-30 13:30:28.141052: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-09-30 13:30:28.348004: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2006] Ignoring visible gpu device (device: 1, name: Quadro P1000, pci bus id: 0000:65:00.0, compute capability: 6.1) with core count: 5. The minimum required count is 8. You can adjust this requirement with the env var TF_MIN_GPU_MULTIPROCESSOR_COUNT.\n",
      "2023-09-30 13:30:28.714988: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /device:GPU:0 with 9631 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2080 Ti, pci bus id: 0000:17:00.0, compute capability: 7.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found GPU at: /device:GPU:0\n",
      "source: ../data/bentham.hdf5\n",
      "output ../output/bentham/shashankbest4senetgateddropouttwo/2023-09-30 13:30:27.081951\n",
      "target ../output/bentham/shashankbest4senetgateddropouttwo/2023-09-30 13:30:27.081951/checkpoint_weights.hdf5\n",
      "charset: 0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~ \n",
      "Train images: 8807\n",
      "Validation images: 1372\n",
      "Test images: 820\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-30 13:30:35.419134: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2006] Ignoring visible gpu device (device: 1, name: Quadro P1000, pci bus id: 0000:65:00.0, compute capability: 6.1) with core count: 5. The minimum required count is 8. You can adjust this requirement with the env var TF_MIN_GPU_MULTIPROCESSOR_COUNT.\n",
      "2023-09-30 13:30:35.420009: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 9631 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2080 Ti, pci bus id: 0000:17:00.0, compute capability: 7.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input (InputLayer)             [(None, 1024, 128,   0           []                               \n",
      "                                1)]                                                               \n",
      "                                                                                                  \n",
      " conv2d (Conv2D)                (None, 1024, 128, 8  80          ['input[0][0]']                  \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " batch_normalization (BatchNorm  (None, 1024, 128, 8  56         ['conv2d[0][0]']                 \n",
      " alization)                     )                                                                 \n",
      "                                                                                                  \n",
      " global_average_pooling2d (Glob  (None, 8)           0           ['batch_normalization[0][0]']    \n",
      " alAveragePooling2D)                                                                              \n",
      "                                                                                                  \n",
      " reshape (Reshape)              (None, 1, 1, 8)      0           ['global_average_pooling2d[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 1, 1, 4)      32          ['reshape[0][0]']                \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 1, 1, 8)      32          ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " multiply (Multiply)            (None, 1024, 128, 8  0           ['batch_normalization[0][0]',    \n",
      "                                )                                 'dense_1[0][0]']                \n",
      "                                                                                                  \n",
      " full_gated_conv2d (FullGatedCo  (None, 1024, 128, 8  1168       ['multiply[0][0]']               \n",
      " nv2D)                          )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_1 (Conv2D)              (None, 512, 64, 16)  1168        ['full_gated_conv2d[0][0]']      \n",
      "                                                                                                  \n",
      " batch_normalization_1 (BatchNo  (None, 512, 64, 16)  112        ['conv2d_1[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " global_average_pooling2d_1 (Gl  (None, 16)          0           ['batch_normalization_1[0][0]']  \n",
      " obalAveragePooling2D)                                                                            \n",
      "                                                                                                  \n",
      " reshape_1 (Reshape)            (None, 1, 1, 16)     0           ['global_average_pooling2d_1[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 1, 1, 8)      128         ['reshape_1[0][0]']              \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 1, 1, 16)     128         ['dense_2[0][0]']                \n",
      "                                                                                                  \n",
      " multiply_1 (Multiply)          (None, 512, 64, 16)  0           ['batch_normalization_1[0][0]',  \n",
      "                                                                  'dense_3[0][0]']                \n",
      "                                                                                                  \n",
      " full_gated_conv2d_1 (FullGated  (None, 512, 64, 16)  4640       ['multiply_1[0][0]']             \n",
      " Conv2D)                                                                                          \n",
      "                                                                                                  \n",
      " conv2d_2 (Conv2D)              (None, 512, 64, 24)  3480        ['full_gated_conv2d_1[0][0]']    \n",
      "                                                                                                  \n",
      " batch_normalization_2 (BatchNo  (None, 512, 64, 24)  168        ['conv2d_2[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " global_average_pooling2d_2 (Gl  (None, 24)          0           ['batch_normalization_2[0][0]']  \n",
      " obalAveragePooling2D)                                                                            \n",
      "                                                                                                  \n",
      " reshape_2 (Reshape)            (None, 1, 1, 24)     0           ['global_average_pooling2d_2[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " dense_4 (Dense)                (None, 1, 1, 12)     288         ['reshape_2[0][0]']              \n",
      "                                                                                                  \n",
      " dense_5 (Dense)                (None, 1, 1, 24)     288         ['dense_4[0][0]']                \n",
      "                                                                                                  \n",
      " multiply_2 (Multiply)          (None, 512, 64, 24)  0           ['batch_normalization_2[0][0]',  \n",
      "                                                                  'dense_5[0][0]']                \n",
      "                                                                                                  \n",
      " full_gated_conv2d_2 (FullGated  (None, 512, 64, 24)  10416      ['multiply_2[0][0]']             \n",
      " Conv2D)                                                                                          \n",
      "                                                                                                  \n",
      " conv2d_3 (Conv2D)              (None, 512, 64, 32)  6944        ['full_gated_conv2d_2[0][0]']    \n",
      "                                                                                                  \n",
      " batch_normalization_3 (BatchNo  (None, 512, 64, 32)  224        ['conv2d_3[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " global_average_pooling2d_3 (Gl  (None, 32)          0           ['batch_normalization_3[0][0]']  \n",
      " obalAveragePooling2D)                                                                            \n",
      "                                                                                                  \n",
      " reshape_3 (Reshape)            (None, 1, 1, 32)     0           ['global_average_pooling2d_3[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " dense_6 (Dense)                (None, 1, 1, 16)     512         ['reshape_3[0][0]']              \n",
      "                                                                                                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " dense_7 (Dense)                (None, 1, 1, 32)     512         ['dense_6[0][0]']                \n",
      "                                                                                                  \n",
      " multiply_3 (Multiply)          (None, 512, 64, 32)  0           ['batch_normalization_3[0][0]',  \n",
      "                                                                  'dense_7[0][0]']                \n",
      "                                                                                                  \n",
      " full_gated_conv2d_3 (FullGated  (None, 512, 64, 32)  18496      ['multiply_3[0][0]']             \n",
      " Conv2D)                                                                                          \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 512, 64, 32)  0           ['full_gated_conv2d_3[0][0]']    \n",
      "                                                                                                  \n",
      " conv2d_4 (Conv2D)              (None, 256, 16, 40)  11560       ['dropout[0][0]']                \n",
      "                                                                                                  \n",
      " batch_normalization_4 (BatchNo  (None, 256, 16, 40)  280        ['conv2d_4[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " global_average_pooling2d_4 (Gl  (None, 40)          0           ['batch_normalization_4[0][0]']  \n",
      " obalAveragePooling2D)                                                                            \n",
      "                                                                                                  \n",
      " reshape_4 (Reshape)            (None, 1, 1, 40)     0           ['global_average_pooling2d_4[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " dense_8 (Dense)                (None, 1, 1, 20)     800         ['reshape_4[0][0]']              \n",
      "                                                                                                  \n",
      " dense_9 (Dense)                (None, 1, 1, 40)     800         ['dense_8[0][0]']                \n",
      "                                                                                                  \n",
      " multiply_4 (Multiply)          (None, 256, 16, 40)  0           ['batch_normalization_4[0][0]',  \n",
      "                                                                  'dense_9[0][0]']                \n",
      "                                                                                                  \n",
      " full_gated_conv2d_4 (FullGated  (None, 256, 16, 40)  28880      ['multiply_4[0][0]']             \n",
      " Conv2D)                                                                                          \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 256, 16, 40)  0           ['full_gated_conv2d_4[0][0]']    \n",
      "                                                                                                  \n",
      " conv2d_5 (Conv2D)              (None, 256, 16, 48)  17328       ['dropout_1[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_5 (BatchNo  (None, 256, 16, 48)  336        ['conv2d_5[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " global_average_pooling2d_5 (Gl  (None, 48)          0           ['batch_normalization_5[0][0]']  \n",
      " obalAveragePooling2D)                                                                            \n",
      "                                                                                                  \n",
      " reshape_5 (Reshape)            (None, 1, 1, 48)     0           ['global_average_pooling2d_5[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " dense_10 (Dense)               (None, 1, 1, 24)     1152        ['reshape_5[0][0]']              \n",
      "                                                                                                  \n",
      " dense_11 (Dense)               (None, 1, 1, 48)     1152        ['dense_10[0][0]']               \n",
      "                                                                                                  \n",
      " multiply_5 (Multiply)          (None, 256, 16, 48)  0           ['batch_normalization_5[0][0]',  \n",
      "                                                                  'dense_11[0][0]']               \n",
      "                                                                                                  \n",
      " full_gated_conv2d_5 (FullGated  (None, 256, 16, 48)  41568      ['multiply_5[0][0]']             \n",
      " Conv2D)                                                                                          \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)            (None, 256, 16, 48)  0           ['full_gated_conv2d_5[0][0]']    \n",
      "                                                                                                  \n",
      " conv2d_6 (Conv2D)              (None, 128, 4, 56)   24248       ['dropout_2[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_6 (BatchNo  (None, 128, 4, 56)  392         ['conv2d_6[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " global_average_pooling2d_6 (Gl  (None, 56)          0           ['batch_normalization_6[0][0]']  \n",
      " obalAveragePooling2D)                                                                            \n",
      "                                                                                                  \n",
      " reshape_6 (Reshape)            (None, 1, 1, 56)     0           ['global_average_pooling2d_6[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " dense_12 (Dense)               (None, 1, 1, 28)     1568        ['reshape_6[0][0]']              \n",
      "                                                                                                  \n",
      " dense_13 (Dense)               (None, 1, 1, 56)     1568        ['dense_12[0][0]']               \n",
      "                                                                                                  \n",
      " multiply_6 (Multiply)          (None, 128, 4, 56)   0           ['batch_normalization_6[0][0]',  \n",
      "                                                                  'dense_13[0][0]']               \n",
      "                                                                                                  \n",
      " full_gated_conv2d_6 (FullGated  (None, 128, 4, 56)  56560       ['multiply_6[0][0]']             \n",
      " Conv2D)                                                                                          \n",
      "                                                                                                  \n",
      " dropout_3 (Dropout)            (None, 128, 4, 56)   0           ['full_gated_conv2d_6[0][0]']    \n",
      "                                                                                                  \n",
      " conv2d_7 (Conv2D)              (None, 128, 4, 64)   32320       ['dropout_3[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_7 (BatchNo  (None, 128, 4, 64)  448         ['conv2d_7[0][0]']               \n",
      " rmalization)                                                                                     \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                                  \n",
      " max_pooling2d (MaxPooling2D)   (None, 128, 2, 64)   0           ['batch_normalization_7[0][0]']  \n",
      "                                                                                                  \n",
      " reshape_7 (Reshape)            (None, 128, 128)     0           ['max_pooling2d[0][0]']          \n",
      "                                                                                                  \n",
      " tnc1_initial_conv77 (Conv1D)   (None, 128, 120)     15480       ['reshape_7[0][0]']              \n",
      "                                                                                                  \n",
      " tnc183_dilated_conv_1_tanh_s0   (None, 128, 120)    28920       ['tnc1_initial_conv77[0][0]']    \n",
      " (Conv1D)                                                                                         \n",
      "                                                                                                  \n",
      " activation (Activation)        (None, 128, 120)     0           ['tnc183_dilated_conv_1_tanh_s0[0\n",
      "                                                                 ][0]']                           \n",
      "                                                                                                  \n",
      " activation_1 (Activation)      (None, 128, 120)     0           ['tnc183_dilated_conv_1_tanh_s0[0\n",
      "                                                                 ][0]']                           \n",
      "                                                                                                  \n",
      " multiply_7 (Multiply)          (None, 128, 120)     0           ['activation[0][0]',             \n",
      "                                                                  'activation_1[0][0]']           \n",
      "                                                                                                  \n",
      " tnc158_spatial_dropout1d_1_s0_  (None, 128, 120)    0           ['multiply_7[0][0]']             \n",
      " 0.200000 (SpatialDropout1D)                                                                      \n",
      "                                                                                                  \n",
      " conv1d (Conv1D)                (None, 128, 120)     14520       ['tnc158_spatial_dropout1d_1_s0_0\n",
      "                                                                 .200000[0][0]']                  \n",
      "                                                                                                  \n",
      " add (Add)                      (None, 128, 120)     0           ['tnc1_initial_conv77[0][0]',    \n",
      "                                                                  'conv1d[0][0]']                 \n",
      "                                                                                                  \n",
      " tnc193_dilated_conv_2_tanh_s0   (None, 128, 120)    28920       ['add[0][0]']                    \n",
      " (Conv1D)                                                                                         \n",
      "                                                                                                  \n",
      " activation_2 (Activation)      (None, 128, 120)     0           ['tnc193_dilated_conv_2_tanh_s0[0\n",
      "                                                                 ][0]']                           \n",
      "                                                                                                  \n",
      " activation_3 (Activation)      (None, 128, 120)     0           ['tnc193_dilated_conv_2_tanh_s0[0\n",
      "                                                                 ][0]']                           \n",
      "                                                                                                  \n",
      " multiply_8 (Multiply)          (None, 128, 120)     0           ['activation_2[0][0]',           \n",
      "                                                                  'activation_3[0][0]']           \n",
      "                                                                                                  \n",
      " tnc184_spatial_dropout1d_2_s0_  (None, 128, 120)    0           ['multiply_8[0][0]']             \n",
      " 0.200000 (SpatialDropout1D)                                                                      \n",
      "                                                                                                  \n",
      " conv1d_1 (Conv1D)              (None, 128, 120)     14520       ['tnc184_spatial_dropout1d_2_s0_0\n",
      "                                                                 .200000[0][0]']                  \n",
      "                                                                                                  \n",
      " add_1 (Add)                    (None, 128, 120)     0           ['add[0][0]',                    \n",
      "                                                                  'conv1d_1[0][0]']               \n",
      "                                                                                                  \n",
      " tnc170_dilated_conv_4_tanh_s0   (None, 128, 120)    28920       ['add_1[0][0]']                  \n",
      " (Conv1D)                                                                                         \n",
      "                                                                                                  \n",
      " activation_4 (Activation)      (None, 128, 120)     0           ['tnc170_dilated_conv_4_tanh_s0[0\n",
      "                                                                 ][0]']                           \n",
      "                                                                                                  \n",
      " activation_5 (Activation)      (None, 128, 120)     0           ['tnc170_dilated_conv_4_tanh_s0[0\n",
      "                                                                 ][0]']                           \n",
      "                                                                                                  \n",
      " multiply_9 (Multiply)          (None, 128, 120)     0           ['activation_4[0][0]',           \n",
      "                                                                  'activation_5[0][0]']           \n",
      "                                                                                                  \n",
      " tnc157_spatial_dropout1d_4_s0_  (None, 128, 120)    0           ['multiply_9[0][0]']             \n",
      " 0.200000 (SpatialDropout1D)                                                                      \n",
      "                                                                                                  \n",
      " conv1d_2 (Conv1D)              (None, 128, 120)     14520       ['tnc157_spatial_dropout1d_4_s0_0\n",
      "                                                                 .200000[0][0]']                  \n",
      "                                                                                                  \n",
      " add_3 (Add)                    (None, 128, 120)     0           ['conv1d[0][0]',                 \n",
      "                                                                  'conv1d_1[0][0]',               \n",
      "                                                                  'conv1d_2[0][0]']               \n",
      "                                                                                                  \n",
      " activation_6 (Activation)      (None, 128, 120)     0           ['add_3[0][0]']                  \n",
      "                                                                                                  \n",
      " dense_14 (Dense)               (None, 128, 256)     30976       ['activation_6[0][0]']           \n",
      "                                                                                                  \n",
      " dense_15 (Dense)               (None, 128, 98)      25186       ['dense_14[0][0]']               \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 471,794\n",
      "Trainable params: 470,354\n",
      "Non-trainable params: 1,440\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-30 13:30:40.880699: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] layout failed: INVALID_ARGUMENT: Size of values 0 does not match size of permutation 4 @ fanin shape inmodel/dropout/dropout/SelectV2-2-TransposeNHWCToNCHW-LayoutOptimizer\n",
      "2023-09-30 13:30:42.282096: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:428] Loaded cuDNN version 8901\n",
      "2023-09-30 13:30:42.760376: W tensorflow/compiler/xla/stream_executor/gpu/asm_compiler.cc:115] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "You may not need to update to CUDA 11.1; cherry-picking the ptxas binary is often sufficient.\n",
      "2023-09-30 13:30:43.286846: I tensorflow/compiler/xla/service/service.cc:173] XLA service 0x55c703599130 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2023-09-30 13:30:43.286889: I tensorflow/compiler/xla/service/service.cc:181]   StreamExecutor device (0): NVIDIA GeForce RTX 2080 Ti, Compute Capability 7.5\n",
      "2023-09-30 13:30:43.296414: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2023-09-30 13:30:43.362642: W tensorflow/compiler/xla/stream_executor/gpu/asm_compiler.cc:115] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "You may not need to update to CUDA 11.1; cherry-picking the ptxas binary is often sufficient.\n",
      "2023-09-30 13:30:43.411352: I tensorflow/compiler/jit/xla_compilation_cache.cc:477] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2023-09-30 13:30:43.486604: W tensorflow/compiler/xla/stream_executor/gpu/asm_compiler.cc:115] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "You may not need to update to CUDA 11.1; cherry-picking the ptxas binary is often sufficient.\n",
      "2023-09-30 13:30:43.582367: W tensorflow/compiler/xla/stream_executor/gpu/asm_compiler.cc:115] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "You may not need to update to CUDA 11.1; cherry-picking the ptxas binary is often sufficient.\n",
      "2023-09-30 13:30:43.653476: W tensorflow/compiler/xla/stream_executor/gpu/asm_compiler.cc:115] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "You may not need to update to CUDA 11.1; cherry-picking the ptxas binary is often sufficient.\n",
      "2023-09-30 13:30:43.759344: W tensorflow/compiler/xla/stream_executor/gpu/asm_compiler.cc:115] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "You may not need to update to CUDA 11.1; cherry-picking the ptxas binary is often sufficient.\n",
      "2023-09-30 13:30:43.849631: W tensorflow/compiler/xla/stream_executor/gpu/asm_compiler.cc:115] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "You may not need to update to CUDA 11.1; cherry-picking the ptxas binary is often sufficient.\n",
      "2023-09-30 13:30:44.164627: W tensorflow/compiler/xla/stream_executor/gpu/asm_compiler.cc:115] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "You may not need to update to CUDA 11.1; cherry-picking the ptxas binary is often sufficient.\n",
      "2023-09-30 13:30:44.909150: W tensorflow/compiler/xla/stream_executor/gpu/asm_compiler.cc:115] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "You may not need to update to CUDA 11.1; cherry-picking the ptxas binary is often sufficient.\n",
      "2023-09-30 13:30:45.159854: W tensorflow/compiler/xla/stream_executor/gpu/asm_compiler.cc:115] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "You may not need to update to CUDA 11.1; cherry-picking the ptxas binary is often sufficient.\n",
      "2023-09-30 13:30:45.419359: W tensorflow/compiler/xla/stream_executor/gpu/asm_compiler.cc:115] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "You may not need to update to CUDA 11.1; cherry-picking the ptxas binary is often sufficient.\n",
      "2023-09-30 13:30:45.526269: W tensorflow/compiler/xla/stream_executor/gpu/asm_compiler.cc:115] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "You may not need to update to CUDA 11.1; cherry-picking the ptxas binary is often sufficient.\n",
      "2023-09-30 13:30:45.616450: W tensorflow/compiler/xla/stream_executor/gpu/asm_compiler.cc:115] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "You may not need to update to CUDA 11.1; cherry-picking the ptxas binary is often sufficient.\n",
      "2023-09-30 13:30:45.774366: W tensorflow/compiler/xla/stream_executor/gpu/asm_compiler.cc:115] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "You may not need to update to CUDA 11.1; cherry-picking the ptxas binary is often sufficient.\n",
      "2023-09-30 13:30:45.936509: W tensorflow/compiler/xla/stream_executor/gpu/asm_compiler.cc:115] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "You may not need to update to CUDA 11.1; cherry-picking the ptxas binary is often sufficient.\n",
      "2023-09-30 13:30:46.072958: W tensorflow/compiler/xla/stream_executor/gpu/asm_compiler.cc:115] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "You may not need to update to CUDA 11.1; cherry-picking the ptxas binary is often sufficient.\n",
      "2023-09-30 13:30:46.168450: W tensorflow/compiler/xla/stream_executor/gpu/asm_compiler.cc:115] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "You may not need to update to CUDA 11.1; cherry-picking the ptxas binary is often sufficient.\n",
      "2023-09-30 13:30:46.239957: W tensorflow/compiler/xla/stream_executor/gpu/asm_compiler.cc:115] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "You may not need to update to CUDA 11.1; cherry-picking the ptxas binary is often sufficient.\n",
      "2023-09-30 13:30:46.389775: W tensorflow/compiler/xla/stream_executor/gpu/asm_compiler.cc:115] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "You may not need to update to CUDA 11.1; cherry-picking the ptxas binary is often sufficient.\n",
      "2023-09-30 13:30:46.517258: W tensorflow/compiler/xla/stream_executor/gpu/asm_compiler.cc:115] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "You may not need to update to CUDA 11.1; cherry-picking the ptxas binary is often sufficient.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-30 13:30:46.694744: W tensorflow/compiler/xla/stream_executor/gpu/asm_compiler.cc:115] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "You may not need to update to CUDA 11.1; cherry-picking the ptxas binary is often sufficient.\n",
      "2023-09-30 13:30:46.764247: W tensorflow/compiler/xla/stream_executor/gpu/asm_compiler.cc:115] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "You may not need to update to CUDA 11.1; cherry-picking the ptxas binary is often sufficient.\n",
      "2023-09-30 13:30:46.911386: W tensorflow/compiler/xla/stream_executor/gpu/asm_compiler.cc:115] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "You may not need to update to CUDA 11.1; cherry-picking the ptxas binary is often sufficient.\n",
      "2023-09-30 13:30:47.038325: W tensorflow/compiler/xla/stream_executor/gpu/asm_compiler.cc:115] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "You may not need to update to CUDA 11.1; cherry-picking the ptxas binary is often sufficient.\n",
      "2023-09-30 13:30:47.343013: W tensorflow/compiler/xla/stream_executor/gpu/asm_compiler.cc:115] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "You may not need to update to CUDA 11.1; cherry-picking the ptxas binary is often sufficient.\n",
      "2023-09-30 13:30:47.452458: W tensorflow/compiler/xla/stream_executor/gpu/asm_compiler.cc:115] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "You may not need to update to CUDA 11.1; cherry-picking the ptxas binary is often sufficient.\n",
      "2023-09-30 13:30:47.627240: W tensorflow/compiler/xla/stream_executor/gpu/asm_compiler.cc:115] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "You may not need to update to CUDA 11.1; cherry-picking the ptxas binary is often sufficient.\n",
      "2023-09-30 13:30:47.859080: W tensorflow/compiler/xla/stream_executor/gpu/asm_compiler.cc:115] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "You may not need to update to CUDA 11.1; cherry-picking the ptxas binary is often sufficient.\n",
      "2023-09-30 13:30:47.968193: W tensorflow/compiler/xla/stream_executor/gpu/asm_compiler.cc:115] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "You may not need to update to CUDA 11.1; cherry-picking the ptxas binary is often sufficient.\n",
      "2023-09-30 13:30:48.080689: W tensorflow/compiler/xla/stream_executor/gpu/asm_compiler.cc:115] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "You may not need to update to CUDA 11.1; cherry-picking the ptxas binary is often sufficient.\n",
      "2023-09-30 13:30:48.295398: W tensorflow/compiler/xla/stream_executor/gpu/asm_compiler.cc:115] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "You may not need to update to CUDA 11.1; cherry-picking the ptxas binary is often sufficient.\n",
      "2023-09-30 13:30:48.476440: W tensorflow/compiler/xla/stream_executor/gpu/asm_compiler.cc:115] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "You may not need to update to CUDA 11.1; cherry-picking the ptxas binary is often sufficient.\n",
      "2023-09-30 13:30:48.639896: W tensorflow/compiler/xla/stream_executor/gpu/asm_compiler.cc:115] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "You may not need to update to CUDA 11.1; cherry-picking the ptxas binary is often sufficient.\n",
      "2023-09-30 13:30:48.805112: W tensorflow/compiler/xla/stream_executor/gpu/asm_compiler.cc:115] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "You may not need to update to CUDA 11.1; cherry-picking the ptxas binary is often sufficient.\n",
      "2023-09-30 13:30:49.473809: W tensorflow/compiler/xla/stream_executor/gpu/asm_compiler.cc:115] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "You may not need to update to CUDA 11.1; cherry-picking the ptxas binary is often sufficient.\n",
      "2023-09-30 13:30:49.665640: W tensorflow/compiler/xla/stream_executor/gpu/asm_compiler.cc:115] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "You may not need to update to CUDA 11.1; cherry-picking the ptxas binary is often sufficient.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "551/551 [==============================] - ETA: 0s - loss: 162.5226\n",
      "Epoch 1: val_loss improved from inf to 138.76109, saving model to ../output/bentham/shashankbest4senetgateddropouttwo/2023-09-30 13:30:27.081951/checkpoint_weights.hdf5\n",
      "551/551 [==============================] - 92s 142ms/step - loss: 162.5226 - val_loss: 138.7611 - lr: 0.0010\n",
      "Epoch 2/1000\n",
      "551/551 [==============================] - ETA: 0s - loss: 97.9600\n",
      "Epoch 2: val_loss improved from 138.76109 to 72.88609, saving model to ../output/bentham/shashankbest4senetgateddropouttwo/2023-09-30 13:30:27.081951/checkpoint_weights.hdf5\n",
      "551/551 [==============================] - 75s 137ms/step - loss: 97.9600 - val_loss: 72.8861 - lr: 0.0010\n",
      "Epoch 3/1000\n",
      "551/551 [==============================] - ETA: 0s - loss: 61.5727\n",
      "Epoch 3: val_loss improved from 72.88609 to 44.33010, saving model to ../output/bentham/shashankbest4senetgateddropouttwo/2023-09-30 13:30:27.081951/checkpoint_weights.hdf5\n",
      "551/551 [==============================] - 75s 136ms/step - loss: 61.5727 - val_loss: 44.3301 - lr: 0.0010\n",
      "Epoch 4/1000\n",
      "551/551 [==============================] - ETA: 0s - loss: 42.0811\n",
      "Epoch 4: val_loss improved from 44.33010 to 31.55932, saving model to ../output/bentham/shashankbest4senetgateddropouttwo/2023-09-30 13:30:27.081951/checkpoint_weights.hdf5\n",
      "551/551 [==============================] - 75s 137ms/step - loss: 42.0811 - val_loss: 31.5593 - lr: 0.0010\n",
      "Epoch 5/1000\n",
      "551/551 [==============================] - ETA: 0s - loss: 32.7723\n",
      "Epoch 5: val_loss improved from 31.55932 to 27.98490, saving model to ../output/bentham/shashankbest4senetgateddropouttwo/2023-09-30 13:30:27.081951/checkpoint_weights.hdf5\n",
      "551/551 [==============================] - 75s 137ms/step - loss: 32.7723 - val_loss: 27.9849 - lr: 0.0010\n",
      "Epoch 6/1000\n",
      "551/551 [==============================] - ETA: 0s - loss: 28.0755\n",
      "Epoch 6: val_loss improved from 27.98490 to 24.15055, saving model to ../output/bentham/shashankbest4senetgateddropouttwo/2023-09-30 13:30:27.081951/checkpoint_weights.hdf5\n",
      "551/551 [==============================] - 75s 137ms/step - loss: 28.0755 - val_loss: 24.1506 - lr: 0.0010\n",
      "Epoch 7/1000\n",
      "551/551 [==============================] - ETA: 0s - loss: 25.0168\n",
      "Epoch 7: val_loss improved from 24.15055 to 21.92824, saving model to ../output/bentham/shashankbest4senetgateddropouttwo/2023-09-30 13:30:27.081951/checkpoint_weights.hdf5\n",
      "551/551 [==============================] - 75s 137ms/step - loss: 25.0168 - val_loss: 21.9282 - lr: 0.0010\n",
      "Epoch 8/1000\n",
      "551/551 [==============================] - ETA: 0s - loss: 22.7825\n",
      "Epoch 8: val_loss improved from 21.92824 to 20.20506, saving model to ../output/bentham/shashankbest4senetgateddropouttwo/2023-09-30 13:30:27.081951/checkpoint_weights.hdf5\n",
      "551/551 [==============================] - 75s 137ms/step - loss: 22.7825 - val_loss: 20.2051 - lr: 0.0010\n",
      "Epoch 9/1000\n",
      "551/551 [==============================] - ETA: 0s - loss: 21.2311\n",
      "Epoch 9: val_loss improved from 20.20506 to 18.84083, saving model to ../output/bentham/shashankbest4senetgateddropouttwo/2023-09-30 13:30:27.081951/checkpoint_weights.hdf5\n",
      "551/551 [==============================] - 75s 137ms/step - loss: 21.2311 - val_loss: 18.8408 - lr: 0.0010\n",
      "Epoch 10/1000\n",
      "551/551 [==============================] - ETA: 0s - loss: 19.7820\n",
      "Epoch 10: val_loss improved from 18.84083 to 16.69365, saving model to ../output/bentham/shashankbest4senetgateddropouttwo/2023-09-30 13:30:27.081951/checkpoint_weights.hdf5\n",
      "551/551 [==============================] - 75s 137ms/step - loss: 19.7820 - val_loss: 16.6936 - lr: 0.0010\n",
      "Epoch 11/1000\n",
      "551/551 [==============================] - ETA: 0s - loss: 18.8175\n",
      "Epoch 11: val_loss improved from 16.69365 to 16.24938, saving model to ../output/bentham/shashankbest4senetgateddropouttwo/2023-09-30 13:30:27.081951/checkpoint_weights.hdf5\n",
      "551/551 [==============================] - 76s 137ms/step - loss: 18.8175 - val_loss: 16.2494 - lr: 0.0010\n",
      "Epoch 12/1000\n",
      "551/551 [==============================] - ETA: 0s - loss: 17.9856\n",
      "Epoch 12: val_loss improved from 16.24938 to 15.50597, saving model to ../output/bentham/shashankbest4senetgateddropouttwo/2023-09-30 13:30:27.081951/checkpoint_weights.hdf5\n",
      "551/551 [==============================] - 75s 136ms/step - loss: 17.9856 - val_loss: 15.5060 - lr: 0.0010\n",
      "Epoch 13/1000\n",
      "551/551 [==============================] - ETA: 0s - loss: 17.2496\n",
      "Epoch 13: val_loss did not improve from 15.50597\n",
      "551/551 [==============================] - 75s 137ms/step - loss: 17.2496 - val_loss: 16.3267 - lr: 0.0010\n",
      "Epoch 14/1000\n",
      "551/551 [==============================] - ETA: 0s - loss: 16.6291\n",
      "Epoch 14: val_loss did not improve from 15.50597\n",
      "551/551 [==============================] - 75s 136ms/step - loss: 16.6291 - val_loss: 15.7649 - lr: 0.0010\n",
      "Epoch 15/1000\n",
      "551/551 [==============================] - ETA: 0s - loss: 16.1107\n",
      "Epoch 15: val_loss improved from 15.50597 to 14.99195, saving model to ../output/bentham/shashankbest4senetgateddropouttwo/2023-09-30 13:30:27.081951/checkpoint_weights.hdf5\n",
      "551/551 [==============================] - 75s 136ms/step - loss: 16.1107 - val_loss: 14.9920 - lr: 0.0010\n",
      "Epoch 16/1000\n",
      "551/551 [==============================] - ETA: 0s - loss: 15.7733\n",
      "Epoch 16: val_loss improved from 14.99195 to 14.07901, saving model to ../output/bentham/shashankbest4senetgateddropouttwo/2023-09-30 13:30:27.081951/checkpoint_weights.hdf5\n",
      "551/551 [==============================] - 75s 136ms/step - loss: 15.7733 - val_loss: 14.0790 - lr: 0.0010\n",
      "Epoch 17/1000\n",
      "551/551 [==============================] - ETA: 0s - loss: 15.3529\n",
      "Epoch 17: val_loss did not improve from 14.07901\n",
      "551/551 [==============================] - 75s 135ms/step - loss: 15.3529 - val_loss: 14.3110 - lr: 0.0010\n",
      "Epoch 18/1000\n",
      "551/551 [==============================] - ETA: 0s - loss: 14.9716\n",
      "Epoch 18: val_loss did not improve from 14.07901\n",
      "551/551 [==============================] - 75s 136ms/step - loss: 14.9716 - val_loss: 14.3879 - lr: 0.0010\n",
      "Epoch 19/1000\n",
      "551/551 [==============================] - ETA: 0s - loss: 14.5677\n",
      "Epoch 19: val_loss improved from 14.07901 to 12.90375, saving model to ../output/bentham/shashankbest4senetgateddropouttwo/2023-09-30 13:30:27.081951/checkpoint_weights.hdf5\n",
      "551/551 [==============================] - 75s 136ms/step - loss: 14.5677 - val_loss: 12.9038 - lr: 0.0010\n",
      "Epoch 20/1000\n",
      "551/551 [==============================] - ETA: 0s - loss: 14.1883\n",
      "Epoch 20: val_loss did not improve from 12.90375\n",
      "551/551 [==============================] - 75s 136ms/step - loss: 14.1883 - val_loss: 12.9120 - lr: 0.0010\n",
      "Epoch 21/1000\n",
      "551/551 [==============================] - ETA: 0s - loss: 13.8611\n",
      "Epoch 21: val_loss did not improve from 12.90375\n",
      "551/551 [==============================] - 75s 136ms/step - loss: 13.8611 - val_loss: 13.3185 - lr: 0.0010\n",
      "Epoch 22/1000\n",
      "551/551 [==============================] - ETA: 0s - loss: 13.6476\n",
      "Epoch 22: val_loss improved from 12.90375 to 12.90049, saving model to ../output/bentham/shashankbest4senetgateddropouttwo/2023-09-30 13:30:27.081951/checkpoint_weights.hdf5\n",
      "551/551 [==============================] - 75s 136ms/step - loss: 13.6476 - val_loss: 12.9005 - lr: 0.0010\n",
      "Epoch 23/1000\n",
      "551/551 [==============================] - ETA: 0s - loss: 13.3944\n",
      "Epoch 23: val_loss improved from 12.90049 to 12.42218, saving model to ../output/bentham/shashankbest4senetgateddropouttwo/2023-09-30 13:30:27.081951/checkpoint_weights.hdf5\n",
      "551/551 [==============================] - 75s 136ms/step - loss: 13.3944 - val_loss: 12.4222 - lr: 0.0010\n",
      "Epoch 24/1000\n",
      "551/551 [==============================] - ETA: 0s - loss: 13.2526\n",
      "Epoch 24: val_loss improved from 12.42218 to 12.24968, saving model to ../output/bentham/shashankbest4senetgateddropouttwo/2023-09-30 13:30:27.081951/checkpoint_weights.hdf5\n",
      "551/551 [==============================] - 75s 136ms/step - loss: 13.2526 - val_loss: 12.2497 - lr: 0.0010\n",
      "Epoch 25/1000\n",
      "551/551 [==============================] - ETA: 0s - loss: 12.9596\n",
      "Epoch 25: val_loss did not improve from 12.24968\n",
      "551/551 [==============================] - 75s 136ms/step - loss: 12.9596 - val_loss: 12.7456 - lr: 0.0010\n",
      "Epoch 26/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "551/551 [==============================] - ETA: 0s - loss: 12.7829\n",
      "Epoch 26: val_loss did not improve from 12.24968\n",
      "551/551 [==============================] - 75s 136ms/step - loss: 12.7829 - val_loss: 12.6552 - lr: 0.0010\n",
      "Epoch 27/1000\n",
      "551/551 [==============================] - ETA: 0s - loss: 12.6732\n",
      "Epoch 27: val_loss improved from 12.24968 to 11.97103, saving model to ../output/bentham/shashankbest4senetgateddropouttwo/2023-09-30 13:30:27.081951/checkpoint_weights.hdf5\n",
      "551/551 [==============================] - 75s 136ms/step - loss: 12.6732 - val_loss: 11.9710 - lr: 0.0010\n",
      "Epoch 28/1000\n",
      "551/551 [==============================] - ETA: 0s - loss: 12.4573\n",
      "Epoch 28: val_loss did not improve from 11.97103\n",
      "551/551 [==============================] - 75s 136ms/step - loss: 12.4573 - val_loss: 12.0983 - lr: 0.0010\n",
      "Epoch 29/1000\n",
      "551/551 [==============================] - ETA: 0s - loss: 12.4114\n",
      "Epoch 29: val_loss did not improve from 11.97103\n",
      "551/551 [==============================] - 75s 136ms/step - loss: 12.4114 - val_loss: 12.0254 - lr: 0.0010\n",
      "Epoch 30/1000\n",
      "551/551 [==============================] - ETA: 0s - loss: 12.2177\n",
      "Epoch 30: val_loss did not improve from 11.97103\n",
      "551/551 [==============================] - 75s 136ms/step - loss: 12.2177 - val_loss: 12.0552 - lr: 0.0010\n",
      "Epoch 31/1000\n",
      "551/551 [==============================] - ETA: 0s - loss: 12.1011\n",
      "Epoch 31: val_loss improved from 11.97103 to 11.69539, saving model to ../output/bentham/shashankbest4senetgateddropouttwo/2023-09-30 13:30:27.081951/checkpoint_weights.hdf5\n",
      "551/551 [==============================] - 75s 137ms/step - loss: 12.1011 - val_loss: 11.6954 - lr: 0.0010\n",
      "Epoch 32/1000\n",
      "551/551 [==============================] - ETA: 0s - loss: 11.9121\n",
      "Epoch 32: val_loss did not improve from 11.69539\n",
      "551/551 [==============================] - 75s 136ms/step - loss: 11.9121 - val_loss: 11.8857 - lr: 0.0010\n",
      "Epoch 33/1000\n",
      "551/551 [==============================] - ETA: 0s - loss: 11.9623\n",
      "Epoch 33: val_loss improved from 11.69539 to 11.52319, saving model to ../output/bentham/shashankbest4senetgateddropouttwo/2023-09-30 13:30:27.081951/checkpoint_weights.hdf5\n",
      "551/551 [==============================] - 75s 136ms/step - loss: 11.9623 - val_loss: 11.5232 - lr: 0.0010\n",
      "Epoch 34/1000\n",
      "551/551 [==============================] - ETA: 0s - loss: 11.6649\n",
      "Epoch 34: val_loss improved from 11.52319 to 10.82480, saving model to ../output/bentham/shashankbest4senetgateddropouttwo/2023-09-30 13:30:27.081951/checkpoint_weights.hdf5\n",
      "551/551 [==============================] - 75s 136ms/step - loss: 11.6649 - val_loss: 10.8248 - lr: 0.0010\n",
      "Epoch 35/1000\n",
      "551/551 [==============================] - ETA: 0s - loss: 11.4184\n",
      "Epoch 35: val_loss did not improve from 10.82480\n",
      "551/551 [==============================] - 75s 136ms/step - loss: 11.4184 - val_loss: 11.5047 - lr: 0.0010\n",
      "Epoch 36/1000\n",
      "551/551 [==============================] - ETA: 0s - loss: 11.4253\n",
      "Epoch 36: val_loss did not improve from 10.82480\n",
      "551/551 [==============================] - 75s 136ms/step - loss: 11.4253 - val_loss: 11.6838 - lr: 0.0010\n",
      "Epoch 37/1000\n",
      "551/551 [==============================] - ETA: 0s - loss: 11.3803\n",
      "Epoch 37: val_loss did not improve from 10.82480\n",
      "551/551 [==============================] - 75s 136ms/step - loss: 11.3803 - val_loss: 11.4084 - lr: 0.0010\n",
      "Epoch 38/1000\n",
      "551/551 [==============================] - ETA: 0s - loss: 11.1423\n",
      "Epoch 38: val_loss did not improve from 10.82480\n",
      "551/551 [==============================] - 75s 136ms/step - loss: 11.1423 - val_loss: 11.8844 - lr: 0.0010\n",
      "Epoch 39/1000\n",
      "551/551 [==============================] - ETA: 0s - loss: 11.0758\n",
      "Epoch 39: val_loss did not improve from 10.82480\n",
      "551/551 [==============================] - 75s 136ms/step - loss: 11.0758 - val_loss: 11.4570 - lr: 0.0010\n",
      "Epoch 40/1000\n",
      "551/551 [==============================] - ETA: 0s - loss: 10.9406\n",
      "Epoch 40: val_loss did not improve from 10.82480\n",
      "551/551 [==============================] - 75s 135ms/step - loss: 10.9406 - val_loss: 11.0337 - lr: 0.0010\n",
      "Epoch 41/1000\n",
      "551/551 [==============================] - ETA: 0s - loss: 10.8669\n",
      "Epoch 41: val_loss did not improve from 10.82480\n",
      "551/551 [==============================] - 75s 136ms/step - loss: 10.8669 - val_loss: 11.1733 - lr: 0.0010\n",
      "Epoch 42/1000\n",
      "551/551 [==============================] - ETA: 0s - loss: 10.8489\n",
      "Epoch 42: val_loss did not improve from 10.82480\n",
      "551/551 [==============================] - 75s 135ms/step - loss: 10.8489 - val_loss: 10.9979 - lr: 0.0010\n",
      "Epoch 43/1000\n",
      "551/551 [==============================] - ETA: 0s - loss: 10.8129\n",
      "Epoch 43: val_loss did not improve from 10.82480\n",
      "551/551 [==============================] - 75s 135ms/step - loss: 10.8129 - val_loss: 11.1602 - lr: 0.0010\n",
      "Epoch 44/1000\n",
      "551/551 [==============================] - ETA: 0s - loss: 10.7455\n",
      "Epoch 44: val_loss did not improve from 10.82480\n",
      "551/551 [==============================] - 75s 136ms/step - loss: 10.7455 - val_loss: 11.0233 - lr: 0.0010\n",
      "Epoch 45/1000\n",
      "551/551 [==============================] - ETA: 0s - loss: 10.5672\n",
      "Epoch 45: val_loss did not improve from 10.82480\n",
      "551/551 [==============================] - 75s 136ms/step - loss: 10.5672 - val_loss: 10.8854 - lr: 0.0010\n",
      "Epoch 46/1000\n",
      "551/551 [==============================] - ETA: 0s - loss: 10.5355\n",
      "Epoch 46: val_loss improved from 10.82480 to 10.72900, saving model to ../output/bentham/shashankbest4senetgateddropouttwo/2023-09-30 13:30:27.081951/checkpoint_weights.hdf5\n",
      "551/551 [==============================] - 75s 135ms/step - loss: 10.5355 - val_loss: 10.7290 - lr: 0.0010\n",
      "Epoch 47/1000\n",
      "551/551 [==============================] - ETA: 0s - loss: 10.5351\n",
      "Epoch 47: val_loss did not improve from 10.72900\n",
      "551/551 [==============================] - 75s 136ms/step - loss: 10.5351 - val_loss: 10.8751 - lr: 0.0010\n",
      "Epoch 48/1000\n",
      "551/551 [==============================] - ETA: 0s - loss: 10.3821\n",
      "Epoch 48: val_loss improved from 10.72900 to 10.71196, saving model to ../output/bentham/shashankbest4senetgateddropouttwo/2023-09-30 13:30:27.081951/checkpoint_weights.hdf5\n",
      "551/551 [==============================] - 75s 136ms/step - loss: 10.3821 - val_loss: 10.7120 - lr: 0.0010\n",
      "Epoch 49/1000\n",
      "551/551 [==============================] - ETA: 0s - loss: 10.2992\n",
      "Epoch 49: val_loss improved from 10.71196 to 10.65116, saving model to ../output/bentham/shashankbest4senetgateddropouttwo/2023-09-30 13:30:27.081951/checkpoint_weights.hdf5\n",
      "551/551 [==============================] - 75s 136ms/step - loss: 10.2992 - val_loss: 10.6512 - lr: 0.0010\n",
      "Epoch 50/1000\n",
      "551/551 [==============================] - ETA: 0s - loss: 10.2093\n",
      "Epoch 50: val_loss did not improve from 10.65116\n",
      "551/551 [==============================] - 75s 135ms/step - loss: 10.2093 - val_loss: 10.9122 - lr: 0.0010\n",
      "Epoch 51/1000\n",
      "551/551 [==============================] - ETA: 0s - loss: 10.1759\n",
      "Epoch 51: val_loss improved from 10.65116 to 10.61150, saving model to ../output/bentham/shashankbest4senetgateddropouttwo/2023-09-30 13:30:27.081951/checkpoint_weights.hdf5\n",
      "551/551 [==============================] - 75s 137ms/step - loss: 10.1759 - val_loss: 10.6115 - lr: 0.0010\n",
      "Epoch 52/1000\n",
      "551/551 [==============================] - ETA: 0s - loss: 10.2012\n",
      "Epoch 52: val_loss did not improve from 10.61150\n",
      "551/551 [==============================] - 75s 136ms/step - loss: 10.2012 - val_loss: 10.9560 - lr: 0.0010\n",
      "Epoch 53/1000\n",
      "551/551 [==============================] - ETA: 0s - loss: 10.1258\n",
      "Epoch 53: val_loss improved from 10.61150 to 10.60611, saving model to ../output/bentham/shashankbest4senetgateddropouttwo/2023-09-30 13:30:27.081951/checkpoint_weights.hdf5\n",
      "551/551 [==============================] - 75s 136ms/step - loss: 10.1258 - val_loss: 10.6061 - lr: 0.0010\n",
      "Epoch 54/1000\n",
      "551/551 [==============================] - ETA: 0s - loss: 10.0127\n",
      "Epoch 54: val_loss improved from 10.60611 to 10.36669, saving model to ../output/bentham/shashankbest4senetgateddropouttwo/2023-09-30 13:30:27.081951/checkpoint_weights.hdf5\n",
      "551/551 [==============================] - 75s 136ms/step - loss: 10.0127 - val_loss: 10.3667 - lr: 0.0010\n",
      "Epoch 55/1000\n",
      "551/551 [==============================] - ETA: 0s - loss: 9.9647\n",
      "Epoch 55: val_loss did not improve from 10.36669\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "551/551 [==============================] - 75s 135ms/step - loss: 9.9647 - val_loss: 10.8236 - lr: 0.0010\n",
      "Epoch 56/1000\n",
      "551/551 [==============================] - ETA: 0s - loss: 9.9084\n",
      "Epoch 56: val_loss did not improve from 10.36669\n",
      "551/551 [==============================] - 75s 136ms/step - loss: 9.9084 - val_loss: 10.4559 - lr: 0.0010\n",
      "Epoch 57/1000\n",
      "551/551 [==============================] - ETA: 0s - loss: 9.8554\n",
      "Epoch 57: val_loss did not improve from 10.36669\n",
      "551/551 [==============================] - 75s 136ms/step - loss: 9.8554 - val_loss: 10.4205 - lr: 0.0010\n",
      "Epoch 58/1000\n",
      "551/551 [==============================] - ETA: 0s - loss: 9.8903\n",
      "Epoch 58: val_loss did not improve from 10.36669\n",
      "551/551 [==============================] - 75s 136ms/step - loss: 9.8903 - val_loss: 10.8334 - lr: 0.0010\n",
      "Epoch 59/1000\n",
      "551/551 [==============================] - ETA: 0s - loss: 9.8099\n",
      "Epoch 59: val_loss did not improve from 10.36669\n",
      "551/551 [==============================] - 75s 136ms/step - loss: 9.8099 - val_loss: 10.6009 - lr: 0.0010\n",
      "Epoch 60/1000\n",
      "551/551 [==============================] - ETA: 0s - loss: 9.7567\n",
      "Epoch 60: val_loss did not improve from 10.36669\n",
      "551/551 [==============================] - 75s 136ms/step - loss: 9.7567 - val_loss: 10.5968 - lr: 0.0010\n",
      "Epoch 61/1000\n",
      "551/551 [==============================] - ETA: 0s - loss: 9.5496\n",
      "Epoch 61: val_loss improved from 10.36669 to 10.06710, saving model to ../output/bentham/shashankbest4senetgateddropouttwo/2023-09-30 13:30:27.081951/checkpoint_weights.hdf5\n",
      "551/551 [==============================] - 75s 136ms/step - loss: 9.5496 - val_loss: 10.0671 - lr: 0.0010\n",
      "Epoch 62/1000\n",
      "551/551 [==============================] - ETA: 0s - loss: 9.6534\n",
      "Epoch 62: val_loss did not improve from 10.06710\n",
      "551/551 [==============================] - 75s 135ms/step - loss: 9.6534 - val_loss: 10.5164 - lr: 0.0010\n",
      "Epoch 63/1000\n",
      "551/551 [==============================] - ETA: 0s - loss: 9.6341\n",
      "Epoch 63: val_loss did not improve from 10.06710\n",
      "551/551 [==============================] - 75s 136ms/step - loss: 9.6341 - val_loss: 10.3452 - lr: 0.0010\n",
      "Epoch 64/1000\n",
      "551/551 [==============================] - ETA: 0s - loss: 9.5746\n",
      "Epoch 64: val_loss did not improve from 10.06710\n",
      "551/551 [==============================] - 75s 135ms/step - loss: 9.5746 - val_loss: 10.6333 - lr: 0.0010\n",
      "Epoch 65/1000\n",
      "551/551 [==============================] - ETA: 0s - loss: 9.6150\n",
      "Epoch 65: val_loss did not improve from 10.06710\n",
      "551/551 [==============================] - 74s 135ms/step - loss: 9.6150 - val_loss: 10.4890 - lr: 0.0010\n",
      "Epoch 66/1000\n",
      "551/551 [==============================] - ETA: 0s - loss: 9.4351\n",
      "Epoch 66: val_loss did not improve from 10.06710\n",
      "551/551 [==============================] - 74s 135ms/step - loss: 9.4351 - val_loss: 10.5705 - lr: 0.0010\n",
      "Epoch 67/1000\n",
      "551/551 [==============================] - ETA: 0s - loss: 9.4162\n",
      "Epoch 67: val_loss improved from 10.06710 to 9.94621, saving model to ../output/bentham/shashankbest4senetgateddropouttwo/2023-09-30 13:30:27.081951/checkpoint_weights.hdf5\n",
      "551/551 [==============================] - 75s 135ms/step - loss: 9.4162 - val_loss: 9.9462 - lr: 0.0010\n",
      "Epoch 68/1000\n",
      "551/551 [==============================] - ETA: 0s - loss: 9.3568\n",
      "Epoch 68: val_loss did not improve from 9.94621\n",
      "551/551 [==============================] - 75s 136ms/step - loss: 9.3568 - val_loss: 9.9534 - lr: 0.0010\n",
      "Epoch 69/1000\n",
      "551/551 [==============================] - ETA: 0s - loss: 9.3117\n",
      "Epoch 69: val_loss did not improve from 9.94621\n",
      "551/551 [==============================] - 75s 136ms/step - loss: 9.3117 - val_loss: 10.4540 - lr: 0.0010\n",
      "Epoch 70/1000\n",
      "551/551 [==============================] - ETA: 0s - loss: 9.2783\n",
      "Epoch 70: val_loss did not improve from 9.94621\n",
      "551/551 [==============================] - 75s 136ms/step - loss: 9.2783 - val_loss: 10.1921 - lr: 0.0010\n",
      "Epoch 71/1000\n",
      "551/551 [==============================] - ETA: 0s - loss: 9.2489\n",
      "Epoch 71: val_loss did not improve from 9.94621\n",
      "551/551 [==============================] - 75s 137ms/step - loss: 9.2489 - val_loss: 10.4115 - lr: 0.0010\n",
      "Epoch 72/1000\n",
      "551/551 [==============================] - ETA: 0s - loss: 9.2027\n",
      "Epoch 72: val_loss did not improve from 9.94621\n",
      "551/551 [==============================] - 75s 137ms/step - loss: 9.2027 - val_loss: 10.0527 - lr: 0.0010\n",
      "Epoch 73/1000\n",
      "551/551 [==============================] - ETA: 0s - loss: 9.2581\n",
      "Epoch 73: val_loss did not improve from 9.94621\n",
      "551/551 [==============================] - 75s 136ms/step - loss: 9.2581 - val_loss: 10.2683 - lr: 0.0010\n",
      "Epoch 74/1000\n",
      "551/551 [==============================] - ETA: 0s - loss: 9.1284\n",
      "Epoch 74: val_loss improved from 9.94621 to 9.92657, saving model to ../output/bentham/shashankbest4senetgateddropouttwo/2023-09-30 13:30:27.081951/checkpoint_weights.hdf5\n",
      "551/551 [==============================] - 75s 137ms/step - loss: 9.1284 - val_loss: 9.9266 - lr: 0.0010\n",
      "Epoch 75/1000\n",
      "551/551 [==============================] - ETA: 0s - loss: 9.1198\n",
      "Epoch 75: val_loss did not improve from 9.92657\n",
      "551/551 [==============================] - 75s 136ms/step - loss: 9.1198 - val_loss: 10.5871 - lr: 0.0010\n",
      "Epoch 76/1000\n",
      "551/551 [==============================] - ETA: 0s - loss: 9.2163\n",
      "Epoch 76: val_loss did not improve from 9.92657\n",
      "551/551 [==============================] - 75s 136ms/step - loss: 9.2163 - val_loss: 10.0540 - lr: 0.0010\n",
      "Epoch 77/1000\n",
      "551/551 [==============================] - ETA: 0s - loss: 9.0827\n",
      "Epoch 77: val_loss did not improve from 9.92657\n",
      "551/551 [==============================] - 75s 136ms/step - loss: 9.0827 - val_loss: 9.9931 - lr: 0.0010\n",
      "Epoch 78/1000\n",
      "551/551 [==============================] - ETA: 0s - loss: 9.0460\n",
      "Epoch 78: val_loss did not improve from 9.92657\n",
      "551/551 [==============================] - 75s 136ms/step - loss: 9.0460 - val_loss: 10.2122 - lr: 0.0010\n",
      "Epoch 79/1000\n",
      "551/551 [==============================] - ETA: 0s - loss: 9.0848\n",
      "Epoch 79: val_loss did not improve from 9.92657\n",
      "551/551 [==============================] - 75s 136ms/step - loss: 9.0848 - val_loss: 10.0487 - lr: 0.0010\n",
      "Epoch 80/1000\n",
      "551/551 [==============================] - ETA: 0s - loss: 8.9450\n",
      "Epoch 80: val_loss did not improve from 9.92657\n",
      "551/551 [==============================] - 75s 136ms/step - loss: 8.9450 - val_loss: 10.2209 - lr: 0.0010\n",
      "Epoch 81/1000\n",
      "551/551 [==============================] - ETA: 0s - loss: 9.0057\n",
      "Epoch 81: val_loss did not improve from 9.92657\n",
      "551/551 [==============================] - 75s 137ms/step - loss: 9.0057 - val_loss: 9.9758 - lr: 0.0010\n",
      "Epoch 82/1000\n",
      "551/551 [==============================] - ETA: 0s - loss: 8.8880\n",
      "Epoch 82: val_loss did not improve from 9.92657\n",
      "551/551 [==============================] - 75s 136ms/step - loss: 8.8880 - val_loss: 10.1474 - lr: 0.0010\n",
      "Epoch 83/1000\n",
      "551/551 [==============================] - ETA: 0s - loss: 8.9202\n",
      "Epoch 83: val_loss did not improve from 9.92657\n",
      "551/551 [==============================] - 75s 135ms/step - loss: 8.9202 - val_loss: 9.9381 - lr: 0.0010\n",
      "Epoch 84/1000\n",
      "551/551 [==============================] - ETA: 0s - loss: 8.9063\n",
      "Epoch 84: val_loss improved from 9.92657 to 9.60826, saving model to ../output/bentham/shashankbest4senetgateddropouttwo/2023-09-30 13:30:27.081951/checkpoint_weights.hdf5\n",
      "551/551 [==============================] - 75s 136ms/step - loss: 8.9063 - val_loss: 9.6083 - lr: 0.0010\n",
      "Epoch 85/1000\n",
      "551/551 [==============================] - ETA: 0s - loss: 8.9744\n",
      "Epoch 85: val_loss did not improve from 9.60826\n",
      "551/551 [==============================] - 75s 135ms/step - loss: 8.9744 - val_loss: 10.3706 - lr: 0.0010\n",
      "Epoch 86/1000\n",
      "551/551 [==============================] - ETA: 0s - loss: 8.7722\n",
      "Epoch 86: val_loss did not improve from 9.60826\n",
      "551/551 [==============================] - 75s 136ms/step - loss: 8.7722 - val_loss: 10.0999 - lr: 0.0010\n",
      "Epoch 87/1000\n",
      "551/551 [==============================] - ETA: 0s - loss: 8.7787\n",
      "Epoch 87: val_loss did not improve from 9.60826\n",
      "551/551 [==============================] - 75s 136ms/step - loss: 8.7787 - val_loss: 10.4231 - lr: 0.0010\n",
      "Epoch 88/1000\n",
      "551/551 [==============================] - ETA: 0s - loss: 8.8656\n",
      "Epoch 88: val_loss did not improve from 9.60826\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "551/551 [==============================] - 75s 136ms/step - loss: 8.8656 - val_loss: 9.8049 - lr: 0.0010\n",
      "Epoch 89/1000\n",
      "551/551 [==============================] - ETA: 0s - loss: 8.7022\n",
      "Epoch 89: val_loss did not improve from 9.60826\n",
      "551/551 [==============================] - 75s 136ms/step - loss: 8.7022 - val_loss: 9.8887 - lr: 0.0010\n",
      "Epoch 90/1000\n",
      "551/551 [==============================] - ETA: 0s - loss: 8.9058\n",
      "Epoch 90: val_loss did not improve from 9.60826\n",
      "551/551 [==============================] - 75s 136ms/step - loss: 8.9058 - val_loss: 9.9098 - lr: 0.0010\n",
      "Epoch 91/1000\n",
      "551/551 [==============================] - ETA: 0s - loss: 8.6899\n",
      "Epoch 91: val_loss did not improve from 9.60826\n",
      "551/551 [==============================] - 75s 137ms/step - loss: 8.6899 - val_loss: 10.3815 - lr: 0.0010\n",
      "Epoch 92/1000\n",
      "551/551 [==============================] - ETA: 0s - loss: 8.6955\n",
      "Epoch 92: val_loss did not improve from 9.60826\n",
      "551/551 [==============================] - 75s 136ms/step - loss: 8.6955 - val_loss: 9.8427 - lr: 0.0010\n",
      "Epoch 93/1000\n",
      "551/551 [==============================] - ETA: 0s - loss: 8.7016\n",
      "Epoch 93: val_loss did not improve from 9.60826\n",
      "551/551 [==============================] - 75s 136ms/step - loss: 8.7016 - val_loss: 9.9159 - lr: 0.0010\n",
      "Epoch 94/1000\n",
      "551/551 [==============================] - ETA: 0s - loss: 8.6230\n",
      "Epoch 94: val_loss did not improve from 9.60826\n",
      "551/551 [==============================] - 75s 136ms/step - loss: 8.6230 - val_loss: 10.0679 - lr: 0.0010\n",
      "Epoch 95/1000\n",
      "551/551 [==============================] - ETA: 0s - loss: 8.6092\n",
      "Epoch 95: val_loss did not improve from 9.60826\n",
      "551/551 [==============================] - 75s 136ms/step - loss: 8.6092 - val_loss: 10.4165 - lr: 0.0010\n",
      "Epoch 96/1000\n",
      "551/551 [==============================] - ETA: 0s - loss: 8.5212\n",
      "Epoch 96: val_loss did not improve from 9.60826\n",
      "551/551 [==============================] - 75s 136ms/step - loss: 8.5212 - val_loss: 9.7318 - lr: 0.0010\n",
      "Epoch 97/1000\n",
      "551/551 [==============================] - ETA: 0s - loss: 8.6317\n",
      "Epoch 97: val_loss did not improve from 9.60826\n",
      "551/551 [==============================] - 75s 136ms/step - loss: 8.6317 - val_loss: 9.6309 - lr: 0.0010\n",
      "Epoch 98/1000\n",
      "551/551 [==============================] - ETA: 0s - loss: 8.5780\n",
      "Epoch 98: val_loss did not improve from 9.60826\n",
      "551/551 [==============================] - 75s 136ms/step - loss: 8.5780 - val_loss: 10.1130 - lr: 0.0010\n",
      "Epoch 99/1000\n",
      "551/551 [==============================] - ETA: 0s - loss: 8.4411\n",
      "Epoch 99: val_loss did not improve from 9.60826\n",
      "\n",
      "Epoch 99: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "551/551 [==============================] - 75s 136ms/step - loss: 8.4411 - val_loss: 9.8493 - lr: 0.0010\n",
      "Epoch 100/1000\n",
      "551/551 [==============================] - ETA: 0s - loss: 7.3661\n",
      "Epoch 100: val_loss improved from 9.60826 to 8.95524, saving model to ../output/bentham/shashankbest4senetgateddropouttwo/2023-09-30 13:30:27.081951/checkpoint_weights.hdf5\n",
      "551/551 [==============================] - 75s 136ms/step - loss: 7.3661 - val_loss: 8.9552 - lr: 2.0000e-04\n",
      "Epoch 101/1000\n",
      "551/551 [==============================] - ETA: 0s - loss: 6.8961\n",
      "Epoch 101: val_loss improved from 8.95524 to 8.91882, saving model to ../output/bentham/shashankbest4senetgateddropouttwo/2023-09-30 13:30:27.081951/checkpoint_weights.hdf5\n",
      "551/551 [==============================] - 76s 137ms/step - loss: 6.8961 - val_loss: 8.9188 - lr: 2.0000e-04\n",
      "Epoch 102/1000\n",
      "551/551 [==============================] - ETA: 0s - loss: 6.8564\n",
      "Epoch 102: val_loss improved from 8.91882 to 8.75343, saving model to ../output/bentham/shashankbest4senetgateddropouttwo/2023-09-30 13:30:27.081951/checkpoint_weights.hdf5\n",
      "551/551 [==============================] - 75s 136ms/step - loss: 6.8564 - val_loss: 8.7534 - lr: 2.0000e-04\n",
      "Epoch 103/1000\n",
      "551/551 [==============================] - ETA: 0s - loss: 6.6829\n",
      "Epoch 103: val_loss did not improve from 8.75343\n",
      "551/551 [==============================] - 75s 136ms/step - loss: 6.6829 - val_loss: 8.9114 - lr: 2.0000e-04\n",
      "Epoch 104/1000\n",
      "551/551 [==============================] - ETA: 0s - loss: 6.6207\n",
      "Epoch 104: val_loss did not improve from 8.75343\n",
      "551/551 [==============================] - 75s 136ms/step - loss: 6.6207 - val_loss: 8.8214 - lr: 2.0000e-04\n",
      "Epoch 105/1000\n",
      "551/551 [==============================] - ETA: 0s - loss: 6.5234\n",
      "Epoch 105: val_loss did not improve from 8.75343\n",
      "551/551 [==============================] - 75s 135ms/step - loss: 6.5234 - val_loss: 8.8242 - lr: 2.0000e-04\n",
      "Epoch 106/1000\n",
      "551/551 [==============================] - ETA: 0s - loss: 6.5363\n",
      "Epoch 106: val_loss did not improve from 8.75343\n",
      "551/551 [==============================] - 75s 136ms/step - loss: 6.5363 - val_loss: 8.9032 - lr: 2.0000e-04\n",
      "Epoch 107/1000\n",
      "551/551 [==============================] - ETA: 0s - loss: 6.4417\n",
      "Epoch 107: val_loss did not improve from 8.75343\n",
      "551/551 [==============================] - 75s 136ms/step - loss: 6.4417 - val_loss: 8.7906 - lr: 2.0000e-04\n",
      "Epoch 108/1000\n",
      "551/551 [==============================] - ETA: 0s - loss: 6.5339\n",
      "Epoch 108: val_loss did not improve from 8.75343\n",
      "551/551 [==============================] - 75s 136ms/step - loss: 6.5339 - val_loss: 8.7948 - lr: 2.0000e-04\n",
      "Epoch 109/1000\n",
      "551/551 [==============================] - ETA: 0s - loss: 6.3806\n",
      "Epoch 109: val_loss improved from 8.75343 to 8.69255, saving model to ../output/bentham/shashankbest4senetgateddropouttwo/2023-09-30 13:30:27.081951/checkpoint_weights.hdf5\n",
      "551/551 [==============================] - 75s 137ms/step - loss: 6.3806 - val_loss: 8.6925 - lr: 2.0000e-04\n",
      "Epoch 110/1000\n",
      "551/551 [==============================] - ETA: 0s - loss: 6.4299\n",
      "Epoch 110: val_loss did not improve from 8.69255\n",
      "551/551 [==============================] - 75s 136ms/step - loss: 6.4299 - val_loss: 8.7395 - lr: 2.0000e-04\n",
      "Epoch 111/1000\n",
      "551/551 [==============================] - ETA: 0s - loss: 6.4479\n",
      "Epoch 111: val_loss did not improve from 8.69255\n",
      "551/551 [==============================] - 75s 136ms/step - loss: 6.4479 - val_loss: 8.8811 - lr: 2.0000e-04\n",
      "Epoch 112/1000\n",
      "551/551 [==============================] - ETA: 0s - loss: 6.2858\n",
      "Epoch 112: val_loss did not improve from 8.69255\n",
      "551/551 [==============================] - 75s 136ms/step - loss: 6.2858 - val_loss: 8.8073 - lr: 2.0000e-04\n",
      "Epoch 113/1000\n",
      "551/551 [==============================] - ETA: 0s - loss: 6.5148\n",
      "Epoch 113: val_loss did not improve from 8.69255\n",
      "551/551 [==============================] - 74s 135ms/step - loss: 6.5148 - val_loss: 8.7739 - lr: 2.0000e-04\n",
      "Epoch 114/1000\n",
      "551/551 [==============================] - ETA: 0s - loss: 6.2910\n",
      "Epoch 114: val_loss did not improve from 8.69255\n",
      "551/551 [==============================] - 75s 136ms/step - loss: 6.2910 - val_loss: 8.7644 - lr: 2.0000e-04\n",
      "Epoch 115/1000\n",
      "551/551 [==============================] - ETA: 0s - loss: 6.3126\n",
      "Epoch 115: val_loss did not improve from 8.69255\n",
      "551/551 [==============================] - 75s 136ms/step - loss: 6.3126 - val_loss: 8.8118 - lr: 2.0000e-04\n",
      "Epoch 116/1000\n",
      "551/551 [==============================] - ETA: 0s - loss: 6.4139\n",
      "Epoch 116: val_loss did not improve from 8.69255\n",
      "551/551 [==============================] - 75s 136ms/step - loss: 6.4139 - val_loss: 8.8116 - lr: 2.0000e-04\n",
      "Epoch 117/1000\n",
      "551/551 [==============================] - ETA: 0s - loss: 6.3321\n",
      "Epoch 117: val_loss did not improve from 8.69255\n",
      "551/551 [==============================] - 75s 136ms/step - loss: 6.3321 - val_loss: 8.8985 - lr: 2.0000e-04\n",
      "Epoch 118/1000\n",
      "551/551 [==============================] - ETA: 0s - loss: 6.2071\n",
      "Epoch 118: val_loss did not improve from 8.69255\n",
      "551/551 [==============================] - 75s 136ms/step - loss: 6.2071 - val_loss: 8.9116 - lr: 2.0000e-04\n",
      "Epoch 119/1000\n",
      "551/551 [==============================] - ETA: 0s - loss: 6.2382\n",
      "Epoch 119: val_loss did not improve from 8.69255\n",
      "551/551 [==============================] - 75s 136ms/step - loss: 6.2382 - val_loss: 8.7524 - lr: 2.0000e-04\n",
      "Epoch 120/1000\n",
      "551/551 [==============================] - ETA: 0s - loss: 6.1821\n",
      "Epoch 120: val_loss did not improve from 8.69255\n",
      "551/551 [==============================] - 75s 136ms/step - loss: 6.1821 - val_loss: 8.8906 - lr: 2.0000e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 121/1000\n",
      "551/551 [==============================] - ETA: 0s - loss: 6.1965\n",
      "Epoch 121: val_loss did not improve from 8.69255\n",
      "551/551 [==============================] - 76s 137ms/step - loss: 6.1965 - val_loss: 8.8007 - lr: 2.0000e-04\n",
      "Epoch 122/1000\n",
      "551/551 [==============================] - ETA: 0s - loss: 6.0011\n",
      "Epoch 122: val_loss did not improve from 8.69255\n",
      "551/551 [==============================] - 75s 137ms/step - loss: 6.0011 - val_loss: 8.8818 - lr: 2.0000e-04\n",
      "Epoch 123/1000\n",
      "551/551 [==============================] - ETA: 0s - loss: 6.0964\n",
      "Epoch 123: val_loss did not improve from 8.69255\n",
      "551/551 [==============================] - 75s 136ms/step - loss: 6.0964 - val_loss: 8.7593 - lr: 2.0000e-04\n",
      "Epoch 124/1000\n",
      "551/551 [==============================] - ETA: 0s - loss: 6.1346\n",
      "Epoch 124: val_loss did not improve from 8.69255\n",
      "\n",
      "Epoch 124: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
      "551/551 [==============================] - 75s 136ms/step - loss: 6.1346 - val_loss: 8.9565 - lr: 2.0000e-04\n",
      "Epoch 125/1000\n",
      "551/551 [==============================] - ETA: 0s - loss: 5.8594\n",
      "Epoch 125: val_loss did not improve from 8.69255\n",
      "551/551 [==============================] - 75s 137ms/step - loss: 5.8594 - val_loss: 8.7193 - lr: 4.0000e-05\n",
      "Epoch 126/1000\n",
      "551/551 [==============================] - ETA: 0s - loss: 5.7936\n",
      "Epoch 126: val_loss did not improve from 8.69255\n",
      "551/551 [==============================] - 75s 136ms/step - loss: 5.7936 - val_loss: 8.7328 - lr: 4.0000e-05\n",
      "Epoch 127/1000\n",
      "551/551 [==============================] - ETA: 0s - loss: 5.8147\n",
      "Epoch 127: val_loss did not improve from 8.69255\n",
      "551/551 [==============================] - 75s 137ms/step - loss: 5.8147 - val_loss: 8.7672 - lr: 4.0000e-05\n",
      "Epoch 128/1000\n",
      "551/551 [==============================] - ETA: 0s - loss: 5.8688\n",
      "Epoch 128: val_loss did not improve from 8.69255\n",
      "551/551 [==============================] - 75s 136ms/step - loss: 5.8688 - val_loss: 8.7467 - lr: 4.0000e-05\n",
      "Epoch 129/1000\n",
      "551/551 [==============================] - ETA: 0s - loss: 5.8897\n",
      "Epoch 129: val_loss did not improve from 8.69255\n",
      "Restoring model weights from the end of the best epoch: 109.\n",
      "551/551 [==============================] - 75s 136ms/step - loss: 5.8897 - val_loss: 8.7022 - lr: 4.0000e-05\n",
      "Epoch 129: early stopping\n",
      "Total train images:      8807\n",
      "Total validation images: 1372\n",
      "Batch:                   16\n",
      "\n",
      "Total time:              2:41:24.032902\n",
      "Time per epoch:          0:01:15.070022\n",
      "Time per item:           0:00:00.007375\n",
      "\n",
      "Total epochs:            129\n",
      "Best epoch               109\n",
      "\n",
      "Training loss:           6.38055277\n",
      "Validation loss:         8.69254589\n",
      "Model Predict\n",
      "52/52 [==============================] - 3s 42ms/step\n",
      "CTC Decode\n",
      " 1/52 [..............................] - ETA: 43s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/project/anaconda3/envs/htr2/lib/python3.8/site-packages/numpy/core/fromnumeric.py:43: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  result = getattr(asarray(obj), method)(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52/52 [==============================] - 45s 873ms/step\n",
      "[array([0.5383595], dtype=float32), array([0.12706001], dtype=float32), array([0.00077464], dtype=float32), array([0.00018513], dtype=float32), array([0.16208607], dtype=float32), array([0.00079867], dtype=float32), array([1.428421e-05], dtype=float32), array([0.02161818], dtype=float32), array([0.00038888], dtype=float32), array([0.00080576], dtype=float32), array([0.00125482], dtype=float32), array([0.02474123], dtype=float32), array([0.00156324], dtype=float32), array([9.4905176e-05], dtype=float32), array([0.21384043], dtype=float32), array([0.01089581], dtype=float32), array([0.12640545], dtype=float32), array([0.18479604], dtype=float32), array([0.10472444], dtype=float32), array([0.0492663], dtype=float32), array([0.02720372], dtype=float32), array([0.03541225], dtype=float32), array([0.33286285], dtype=float32), array([0.01826109], dtype=float32), array([0.07076499], dtype=float32), array([0.03937029], dtype=float32), array([0.00863074], dtype=float32), array([0.06250587], dtype=float32), array([0.18414104], dtype=float32), array([0.01320301], dtype=float32), array([0.04087829], dtype=float32), array([0.00389378], dtype=float32), array([0.07158447], dtype=float32), array([0.10854474], dtype=float32), array([0.02679473], dtype=float32), array([0.03722864], dtype=float32), array([0.01101052], dtype=float32), array([0.0020169], dtype=float32), array([7.963735e-05], dtype=float32), array([1.7028757e-05], dtype=float32), array([1.0236287e-05], dtype=float32), array([0.00687099], dtype=float32), array([0.08411574], dtype=float32), array([0.14283709], dtype=float32), array([0.4341215], dtype=float32), array([0.22990572], dtype=float32), array([0.11550643], dtype=float32), array([0.15581268], dtype=float32), array([0.3611735], dtype=float32), array([0.32657254], dtype=float32), array([0.04106167], dtype=float32), array([0.5976396], dtype=float32), array([0.00036813], dtype=float32), array([0.39757434], dtype=float32), array([0.00074959], dtype=float32), array([0.00878358], dtype=float32), array([0.28488094], dtype=float32), array([0.30204538], dtype=float32), array([0.11960066], dtype=float32), array([0.97389126], dtype=float32), array([0.11638205], dtype=float32), array([0.7911859], dtype=float32), array([0.9099287], dtype=float32), array([0.65287334], dtype=float32), array([0.848074], dtype=float32), array([0.23125008], dtype=float32), array([0.09902783], dtype=float32), array([0.01159636], dtype=float32), array([0.6982039], dtype=float32), array([0.96291035], dtype=float32), array([0.02203743], dtype=float32), array([0.17891705], dtype=float32), array([0.8880499], dtype=float32), array([0.3272042], dtype=float32), array([0.81597567], dtype=float32), array([0.50601524], dtype=float32), array([0.80326104], dtype=float32), array([0.41660014], dtype=float32), array([0.68952394], dtype=float32), array([0.8923823], dtype=float32), array([0.69309825], dtype=float32), array([0.8704938], dtype=float32), array([0.48729697], dtype=float32), array([0.25937423], dtype=float32), array([0.9076706], dtype=float32), array([0.89908975], dtype=float32), array([0.8972917], dtype=float32), array([0.9189035], dtype=float32), array([0.94677407], dtype=float32), array([0.87547535], dtype=float32), array([0.35805023], dtype=float32), array([0.8866227], dtype=float32), array([0.47030345], dtype=float32), array([0.08657479], dtype=float32), array([0.14524505], dtype=float32), array([0.7816805], dtype=float32), array([0.47883525], dtype=float32), array([0.17518117], dtype=float32), array([0.6078325], dtype=float32), array([0.76148], dtype=float32), array([0.91040266], dtype=float32), array([0.6060033], dtype=float32), array([0.47259098], dtype=float32), array([0.62934893], dtype=float32), array([0.33286804], dtype=float32), array([0.98582405], dtype=float32), array([0.8247914], dtype=float32), array([0.7303429], dtype=float32), array([0.18875623], dtype=float32), array([0.8385569], dtype=float32), array([0.23884442], dtype=float32), array([0.07098191], dtype=float32), array([0.55898327], dtype=float32), array([0.53424084], dtype=float32), array([0.01567157], dtype=float32), array([0.7138395], dtype=float32), array([0.54523736], dtype=float32), array([0.16618472], dtype=float32), array([0.82224774], dtype=float32), array([0.8631355], dtype=float32), array([0.51556873], dtype=float32), array([0.9739699], dtype=float32), array([0.11825708], dtype=float32), array([0.15136397], dtype=float32), array([0.8943286], dtype=float32), array([0.8808191], dtype=float32), array([0.84358287], dtype=float32), array([0.27239504], dtype=float32), array([0.9116129], dtype=float32), array([0.7307664], dtype=float32), array([0.4467883], dtype=float32), array([0.90066916], dtype=float32), array([0.18223873], dtype=float32), array([0.7662263], dtype=float32), array([0.7833667], dtype=float32), array([0.7993738], dtype=float32), array([0.7497274], dtype=float32), array([0.54393536], dtype=float32), array([0.02034672], dtype=float32), array([2.1000375e-05], dtype=float32), array([0.17603311], dtype=float32), array([0.35293463], dtype=float32), array([0.2969732], dtype=float32), array([0.48742613], dtype=float32), array([0.28486553], dtype=float32), array([0.02982594], dtype=float32), array([0.00314878], dtype=float32), array([0.7202374], dtype=float32), array([0.43505144], dtype=float32), array([0.947908], dtype=float32), array([0.637782], dtype=float32), array([0.9473827], dtype=float32), array([0.6392027], dtype=float32), array([0.34625155], dtype=float32), array([0.37632772], dtype=float32), array([0.7319703], dtype=float32), array([0.683773], dtype=float32), array([0.8363389], dtype=float32), array([0.32017538], dtype=float32), array([0.00989074], dtype=float32), array([0.12442911], dtype=float32), array([0.59458107], dtype=float32), array([0.09555715], dtype=float32), array([0.03311345], dtype=float32), array([0.27365807], dtype=float32), array([0.11384368], dtype=float32), array([0.32054517], dtype=float32), array([0.30810007], dtype=float32), array([0.02617695], dtype=float32), array([0.22220956], dtype=float32), array([0.30419174], dtype=float32), array([0.16551024], dtype=float32), array([0.35260463], dtype=float32), array([0.02849337], dtype=float32), array([0.2180038], dtype=float32), array([0.01937395], dtype=float32), array([0.07904252], dtype=float32), array([0.12580715], dtype=float32), array([0.00581096], dtype=float32), array([0.10692115], dtype=float32), array([0.39132437], dtype=float32), array([0.15732236], dtype=float32), array([0.06983254], dtype=float32), array([0.18858455], dtype=float32), array([0.4476557], dtype=float32), array([0.17385884], dtype=float32), array([0.14254002], dtype=float32), array([0.529363], dtype=float32), array([0.17948753], dtype=float32), array([0.05409136], dtype=float32), array([0.47258982], dtype=float32), array([0.06392864], dtype=float32), array([0.08240607], dtype=float32), array([0.04656799], dtype=float32), array([0.17534892], dtype=float32), array([0.21396968], dtype=float32), array([0.06466182], dtype=float32), array([0.4019841], dtype=float32), array([0.05385269], dtype=float32), array([0.04575381], dtype=float32), array([0.00350901], dtype=float32), array([0.02207426], dtype=float32), array([0.00181251], dtype=float32), array([1.0975274e-05], dtype=float32), array([0.02643405], dtype=float32), array([0.12120939], dtype=float32), array([0.00067658], dtype=float32), array([8.357252e-06], dtype=float32), array([6.061958e-05], dtype=float32), array([0.00010173], dtype=float32), array([3.4800916e-08], dtype=float32), array([0.00073014], dtype=float32), array([0.0009725], dtype=float32), array([0.00326265], dtype=float32), array([0.0010668], dtype=float32), array([0.00011422], dtype=float32), array([4.2726533e-06], dtype=float32), array([0.00275481], dtype=float32), array([0.0013801], dtype=float32), array([2.678952e-08], dtype=float32), array([0.00048519], dtype=float32), array([3.505459e-05], dtype=float32), array([0.03769339], dtype=float32), array([0.322714], dtype=float32), array([0.37655354], dtype=float32), array([0.2855128], dtype=float32), array([0.6245208], dtype=float32), array([0.5782067], dtype=float32), array([0.0315648], dtype=float32), array([0.09855523], dtype=float32), array([0.8519291], dtype=float32), array([0.48844543], dtype=float32), array([0.68913496], dtype=float32), array([0.42457098], dtype=float32), array([0.49503297], dtype=float32), array([0.78748196], dtype=float32), array([0.26556164], dtype=float32), array([0.35200146], dtype=float32), array([0.40568176], dtype=float32), array([0.00210446], dtype=float32), array([0.03111893], dtype=float32), array([0.16239038], dtype=float32), array([0.44623253], dtype=float32), array([0.58096254], dtype=float32), array([0.40222013], dtype=float32), array([0.05235481], dtype=float32), array([0.09347052], dtype=float32), array([0.01784047], dtype=float32), array([0.17774074], dtype=float32), array([0.13962486], dtype=float32), array([0.00702532], dtype=float32), array([0.03732434], dtype=float32), array([0.15227522], dtype=float32), array([0.5990537], dtype=float32), array([0.1230223], dtype=float32), array([0.0055927], dtype=float32), array([0.6823957], dtype=float32), array([0.25764483], dtype=float32), array([0.072385], dtype=float32), array([0.13201511], dtype=float32), array([0.3372215], dtype=float32), array([0.30313626], dtype=float32), array([0.03875725], dtype=float32), array([0.19242246], dtype=float32), array([0.12135004], dtype=float32), array([0.00875204], dtype=float32), array([0.02846685], dtype=float32), array([0.02338876], dtype=float32), array([0.4327769], dtype=float32), array([0.12308526], dtype=float32), array([0.07849567], dtype=float32), array([0.30683246], dtype=float32), array([0.1545085], dtype=float32), array([0.40745556], dtype=float32), array([0.32304463], dtype=float32), array([0.71469843], dtype=float32), array([0.94481844], dtype=float32), array([0.7082761], dtype=float32), array([0.4904646], dtype=float32), array([0.06826189], dtype=float32), array([0.15837245], dtype=float32), array([0.4325954], dtype=float32), array([0.34944993], dtype=float32), array([0.0258253], dtype=float32), array([0.94039685], dtype=float32), array([0.56133145], dtype=float32), array([0.00493231], dtype=float32), array([0.04906038], dtype=float32), array([0.47589174], dtype=float32), array([0.80716914], dtype=float32), array([0.6409218], dtype=float32), array([0.36794916], dtype=float32), array([0.13037959], dtype=float32), array([0.3779757], dtype=float32), array([0.55926263], dtype=float32), array([0.21119644], dtype=float32), array([0.02828], dtype=float32), array([0.14385441], dtype=float32), array([0.75121033], dtype=float32), array([0.65945184], dtype=float32), array([0.27956486], dtype=float32), array([0.40575418], dtype=float32), array([0.56203145], dtype=float32), array([0.10812173], dtype=float32), array([0.38193467], dtype=float32), array([0.0051351], dtype=float32), array([0.00452379], dtype=float32), array([0.58022845], dtype=float32), array([0.89829373], dtype=float32), array([0.4265482], dtype=float32), array([0.62823254], dtype=float32), array([0.60374546], dtype=float32), array([0.70834816], dtype=float32), array([0.95146775], dtype=float32), array([0.5109634], dtype=float32), array([0.3664123], dtype=float32), array([0.00012924], dtype=float32), array([0.11314482], dtype=float32), array([0.34871075], dtype=float32), array([0.7336147], dtype=float32), array([0.94633144], dtype=float32), array([0.940984], dtype=float32), array([0.7424272], dtype=float32), array([0.8781734], dtype=float32), array([0.10543787], dtype=float32), array([0.9582299], dtype=float32), array([0.67257696], dtype=float32), array([0.57956475], dtype=float32), array([0.9390428], dtype=float32), array([0.55025953], dtype=float32), array([0.22085987], dtype=float32), array([0.43446192], dtype=float32), array([0.94147277], dtype=float32), array([0.871001], dtype=float32), array([0.9070885], dtype=float32), array([0.78923595], dtype=float32), array([0.8560416], dtype=float32), array([0.92244625], dtype=float32), array([0.09570961], dtype=float32), array([0.8763697], dtype=float32), array([0.8333908], dtype=float32), array([0.75005543], dtype=float32), array([0.7599204], dtype=float32), array([0.74132925], dtype=float32), array([0.45284456], dtype=float32), array([0.9235248], dtype=float32), array([0.84998643], dtype=float32), array([0.6054966], dtype=float32), array([0.45395324], dtype=float32), array([0.13121872], dtype=float32), array([0.75169456], dtype=float32), array([0.48600957], dtype=float32), array([0.37669778], dtype=float32), array([0.7914483], dtype=float32), array([0.7043858], dtype=float32), array([0.63425803], dtype=float32), array([0.2737955], dtype=float32), array([0.830585], dtype=float32), array([0.9131986], dtype=float32), array([0.41664726], dtype=float32), array([0.2291301], dtype=float32), array([0.53880984], dtype=float32), array([0.42333165], dtype=float32), array([0.44136116], dtype=float32), array([0.82798463], dtype=float32), array([0.94432575], dtype=float32), array([0.94908273], dtype=float32), array([0.5848733], dtype=float32), array([0.8740818], dtype=float32), array([0.8180463], dtype=float32), array([0.7182127], dtype=float32), array([0.8526604], dtype=float32), array([0.43574575], dtype=float32), array([0.7291574], dtype=float32), array([0.94731367], dtype=float32), array([0.7585582], dtype=float32), array([0.9571363], dtype=float32), array([0.85762525], dtype=float32), array([0.46438438], dtype=float32), array([0.03917317], dtype=float32), array([0.5054752], dtype=float32), array([0.52601963], dtype=float32), array([0.67300445], dtype=float32), array([0.37055284], dtype=float32), array([0.5230253], dtype=float32), array([0.66348946], dtype=float32), array([0.35835034], dtype=float32), array([0.3809599], dtype=float32), array([0.48004276], dtype=float32), array([0.83311677], dtype=float32), array([0.91105384], dtype=float32), array([0.6559284], dtype=float32), array([0.8561266], dtype=float32), array([0.8749242], dtype=float32), array([0.94268334], dtype=float32), array([0.5773886], dtype=float32), array([0.78422326], dtype=float32), array([0.9007906], dtype=float32), array([0.9077675], dtype=float32), array([0.7894844], dtype=float32), array([0.44313723], dtype=float32), array([0.31181145], dtype=float32), array([0.8548157], dtype=float32), array([0.73718417], dtype=float32), array([0.87255186], dtype=float32), array([0.8186973], dtype=float32), array([0.12649257], dtype=float32), array([0.9071162], dtype=float32), array([0.89341486], dtype=float32), array([0.8269681], dtype=float32), array([0.45005488], dtype=float32), array([0.7568043], dtype=float32), array([0.85734963], dtype=float32), array([0.46372998], dtype=float32), array([0.9060733], dtype=float32), array([0.85100085], dtype=float32), array([0.69131464], dtype=float32), array([0.73181015], dtype=float32), array([0.64969885], dtype=float32), array([0.92845434], dtype=float32), array([0.8439158], dtype=float32), array([0.8948595], dtype=float32), array([0.5550288], dtype=float32), array([0.8989729], dtype=float32), array([0.736793], dtype=float32), array([0.5686201], dtype=float32), array([0.71793795], dtype=float32), array([0.24666996], dtype=float32), array([0.94927293], dtype=float32), array([0.01432699], dtype=float32), array([0.14746079], dtype=float32), array([0.22686063], dtype=float32), array([0.65002203], dtype=float32), array([0.31978136], dtype=float32), array([0.17977887], dtype=float32), array([0.69648916], dtype=float32), array([0.32867837], dtype=float32), array([0.49959123], dtype=float32), array([0.9250443], dtype=float32), array([0.88425493], dtype=float32), array([0.32052225], dtype=float32), array([0.76267105], dtype=float32), array([0.9027917], dtype=float32), array([0.48971498], dtype=float32), array([0.9598074], dtype=float32), array([0.85017407], dtype=float32), array([0.34776452], dtype=float32), array([0.9683842], dtype=float32), array([0.55514175], dtype=float32), array([0.06563358], dtype=float32), array([0.18046442], dtype=float32), array([0.7219502], dtype=float32), array([0.89252913], dtype=float32), array([0.17645016], dtype=float32), array([0.871569], dtype=float32), array([0.52148163], dtype=float32), array([0.880392], dtype=float32), array([0.40214372], dtype=float32), array([0.68892944], dtype=float32), array([0.70606], dtype=float32), array([0.6418726], dtype=float32), array([0.3008467], dtype=float32), array([0.12403914], dtype=float32), array([0.47985086], dtype=float32), array([0.13994345], dtype=float32), array([0.31069925], dtype=float32), array([0.02265628], dtype=float32), array([0.32060364], dtype=float32), array([0.29951394], dtype=float32), array([0.40969443], dtype=float32), array([0.11336726], dtype=float32), array([0.19965449], dtype=float32), array([0.7744224], dtype=float32), array([0.6642197], dtype=float32), array([0.8604556], dtype=float32), array([0.40846035], dtype=float32), array([0.3057772], dtype=float32), array([0.23501742], dtype=float32), array([0.01337395], dtype=float32), array([0.27728048], dtype=float32), array([0.6565845], dtype=float32), array([0.17539637], dtype=float32), array([0.79647046], dtype=float32), array([0.4874338], dtype=float32), array([0.36043176], dtype=float32), array([0.12918252], dtype=float32), array([0.00031593], dtype=float32), array([1.1944787e-05], dtype=float32), array([0.00079629], dtype=float32), array([4.7575406e-05], dtype=float32), array([0.02338182], dtype=float32), array([0.08511861], dtype=float32), array([4.263902e-06], dtype=float32), array([0.00020776], dtype=float32), array([0.00194164], dtype=float32), array([0.00102312], dtype=float32), array([0.00248469], dtype=float32), array([8.960646e-06], dtype=float32), array([0.00116322], dtype=float32), array([0.00024426], dtype=float32), array([0.00059672], dtype=float32), array([1.3000564e-05], dtype=float32), array([4.3208006e-06], dtype=float32), array([0.00030296], dtype=float32), array([2.8708131e-05], dtype=float32), array([0.00372312], dtype=float32), array([0.00031839], dtype=float32), array([1.2971199e-09], dtype=float32), array([0.00765339], dtype=float32), array([2.7352713e-05], dtype=float32), array([0.02019807], dtype=float32), array([0.5487984], dtype=float32), array([0.11579127], dtype=float32), array([0.10649132], dtype=float32), array([0.82951444], dtype=float32), array([0.12905791], dtype=float32), array([0.2711084], dtype=float32), array([0.25352296], dtype=float32), array([0.04561071], dtype=float32), array([0.10961426], dtype=float32), array([0.05207824], dtype=float32), array([0.01871003], dtype=float32), array([0.06839573], dtype=float32), array([0.4033101], dtype=float32), array([0.17212997], dtype=float32), array([0.21542592], dtype=float32), array([0.01757616], dtype=float32), array([0.01640451], dtype=float32), array([0.0014678], dtype=float32), array([0.0134179], dtype=float32), array([0.26658615], dtype=float32), array([0.06477913], dtype=float32), array([0.06260118], dtype=float32), array([0.20902069], dtype=float32), array([0.09079745], dtype=float32), array([0.01677811], dtype=float32), array([0.8029556], dtype=float32), array([0.6647488], dtype=float32), array([0.3760914], dtype=float32), array([0.8959065], dtype=float32), array([0.9300341], dtype=float32), array([0.12378256], dtype=float32), array([0.27820823], dtype=float32), array([0.58641726], dtype=float32), array([0.5131677], dtype=float32), array([0.9464059], dtype=float32), array([0.39533105], dtype=float32), array([0.7106236], dtype=float32), array([0.25166732], dtype=float32), array([0.8670622], dtype=float32), array([0.5588247], dtype=float32), array([0.4658667], dtype=float32), array([0.27322373], dtype=float32), array([0.9285343], dtype=float32), array([0.5874933], dtype=float32), array([0.7843644], dtype=float32), array([0.6727293], dtype=float32), array([0.41120538], dtype=float32), array([0.25604105], dtype=float32), array([0.81678116], dtype=float32), array([0.57544607], dtype=float32), array([0.78156245], dtype=float32), array([0.208445], dtype=float32), array([0.00067246], dtype=float32), array([2.1056842e-11], dtype=float32), array([0.74320173], dtype=float32), array([0.6759711], dtype=float32), array([0.4527955], dtype=float32), array([0.9416815], dtype=float32), array([0.7838032], dtype=float32), array([0.906012], dtype=float32), array([0.09316883], dtype=float32), array([0.81644166], dtype=float32), array([0.41258347], dtype=float32), array([0.7278056], dtype=float32), array([0.48383918], dtype=float32), array([0.8456269], dtype=float32), array([0.10788804], dtype=float32), array([0.08768599], dtype=float32), array([0.4522994], dtype=float32), array([0.02384803], dtype=float32), array([0.39020756], dtype=float32), array([0.02075551], dtype=float32), array([0.3099389], dtype=float32), array([0.5113992], dtype=float32), array([0.08286068], dtype=float32), array([0.849705], dtype=float32), array([0.38551837], dtype=float32), array([0.31901062], dtype=float32), array([0.38706052], dtype=float32), array([0.06491226], dtype=float32), array([0.30661744], dtype=float32), array([0.28718883], dtype=float32), array([0.7424244], dtype=float32), array([0.473897], dtype=float32), array([0.28602785], dtype=float32), array([0.26262516], dtype=float32), array([0.13527209], dtype=float32), array([0.5756786], dtype=float32), array([0.3918196], dtype=float32), array([0.34208572], dtype=float32), array([0.8497887], dtype=float32), array([0.29820013], dtype=float32), array([0.71861863], dtype=float32), array([0.5410132], dtype=float32), array([0.28048182], dtype=float32), array([0.31845534], dtype=float32), array([0.46645123], dtype=float32), array([0.65762794], dtype=float32), array([0.73565704], dtype=float32), array([3.9952927e-05], dtype=float32), array([0.07452027], dtype=float32), array([0.00323054], dtype=float32), array([0.00980974], dtype=float32), array([8.162524e-05], dtype=float32), array([0.00400536], dtype=float32), array([0.00169864], dtype=float32), array([0.00039798], dtype=float32), array([0.0009394], dtype=float32), array([0.00147562], dtype=float32), array([0.00044629], dtype=float32), array([1.8844172e-05], dtype=float32), array([0.00806567], dtype=float32), array([0.00670585], dtype=float32), array([0.00515532], dtype=float32), array([0.02356005], dtype=float32), array([0.00024063], dtype=float32), array([0.0225809], dtype=float32), array([6.223003e-06], dtype=float32), array([0.03022909], dtype=float32), array([0.0001213], dtype=float32), array([0.17001492], dtype=float32), array([0.13449462], dtype=float32), array([0.01512887], dtype=float32), array([0.0180493], dtype=float32), array([0.3051461], dtype=float32), array([0.2362605], dtype=float32), array([0.1697547], dtype=float32), array([0.07595431], dtype=float32), array([0.9019947], dtype=float32), array([0.86552185], dtype=float32), array([0.03019351], dtype=float32), array([0.6103003], dtype=float32), array([0.20463066], dtype=float32), array([0.5729966], dtype=float32), array([0.72817934], dtype=float32), array([0.43600664], dtype=float32), array([0.83299094], dtype=float32), array([0.54644036], dtype=float32), array([0.48273134], dtype=float32), array([0.8867653], dtype=float32), array([0.18281713], dtype=float32), array([0.5141471], dtype=float32), array([0.6029624], dtype=float32), array([0.5723667], dtype=float32), array([0.8298259], dtype=float32), array([0.5897325], dtype=float32), array([0.61472607], dtype=float32), array([0.51224875], dtype=float32), array([0.9052788], dtype=float32), array([0.476072], dtype=float32), array([0.00200461], dtype=float32), array([0.3328939], dtype=float32), array([0.50148183], dtype=float32), array([0.3789466], dtype=float32), array([0.65542066], dtype=float32), array([0.35296822], dtype=float32), array([0.00091262], dtype=float32), array([4.5856785e-05], dtype=float32), array([0.07312698], dtype=float32), array([0.76418537], dtype=float32), array([0.38390446], dtype=float32), array([0.48543182], dtype=float32), array([0.63168174], dtype=float32), array([7.305405e-05], dtype=float32), array([0.10392245], dtype=float32), array([0.25122872], dtype=float32), array([0.00324459], dtype=float32), array([0.01292509], dtype=float32), array([0.45932958], dtype=float32), array([0.8406933], dtype=float32), array([0.03793846], dtype=float32), array([0.36727676], dtype=float32), array([0.21121556], dtype=float32), array([0.38952214], dtype=float32), array([0.11403742], dtype=float32), array([0.6401619], dtype=float32), array([0.60735184], dtype=float32), array([0.5903967], dtype=float32), array([0.9600417], dtype=float32), array([0.4195088], dtype=float32), array([0.77052015], dtype=float32), array([0.897831], dtype=float32), array([0.4668357], dtype=float32), array([0.37200126], dtype=float32), array([0.23192145], dtype=float32), array([0.16030408], dtype=float32), array([0.729851], dtype=float32), array([0.86688775], dtype=float32), array([0.6247651], dtype=float32), array([0.349338], dtype=float32), array([0.8835914], dtype=float32), array([0.42068553], dtype=float32), array([0.8858399], dtype=float32), array([0.58278835], dtype=float32), array([0.2101938], dtype=float32), array([0.35610244], dtype=float32), array([0.90181935], dtype=float32), array([0.4187301], dtype=float32), array([0.9399426], dtype=float32), array([0.5971265], dtype=float32), array([0.15025575], dtype=float32), array([0.01616983], dtype=float32), array([0.8475497], dtype=float32), array([0.33653754], dtype=float32), array([0.32184663], dtype=float32), array([0.16447268], dtype=float32), array([0.18560503], dtype=float32), array([0.14208739], dtype=float32), array([0.40471998], dtype=float32), array([0.03414683], dtype=float32), array([0.5557912], dtype=float32), array([0.03703908], dtype=float32), array([0.20507018], dtype=float32), array([0.7316289], dtype=float32), array([0.4201562], dtype=float32), array([0.29337552], dtype=float32), array([0.05528032], dtype=float32), array([0.17284061], dtype=float32), array([0.01187248], dtype=float32), array([0.6927028], dtype=float32), array([0.17314701], dtype=float32), array([0.23898676], dtype=float32), array([0.06261656], dtype=float32), array([0.38983268], dtype=float32), array([0.36375695], dtype=float32), array([0.30028787], dtype=float32), array([0.3750475], dtype=float32), array([0.1434855], dtype=float32), array([0.01298623], dtype=float32), array([0.9109018], dtype=float32), array([0.39340162], dtype=float32), array([0.09209837], dtype=float32), array([0.01010905], dtype=float32), array([0.32998598], dtype=float32), array([0.26070544], dtype=float32), array([0.22481027], dtype=float32), array([0.00133166], dtype=float32), array([0.15129526], dtype=float32), array([0.27457207], dtype=float32), array([0.8435608], dtype=float32), array([0.651804], dtype=float32), array([0.9083434], dtype=float32), array([0.18143581], dtype=float32), array([0.00729204], dtype=float32), array([0.01998334], dtype=float32), array([0.03015789], dtype=float32), array([0.07249241], dtype=float32), array([0.0219397], dtype=float32), array([0.18400656], dtype=float32), array([0.14305003], dtype=float32), array([0.07333998], dtype=float32), array([0.11787], dtype=float32), array([0.72451997], dtype=float32), array([0.8213992], dtype=float32), array([0.35717145], dtype=float32), array([0.24298654], dtype=float32), array([0.8203526], dtype=float32), array([0.27841365], dtype=float32), array([0.69523686], dtype=float32), array([0.23325643], dtype=float32), array([0.24433264], dtype=float32), array([0.2127157], dtype=float32), array([0.4831535], dtype=float32), array([0.31184903], dtype=float32), array([0.00198725], dtype=float32), array([0.0261764], dtype=float32), array([0.7565072], dtype=float32), array([0.06220726], dtype=float32), array([0.12747194], dtype=float32), array([0.35657576], dtype=float32), array([0.07289063], dtype=float32), array([0.55360234], dtype=float32), array([0.295911], dtype=float32), array([0.7911785], dtype=float32), array([0.64447767], dtype=float32), array([0.6463372], dtype=float32), array([0.5507271], dtype=float32), array([0.3124349], dtype=float32), array([0.01985117], dtype=float32), array([0.27006605], dtype=float32), array([0.40568691], dtype=float32), array([0.37980062], dtype=float32), array([0.24210908], dtype=float32), array([0.5590149], dtype=float32), array([0.11221374], dtype=float32), array([0.11261319], dtype=float32), array([0.29411447], dtype=float32), array([0.5147745], dtype=float32), array([0.02787734], dtype=float32), array([0.07111707], dtype=float32), array([0.0964568], dtype=float32), array([0.36211878], dtype=float32), array([0.24005626], dtype=float32), array([0.01173035], dtype=float32), array([0.27673307], dtype=float32), array([0.13798407], dtype=float32), array([0.31352258], dtype=float32), array([0.2798497], dtype=float32), array([0.12969486], dtype=float32), array([0.07222059], dtype=float32), array([0.7134309], dtype=float32), array([0.02030753], dtype=float32), array([0.25212184], dtype=float32), array([0.18170539], dtype=float32), array([0.4063721], dtype=float32), array([0.21036765], dtype=float32), array([0.0607175], dtype=float32), array([0.09571605], dtype=float32), array([0.8116638], dtype=float32)]\n",
      "================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================ \n",
      "\n",
      "brain.\n",
      "brain . \n",
      "\n",
      "================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================ \n",
      "\n",
      "From this review then it appears, that\n",
      "Fromthis review then it oppears , that \n",
      "\n",
      "================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================ \n",
      "\n",
      "in paving the way to the consideration of\n",
      "im paving thewag to the conpde sabjin 9 \n",
      "\n",
      "================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================ \n",
      "\n",
      "municipal Law, our Author has spent\n",
      "nuniupalLaw , our Aathor bas paut \n",
      "\n",
      "================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================ \n",
      "\n",
      "supposed\n",
      "sapponcd \n",
      "\n",
      "================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================ \n",
      "\n",
      "much time in explaining the nature &\n",
      "Onuek time in ex plaining the pature & \n",
      "\n",
      "================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================ \n",
      "\n",
      "Properties of Laws which do not exist &\n",
      "B0 rerties of _Lavs whiah do not exist : t \n",
      "\n",
      "================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================ \n",
      "\n",
      "in misapplying those which do: & therefore,\n",
      "in misappling those whih do : & herefore , \n",
      "\n",
      "================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================ \n",
      "\n",
      "that the best thing which a Student of the\n",
      "pat thebest thing whivch _ Shudenl of tae \n",
      "\n",
      "================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================ \n",
      "\n",
      "Law can do is to forget all that our Author\n",
      "Law car do is to forget all pal orlalbor \n",
      "\n",
      "\n",
      "eval= [0.08356465 0.345202   0.82804878]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: kaldiio in /home/project/anaconda3/envs/htr2/lib/python3.8/site-packages (2.17.2)\n",
      "Requirement already satisfied: numpy in /home/project/anaconda3/envs/htr2/lib/python3.8/site-packages (from kaldiio) (1.22.3)\n",
      "Requirement already satisfied: stn in /home/project/anaconda3/envs/htr2/lib/python3.8/site-packages (1.0.1)\n",
      "Requirement already satisfied: numpy in /home/project/anaconda3/envs/htr2/lib/python3.8/site-packages (from stn) (1.22.3)\n",
      "Requirement already satisfied: rapidfuzz in /home/project/anaconda3/envs/htr2/lib/python3.8/site-packages (2.3.0)\n",
      "Requirement already satisfied: jarowinkler<2.0.0,>=1.2.0 in /home/project/anaconda3/envs/htr2/lib/python3.8/site-packages (from rapidfuzz) (1.2.0)\n",
      "Requirement already satisfied: seaborn in /home/project/anaconda3/envs/htr2/lib/python3.8/site-packages (0.11.2)\n",
      "Requirement already satisfied: scipy>=1.0 in /home/project/anaconda3/envs/htr2/lib/python3.8/site-packages (from seaborn) (1.8.1)\n",
      "Requirement already satisfied: numpy>=1.15 in /home/project/anaconda3/envs/htr2/lib/python3.8/site-packages (from seaborn) (1.22.3)\n",
      "Requirement already satisfied: pandas>=0.23 in /home/project/anaconda3/envs/htr2/lib/python3.8/site-packages (from seaborn) (1.4.3)\n",
      "Requirement already satisfied: matplotlib>=2.2 in /home/project/anaconda3/envs/htr2/lib/python3.8/site-packages (from seaborn) (3.5.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/project/anaconda3/envs/htr2/lib/python3.8/site-packages (from matplotlib>=2.2->seaborn) (0.11.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /home/project/anaconda3/envs/htr2/lib/python3.8/site-packages (from matplotlib>=2.2->seaborn) (9.1.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/project/anaconda3/envs/htr2/lib/python3.8/site-packages (from matplotlib>=2.2->seaborn) (4.34.4)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/project/anaconda3/envs/htr2/lib/python3.8/site-packages (from matplotlib>=2.2->seaborn) (1.4.4)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /home/project/anaconda3/envs/htr2/lib/python3.8/site-packages (from matplotlib>=2.2->seaborn) (3.0.8)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/project/anaconda3/envs/htr2/lib/python3.8/site-packages (from matplotlib>=2.2->seaborn) (21.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/project/anaconda3/envs/htr2/lib/python3.8/site-packages (from matplotlib>=2.2->seaborn) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/project/anaconda3/envs/htr2/lib/python3.8/site-packages (from pandas>=0.23->seaborn) (2022.1)\n",
      "Requirement already satisfied: six>=1.5 in /home/project/anaconda3/envs/htr2/lib/python3.8/site-packages (from python-dateutil>=2.7->matplotlib>=2.2->seaborn) (1.16.0)\n",
      "Requirement already satisfied: tensorflow_addons in /home/project/anaconda3/envs/htr2/lib/python3.8/site-packages (0.17.1)\n",
      "Requirement already satisfied: packaging in /home/project/anaconda3/envs/htr2/lib/python3.8/site-packages (from tensorflow_addons) (21.3)\n",
      "Requirement already satisfied: typeguard>=2.7 in /home/project/anaconda3/envs/htr2/lib/python3.8/site-packages (from tensorflow_addons) (2.13.3)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/project/anaconda3/envs/htr2/lib/python3.8/site-packages (from packaging->tensorflow_addons) (3.0.8)\n",
      "Found GPU at: /device:GPU:0\n",
      "source: ../data/bentham.hdf5\n",
      "output ../output/bentham/shashankbest4senetgateddropouttwo/2023-09-30 16:12:56.237395\n",
      "target ../output/bentham/shashankbest4senetgateddropouttwo/2023-09-30 16:12:56.237395/checkpoint_weights.hdf5\n",
      "charset: 0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~ \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-30 16:12:56.239993: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2006] Ignoring visible gpu device (device: 1, name: Quadro P1000, pci bus id: 0000:65:00.0, compute capability: 6.1) with core count: 5. The minimum required count is 8. You can adjust this requirement with the env var TF_MIN_GPU_MULTIPROCESSOR_COUNT.\n",
      "2023-09-30 16:12:56.240807: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /device:GPU:0 with 9631 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2080 Ti, pci bus id: 0000:17:00.0, compute capability: 7.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train images: 8807\n",
      "Validation images: 1372\n",
      "Test images: 820\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input (InputLayer)             [(None, 1024, 128,   0           []                               \n",
      "                                1)]                                                               \n",
      "                                                                                                  \n",
      " conv2d_8 (Conv2D)              (None, 1024, 128, 8  80          ['input[0][0]']                  \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " batch_normalization_8 (BatchNo  (None, 1024, 128, 8  56         ['conv2d_8[0][0]']               \n",
      " rmalization)                   )                                                                 \n",
      "                                                                                                  \n",
      " global_average_pooling2d_7 (Gl  (None, 8)           0           ['batch_normalization_8[0][0]']  \n",
      " obalAveragePooling2D)                                                                            \n",
      "                                                                                                  \n",
      " reshape_8 (Reshape)            (None, 1, 1, 8)      0           ['global_average_pooling2d_7[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " dense_16 (Dense)               (None, 1, 1, 4)      32          ['reshape_8[0][0]']              \n",
      "                                                                                                  \n",
      " dense_17 (Dense)               (None, 1, 1, 8)      32          ['dense_16[0][0]']               \n",
      "                                                                                                  \n",
      " multiply_10 (Multiply)         (None, 1024, 128, 8  0           ['batch_normalization_8[0][0]',  \n",
      "                                )                                 'dense_17[0][0]']               \n",
      "                                                                                                  \n",
      " full_gated_conv2d_7 (FullGated  (None, 1024, 128, 8  1168       ['multiply_10[0][0]']            \n",
      " Conv2D)                        )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_9 (Conv2D)              (None, 512, 64, 16)  1168        ['full_gated_conv2d_7[0][0]']    \n",
      "                                                                                                  \n",
      " batch_normalization_9 (BatchNo  (None, 512, 64, 16)  112        ['conv2d_9[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " global_average_pooling2d_8 (Gl  (None, 16)          0           ['batch_normalization_9[0][0]']  \n",
      " obalAveragePooling2D)                                                                            \n",
      "                                                                                                  \n",
      " reshape_9 (Reshape)            (None, 1, 1, 16)     0           ['global_average_pooling2d_8[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " dense_18 (Dense)               (None, 1, 1, 8)      128         ['reshape_9[0][0]']              \n",
      "                                                                                                  \n",
      " dense_19 (Dense)               (None, 1, 1, 16)     128         ['dense_18[0][0]']               \n",
      "                                                                                                  \n",
      " multiply_11 (Multiply)         (None, 512, 64, 16)  0           ['batch_normalization_9[0][0]',  \n",
      "                                                                  'dense_19[0][0]']               \n",
      "                                                                                                  \n",
      " full_gated_conv2d_8 (FullGated  (None, 512, 64, 16)  4640       ['multiply_11[0][0]']            \n",
      " Conv2D)                                                                                          \n",
      "                                                                                                  \n",
      " conv2d_10 (Conv2D)             (None, 512, 64, 24)  3480        ['full_gated_conv2d_8[0][0]']    \n",
      "                                                                                                  \n",
      " batch_normalization_10 (BatchN  (None, 512, 64, 24)  168        ['conv2d_10[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " global_average_pooling2d_9 (Gl  (None, 24)          0           ['batch_normalization_10[0][0]'] \n",
      " obalAveragePooling2D)                                                                            \n",
      "                                                                                                  \n",
      " reshape_10 (Reshape)           (None, 1, 1, 24)     0           ['global_average_pooling2d_9[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " dense_20 (Dense)               (None, 1, 1, 12)     288         ['reshape_10[0][0]']             \n",
      "                                                                                                  \n",
      " dense_21 (Dense)               (None, 1, 1, 24)     288         ['dense_20[0][0]']               \n",
      "                                                                                                  \n",
      " multiply_12 (Multiply)         (None, 512, 64, 24)  0           ['batch_normalization_10[0][0]', \n",
      "                                                                  'dense_21[0][0]']               \n",
      "                                                                                                  \n",
      " full_gated_conv2d_9 (FullGated  (None, 512, 64, 24)  10416      ['multiply_12[0][0]']            \n",
      " Conv2D)                                                                                          \n",
      "                                                                                                  \n",
      " conv2d_11 (Conv2D)             (None, 512, 64, 32)  6944        ['full_gated_conv2d_9[0][0]']    \n",
      "                                                                                                  \n",
      " batch_normalization_11 (BatchN  (None, 512, 64, 32)  224        ['conv2d_11[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " global_average_pooling2d_10 (G  (None, 32)          0           ['batch_normalization_11[0][0]'] \n",
      " lobalAveragePooling2D)                                                                           \n",
      "                                                                                                  \n",
      " reshape_11 (Reshape)           (None, 1, 1, 32)     0           ['global_average_pooling2d_10[0][\n",
      "                                                                 0]']                             \n",
      "                                                                                                  \n",
      " dense_22 (Dense)               (None, 1, 1, 16)     512         ['reshape_11[0][0]']             \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                                  \n",
      " dense_23 (Dense)               (None, 1, 1, 32)     512         ['dense_22[0][0]']               \n",
      "                                                                                                  \n",
      " multiply_13 (Multiply)         (None, 512, 64, 32)  0           ['batch_normalization_11[0][0]', \n",
      "                                                                  'dense_23[0][0]']               \n",
      "                                                                                                  \n",
      " full_gated_conv2d_10 (FullGate  (None, 512, 64, 32)  18496      ['multiply_13[0][0]']            \n",
      " dConv2D)                                                                                         \n",
      "                                                                                                  \n",
      " dropout_4 (Dropout)            (None, 512, 64, 32)  0           ['full_gated_conv2d_10[0][0]']   \n",
      "                                                                                                  \n",
      " conv2d_12 (Conv2D)             (None, 256, 16, 40)  11560       ['dropout_4[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_12 (BatchN  (None, 256, 16, 40)  280        ['conv2d_12[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " global_average_pooling2d_11 (G  (None, 40)          0           ['batch_normalization_12[0][0]'] \n",
      " lobalAveragePooling2D)                                                                           \n",
      "                                                                                                  \n",
      " reshape_12 (Reshape)           (None, 1, 1, 40)     0           ['global_average_pooling2d_11[0][\n",
      "                                                                 0]']                             \n",
      "                                                                                                  \n",
      " dense_24 (Dense)               (None, 1, 1, 20)     800         ['reshape_12[0][0]']             \n",
      "                                                                                                  \n",
      " dense_25 (Dense)               (None, 1, 1, 40)     800         ['dense_24[0][0]']               \n",
      "                                                                                                  \n",
      " multiply_14 (Multiply)         (None, 256, 16, 40)  0           ['batch_normalization_12[0][0]', \n",
      "                                                                  'dense_25[0][0]']               \n",
      "                                                                                                  \n",
      " full_gated_conv2d_11 (FullGate  (None, 256, 16, 40)  28880      ['multiply_14[0][0]']            \n",
      " dConv2D)                                                                                         \n",
      "                                                                                                  \n",
      " dropout_5 (Dropout)            (None, 256, 16, 40)  0           ['full_gated_conv2d_11[0][0]']   \n",
      "                                                                                                  \n",
      " conv2d_13 (Conv2D)             (None, 256, 16, 48)  17328       ['dropout_5[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_13 (BatchN  (None, 256, 16, 48)  336        ['conv2d_13[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " global_average_pooling2d_12 (G  (None, 48)          0           ['batch_normalization_13[0][0]'] \n",
      " lobalAveragePooling2D)                                                                           \n",
      "                                                                                                  \n",
      " reshape_13 (Reshape)           (None, 1, 1, 48)     0           ['global_average_pooling2d_12[0][\n",
      "                                                                 0]']                             \n",
      "                                                                                                  \n",
      " dense_26 (Dense)               (None, 1, 1, 24)     1152        ['reshape_13[0][0]']             \n",
      "                                                                                                  \n",
      " dense_27 (Dense)               (None, 1, 1, 48)     1152        ['dense_26[0][0]']               \n",
      "                                                                                                  \n",
      " multiply_15 (Multiply)         (None, 256, 16, 48)  0           ['batch_normalization_13[0][0]', \n",
      "                                                                  'dense_27[0][0]']               \n",
      "                                                                                                  \n",
      " full_gated_conv2d_12 (FullGate  (None, 256, 16, 48)  41568      ['multiply_15[0][0]']            \n",
      " dConv2D)                                                                                         \n",
      "                                                                                                  \n",
      " dropout_6 (Dropout)            (None, 256, 16, 48)  0           ['full_gated_conv2d_12[0][0]']   \n",
      "                                                                                                  \n",
      " conv2d_14 (Conv2D)             (None, 128, 4, 56)   24248       ['dropout_6[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_14 (BatchN  (None, 128, 4, 56)  392         ['conv2d_14[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " global_average_pooling2d_13 (G  (None, 56)          0           ['batch_normalization_14[0][0]'] \n",
      " lobalAveragePooling2D)                                                                           \n",
      "                                                                                                  \n",
      " reshape_14 (Reshape)           (None, 1, 1, 56)     0           ['global_average_pooling2d_13[0][\n",
      "                                                                 0]']                             \n",
      "                                                                                                  \n",
      " dense_28 (Dense)               (None, 1, 1, 28)     1568        ['reshape_14[0][0]']             \n",
      "                                                                                                  \n",
      " dense_29 (Dense)               (None, 1, 1, 56)     1568        ['dense_28[0][0]']               \n",
      "                                                                                                  \n",
      " multiply_16 (Multiply)         (None, 128, 4, 56)   0           ['batch_normalization_14[0][0]', \n",
      "                                                                  'dense_29[0][0]']               \n",
      "                                                                                                  \n",
      " full_gated_conv2d_13 (FullGate  (None, 128, 4, 56)  56560       ['multiply_16[0][0]']            \n",
      " dConv2D)                                                                                         \n",
      "                                                                                                  \n",
      " dropout_7 (Dropout)            (None, 128, 4, 56)   0           ['full_gated_conv2d_13[0][0]']   \n",
      "                                                                                                  \n",
      " conv2d_15 (Conv2D)             (None, 128, 4, 64)   32320       ['dropout_7[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_15 (BatchN  (None, 128, 4, 64)  448         ['conv2d_15[0][0]']              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " max_pooling2d_1 (MaxPooling2D)  (None, 128, 2, 64)  0           ['batch_normalization_15[0][0]'] \n",
      "                                                                                                  \n",
      " reshape_15 (Reshape)           (None, 128, 128)     0           ['max_pooling2d_1[0][0]']        \n",
      "                                                                                                  \n",
      " tnc1_initial_conv19 (Conv1D)   (None, 128, 120)     15480       ['reshape_15[0][0]']             \n",
      "                                                                                                  \n",
      " tnc120_dilated_conv_1_tanh_s0   (None, 128, 120)    28920       ['tnc1_initial_conv19[0][0]']    \n",
      " (Conv1D)                                                                                         \n",
      "                                                                                                  \n",
      " activation_7 (Activation)      (None, 128, 120)     0           ['tnc120_dilated_conv_1_tanh_s0[0\n",
      "                                                                 ][0]']                           \n",
      "                                                                                                  \n",
      " activation_8 (Activation)      (None, 128, 120)     0           ['tnc120_dilated_conv_1_tanh_s0[0\n",
      "                                                                 ][0]']                           \n",
      "                                                                                                  \n",
      " multiply_17 (Multiply)         (None, 128, 120)     0           ['activation_7[0][0]',           \n",
      "                                                                  'activation_8[0][0]']           \n",
      "                                                                                                  \n",
      " tnc146_spatial_dropout1d_1_s0_  (None, 128, 120)    0           ['multiply_17[0][0]']            \n",
      " 0.200000 (SpatialDropout1D)                                                                      \n",
      "                                                                                                  \n",
      " conv1d_3 (Conv1D)              (None, 128, 120)     14520       ['tnc146_spatial_dropout1d_1_s0_0\n",
      "                                                                 .200000[0][0]']                  \n",
      "                                                                                                  \n",
      " add_4 (Add)                    (None, 128, 120)     0           ['tnc1_initial_conv19[0][0]',    \n",
      "                                                                  'conv1d_3[0][0]']               \n",
      "                                                                                                  \n",
      " tnc183_dilated_conv_2_tanh_s0   (None, 128, 120)    28920       ['add_4[0][0]']                  \n",
      " (Conv1D)                                                                                         \n",
      "                                                                                                  \n",
      " activation_9 (Activation)      (None, 128, 120)     0           ['tnc183_dilated_conv_2_tanh_s0[0\n",
      "                                                                 ][0]']                           \n",
      "                                                                                                  \n",
      " activation_10 (Activation)     (None, 128, 120)     0           ['tnc183_dilated_conv_2_tanh_s0[0\n",
      "                                                                 ][0]']                           \n",
      "                                                                                                  \n",
      " multiply_18 (Multiply)         (None, 128, 120)     0           ['activation_9[0][0]',           \n",
      "                                                                  'activation_10[0][0]']          \n",
      "                                                                                                  \n",
      " tnc177_spatial_dropout1d_2_s0_  (None, 128, 120)    0           ['multiply_18[0][0]']            \n",
      " 0.200000 (SpatialDropout1D)                                                                      \n",
      "                                                                                                  \n",
      " conv1d_4 (Conv1D)              (None, 128, 120)     14520       ['tnc177_spatial_dropout1d_2_s0_0\n",
      "                                                                 .200000[0][0]']                  \n",
      "                                                                                                  \n",
      " add_5 (Add)                    (None, 128, 120)     0           ['add_4[0][0]',                  \n",
      "                                                                  'conv1d_4[0][0]']               \n",
      "                                                                                                  \n",
      " tnc196_dilated_conv_4_tanh_s0   (None, 128, 120)    28920       ['add_5[0][0]']                  \n",
      " (Conv1D)                                                                                         \n",
      "                                                                                                  \n",
      " activation_11 (Activation)     (None, 128, 120)     0           ['tnc196_dilated_conv_4_tanh_s0[0\n",
      "                                                                 ][0]']                           \n",
      "                                                                                                  \n",
      " activation_12 (Activation)     (None, 128, 120)     0           ['tnc196_dilated_conv_4_tanh_s0[0\n",
      "                                                                 ][0]']                           \n",
      "                                                                                                  \n",
      " multiply_19 (Multiply)         (None, 128, 120)     0           ['activation_11[0][0]',          \n",
      "                                                                  'activation_12[0][0]']          \n",
      "                                                                                                  \n",
      " tnc152_spatial_dropout1d_4_s0_  (None, 128, 120)    0           ['multiply_19[0][0]']            \n",
      " 0.200000 (SpatialDropout1D)                                                                      \n",
      "                                                                                                  \n",
      " conv1d_5 (Conv1D)              (None, 128, 120)     14520       ['tnc152_spatial_dropout1d_4_s0_0\n",
      "                                                                 .200000[0][0]']                  \n",
      "                                                                                                  \n",
      " add_7 (Add)                    (None, 128, 120)     0           ['conv1d_3[0][0]',               \n",
      "                                                                  'conv1d_4[0][0]',               \n",
      "                                                                  'conv1d_5[0][0]']               \n",
      "                                                                                                  \n",
      " activation_13 (Activation)     (None, 128, 120)     0           ['add_7[0][0]']                  \n",
      "                                                                                                  \n",
      " dense_30 (Dense)               (None, 128, 256)     30976       ['activation_13[0][0]']          \n",
      "                                                                                                  \n",
      " dense_31 (Dense)               (None, 128, 98)      25186       ['dense_30[0][0]']               \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 471,794\n",
      "Trainable params: 470,354\n",
      "Non-trainable params: 1,440\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-30 16:13:07.733869: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] layout failed: INVALID_ARGUMENT: Size of values 0 does not match size of permutation 4 @ fanin shape inmodel_1/dropout_4/dropout/SelectV2-2-TransposeNHWCToNCHW-LayoutOptimizer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "551/551 [==============================] - ETA: 0s - loss: 155.4431\n",
      "Epoch 1: val_loss improved from inf to 119.40949, saving model to ../output/bentham/shashankbest4senetgateddropouttwo/2023-09-30 16:12:56.237395/checkpoint_weights.hdf5\n",
      "551/551 [==============================] - 84s 138ms/step - loss: 155.4431 - val_loss: 119.4095 - lr: 0.0010\n",
      "Epoch 2/1000\n",
      "551/551 [==============================] - ETA: 0s - loss: 97.6677\n",
      "Epoch 2: val_loss improved from 119.40949 to 68.68932, saving model to ../output/bentham/shashankbest4senetgateddropouttwo/2023-09-30 16:12:56.237395/checkpoint_weights.hdf5\n",
      "551/551 [==============================] - 75s 136ms/step - loss: 97.6677 - val_loss: 68.6893 - lr: 0.0010\n",
      "Epoch 3/1000\n",
      "551/551 [==============================] - ETA: 0s - loss: 60.8677\n",
      "Epoch 3: val_loss improved from 68.68932 to 49.21790, saving model to ../output/bentham/shashankbest4senetgateddropouttwo/2023-09-30 16:12:56.237395/checkpoint_weights.hdf5\n",
      "551/551 [==============================] - 75s 136ms/step - loss: 60.8677 - val_loss: 49.2179 - lr: 0.0010\n",
      "Epoch 4/1000\n",
      "551/551 [==============================] - ETA: 0s - loss: 40.9534\n",
      "Epoch 4: val_loss improved from 49.21790 to 35.27914, saving model to ../output/bentham/shashankbest4senetgateddropouttwo/2023-09-30 16:12:56.237395/checkpoint_weights.hdf5\n",
      "551/551 [==============================] - 75s 136ms/step - loss: 40.9534 - val_loss: 35.2791 - lr: 0.0010\n",
      "Epoch 5/1000\n",
      "551/551 [==============================] - ETA: 0s - loss: 32.6319\n",
      "Epoch 5: val_loss improved from 35.27914 to 27.75629, saving model to ../output/bentham/shashankbest4senetgateddropouttwo/2023-09-30 16:12:56.237395/checkpoint_weights.hdf5\n",
      "551/551 [==============================] - 75s 136ms/step - loss: 32.6319 - val_loss: 27.7563 - lr: 0.0010\n",
      "Epoch 6/1000\n",
      "551/551 [==============================] - ETA: 0s - loss: 28.0196\n",
      "Epoch 6: val_loss improved from 27.75629 to 23.34867, saving model to ../output/bentham/shashankbest4senetgateddropouttwo/2023-09-30 16:12:56.237395/checkpoint_weights.hdf5\n",
      "551/551 [==============================] - 75s 136ms/step - loss: 28.0196 - val_loss: 23.3487 - lr: 0.0010\n",
      "Epoch 7/1000\n",
      "551/551 [==============================] - ETA: 0s - loss: 24.8480\n",
      "Epoch 7: val_loss improved from 23.34867 to 23.26037, saving model to ../output/bentham/shashankbest4senetgateddropouttwo/2023-09-30 16:12:56.237395/checkpoint_weights.hdf5\n",
      "551/551 [==============================] - 75s 136ms/step - loss: 24.8480 - val_loss: 23.2604 - lr: 0.0010\n",
      "Epoch 8/1000\n",
      "551/551 [==============================] - ETA: 0s - loss: 22.7492\n",
      "Epoch 8: val_loss improved from 23.26037 to 20.90506, saving model to ../output/bentham/shashankbest4senetgateddropouttwo/2023-09-30 16:12:56.237395/checkpoint_weights.hdf5\n",
      "551/551 [==============================] - 75s 136ms/step - loss: 22.7492 - val_loss: 20.9051 - lr: 0.0010\n",
      "Epoch 9/1000\n",
      "551/551 [==============================] - ETA: 0s - loss: 21.4261\n",
      "Epoch 9: val_loss improved from 20.90506 to 18.84689, saving model to ../output/bentham/shashankbest4senetgateddropouttwo/2023-09-30 16:12:56.237395/checkpoint_weights.hdf5\n",
      "551/551 [==============================] - 75s 136ms/step - loss: 21.4261 - val_loss: 18.8469 - lr: 0.0010\n",
      "Epoch 10/1000\n",
      "551/551 [==============================] - ETA: 0s - loss: 19.9562\n",
      "Epoch 10: val_loss improved from 18.84689 to 18.13958, saving model to ../output/bentham/shashankbest4senetgateddropouttwo/2023-09-30 16:12:56.237395/checkpoint_weights.hdf5\n",
      "551/551 [==============================] - 75s 136ms/step - loss: 19.9562 - val_loss: 18.1396 - lr: 0.0010\n",
      "Epoch 11/1000\n",
      "551/551 [==============================] - ETA: 0s - loss: 18.9815\n",
      "Epoch 11: val_loss improved from 18.13958 to 16.66065, saving model to ../output/bentham/shashankbest4senetgateddropouttwo/2023-09-30 16:12:56.237395/checkpoint_weights.hdf5\n",
      "551/551 [==============================] - 76s 138ms/step - loss: 18.9815 - val_loss: 16.6607 - lr: 0.0010\n",
      "Epoch 12/1000\n",
      "551/551 [==============================] - ETA: 0s - loss: 18.0099\n",
      "Epoch 12: val_loss improved from 16.66065 to 15.97782, saving model to ../output/bentham/shashankbest4senetgateddropouttwo/2023-09-30 16:12:56.237395/checkpoint_weights.hdf5\n",
      "551/551 [==============================] - 75s 136ms/step - loss: 18.0099 - val_loss: 15.9778 - lr: 0.0010\n",
      "Epoch 13/1000\n",
      "551/551 [==============================] - ETA: 0s - loss: 17.3590\n",
      "Epoch 13: val_loss improved from 15.97782 to 15.93523, saving model to ../output/bentham/shashankbest4senetgateddropouttwo/2023-09-30 16:12:56.237395/checkpoint_weights.hdf5\n",
      "551/551 [==============================] - 75s 136ms/step - loss: 17.3590 - val_loss: 15.9352 - lr: 0.0010\n",
      "Epoch 14/1000\n",
      "551/551 [==============================] - ETA: 0s - loss: 16.7346\n",
      "Epoch 14: val_loss improved from 15.93523 to 15.73375, saving model to ../output/bentham/shashankbest4senetgateddropouttwo/2023-09-30 16:12:56.237395/checkpoint_weights.hdf5\n",
      "551/551 [==============================] - 75s 136ms/step - loss: 16.7346 - val_loss: 15.7338 - lr: 0.0010\n",
      "Epoch 15/1000\n",
      "551/551 [==============================] - ETA: 0s - loss: 16.3901\n",
      "Epoch 15: val_loss improved from 15.73375 to 15.16806, saving model to ../output/bentham/shashankbest4senetgateddropouttwo/2023-09-30 16:12:56.237395/checkpoint_weights.hdf5\n",
      "551/551 [==============================] - 75s 136ms/step - loss: 16.3901 - val_loss: 15.1681 - lr: 0.0010\n",
      "Epoch 16/1000\n",
      "551/551 [==============================] - ETA: 0s - loss: 15.8434\n",
      "Epoch 16: val_loss improved from 15.16806 to 14.53918, saving model to ../output/bentham/shashankbest4senetgateddropouttwo/2023-09-30 16:12:56.237395/checkpoint_weights.hdf5\n",
      "551/551 [==============================] - 75s 137ms/step - loss: 15.8434 - val_loss: 14.5392 - lr: 0.0010\n",
      "Epoch 17/1000\n",
      "551/551 [==============================] - ETA: 0s - loss: 15.4604\n",
      "Epoch 17: val_loss improved from 14.53918 to 14.09528, saving model to ../output/bentham/shashankbest4senetgateddropouttwo/2023-09-30 16:12:56.237395/checkpoint_weights.hdf5\n",
      "551/551 [==============================] - 75s 136ms/step - loss: 15.4604 - val_loss: 14.0953 - lr: 0.0010\n",
      "Epoch 18/1000\n",
      "551/551 [==============================] - ETA: 0s - loss: 15.0043\n",
      "Epoch 18: val_loss improved from 14.09528 to 14.08355, saving model to ../output/bentham/shashankbest4senetgateddropouttwo/2023-09-30 16:12:56.237395/checkpoint_weights.hdf5\n",
      "551/551 [==============================] - 75s 136ms/step - loss: 15.0043 - val_loss: 14.0836 - lr: 0.0010\n",
      "Epoch 19/1000\n",
      "551/551 [==============================] - ETA: 0s - loss: 14.6709\n",
      "Epoch 19: val_loss did not improve from 14.08355\n",
      "551/551 [==============================] - 75s 136ms/step - loss: 14.6709 - val_loss: 14.1880 - lr: 0.0010\n",
      "Epoch 20/1000\n",
      "551/551 [==============================] - ETA: 0s - loss: 14.2411\n",
      "Epoch 20: val_loss improved from 14.08355 to 12.99187, saving model to ../output/bentham/shashankbest4senetgateddropouttwo/2023-09-30 16:12:56.237395/checkpoint_weights.hdf5\n",
      "551/551 [==============================] - 75s 136ms/step - loss: 14.2411 - val_loss: 12.9919 - lr: 0.0010\n",
      "Epoch 21/1000\n",
      "551/551 [==============================] - ETA: 0s - loss: 14.0549\n",
      "Epoch 21: val_loss did not improve from 12.99187\n",
      "551/551 [==============================] - 75s 137ms/step - loss: 14.0549 - val_loss: 13.1382 - lr: 0.0010\n",
      "Epoch 22/1000\n",
      "551/551 [==============================] - ETA: 0s - loss: 13.8301\n",
      "Epoch 22: val_loss did not improve from 12.99187\n",
      "551/551 [==============================] - 75s 136ms/step - loss: 13.8301 - val_loss: 13.5423 - lr: 0.0010\n",
      "Epoch 23/1000\n",
      "551/551 [==============================] - ETA: 0s - loss: 13.4894\n",
      "Epoch 23: val_loss improved from 12.99187 to 12.73205, saving model to ../output/bentham/shashankbest4senetgateddropouttwo/2023-09-30 16:12:56.237395/checkpoint_weights.hdf5\n",
      "551/551 [==============================] - 75s 136ms/step - loss: 13.4894 - val_loss: 12.7320 - lr: 0.0010\n",
      "Epoch 24/1000\n",
      "551/551 [==============================] - ETA: 0s - loss: 13.3251\n",
      "Epoch 24: val_loss improved from 12.73205 to 12.30616, saving model to ../output/bentham/shashankbest4senetgateddropouttwo/2023-09-30 16:12:56.237395/checkpoint_weights.hdf5\n",
      "551/551 [==============================] - 75s 136ms/step - loss: 13.3251 - val_loss: 12.3062 - lr: 0.0010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/1000\n",
      "551/551 [==============================] - ETA: 0s - loss: 13.1333\n",
      "Epoch 25: val_loss did not improve from 12.30616\n",
      "551/551 [==============================] - 75s 136ms/step - loss: 13.1333 - val_loss: 13.0603 - lr: 0.0010\n",
      "Epoch 26/1000\n",
      "551/551 [==============================] - ETA: 0s - loss: 12.9648\n",
      "Epoch 26: val_loss did not improve from 12.30616\n",
      "551/551 [==============================] - 75s 136ms/step - loss: 12.9648 - val_loss: 12.6503 - lr: 0.0010\n",
      "Epoch 27/1000\n",
      "551/551 [==============================] - ETA: 0s - loss: 12.6655\n",
      "Epoch 27: val_loss improved from 12.30616 to 12.24258, saving model to ../output/bentham/shashankbest4senetgateddropouttwo/2023-09-30 16:12:56.237395/checkpoint_weights.hdf5\n",
      "551/551 [==============================] - 75s 137ms/step - loss: 12.6655 - val_loss: 12.2426 - lr: 0.0010\n",
      "Epoch 28/1000\n",
      "551/551 [==============================] - ETA: 0s - loss: 12.5338\n",
      "Epoch 28: val_loss did not improve from 12.24258\n",
      "551/551 [==============================] - 75s 136ms/step - loss: 12.5338 - val_loss: 12.3103 - lr: 0.0010\n",
      "Epoch 29/1000\n",
      "551/551 [==============================] - ETA: 0s - loss: 12.5407\n",
      "Epoch 29: val_loss improved from 12.24258 to 11.82646, saving model to ../output/bentham/shashankbest4senetgateddropouttwo/2023-09-30 16:12:56.237395/checkpoint_weights.hdf5\n",
      "551/551 [==============================] - 75s 136ms/step - loss: 12.5407 - val_loss: 11.8265 - lr: 0.0010\n",
      "Epoch 30/1000\n",
      "551/551 [==============================] - ETA: 0s - loss: 12.3011\n",
      "Epoch 30: val_loss did not improve from 11.82646\n",
      "551/551 [==============================] - 75s 136ms/step - loss: 12.3011 - val_loss: 12.0552 - lr: 0.0010\n",
      "Epoch 31/1000\n",
      "551/551 [==============================] - ETA: 0s - loss: 12.1572\n",
      "Epoch 31: val_loss did not improve from 11.82646\n",
      "551/551 [==============================] - 76s 137ms/step - loss: 12.1572 - val_loss: 11.9633 - lr: 0.0010\n",
      "Epoch 32/1000\n",
      "551/551 [==============================] - ETA: 0s - loss: 12.0716\n",
      "Epoch 32: val_loss did not improve from 11.82646\n",
      "551/551 [==============================] - 75s 136ms/step - loss: 12.0716 - val_loss: 12.9935 - lr: 0.0010\n",
      "Epoch 33/1000\n",
      "551/551 [==============================] - ETA: 0s - loss: 12.1005\n",
      "Epoch 33: val_loss improved from 11.82646 to 11.50509, saving model to ../output/bentham/shashankbest4senetgateddropouttwo/2023-09-30 16:12:56.237395/checkpoint_weights.hdf5\n",
      "551/551 [==============================] - 75s 136ms/step - loss: 12.1005 - val_loss: 11.5051 - lr: 0.0010\n",
      "Epoch 34/1000\n",
      "551/551 [==============================] - ETA: 0s - loss: 11.7300\n",
      "Epoch 34: val_loss improved from 11.50509 to 11.44590, saving model to ../output/bentham/shashankbest4senetgateddropouttwo/2023-09-30 16:12:56.237395/checkpoint_weights.hdf5\n",
      "551/551 [==============================] - 75s 136ms/step - loss: 11.7300 - val_loss: 11.4459 - lr: 0.0010\n",
      "Epoch 35/1000\n",
      "551/551 [==============================] - ETA: 0s - loss: 11.4904\n",
      "Epoch 35: val_loss improved from 11.44590 to 11.20504, saving model to ../output/bentham/shashankbest4senetgateddropouttwo/2023-09-30 16:12:56.237395/checkpoint_weights.hdf5\n",
      "551/551 [==============================] - 75s 136ms/step - loss: 11.4904 - val_loss: 11.2050 - lr: 0.0010\n",
      "Epoch 36/1000\n",
      "551/551 [==============================] - ETA: 0s - loss: 11.4705\n",
      "Epoch 36: val_loss did not improve from 11.20504\n",
      "551/551 [==============================] - 75s 136ms/step - loss: 11.4705 - val_loss: 11.6304 - lr: 0.0010\n",
      "Epoch 37/1000\n",
      "551/551 [==============================] - ETA: 0s - loss: 11.4479\n",
      "Epoch 37: val_loss did not improve from 11.20504\n",
      "551/551 [==============================] - 75s 136ms/step - loss: 11.4479 - val_loss: 11.5442 - lr: 0.0010\n",
      "Epoch 38/1000\n",
      "551/551 [==============================] - ETA: 0s - loss: 11.2070\n",
      "Epoch 38: val_loss did not improve from 11.20504\n",
      "551/551 [==============================] - 75s 135ms/step - loss: 11.2070 - val_loss: 11.2236 - lr: 0.0010\n",
      "Epoch 39/1000\n",
      "551/551 [==============================] - ETA: 0s - loss: 11.1718\n",
      "Epoch 39: val_loss did not improve from 11.20504\n",
      "551/551 [==============================] - 75s 135ms/step - loss: 11.1718 - val_loss: 11.9353 - lr: 0.0010\n",
      "Epoch 40/1000\n",
      "551/551 [==============================] - ETA: 0s - loss: 11.0272\n",
      "Epoch 40: val_loss improved from 11.20504 to 11.07472, saving model to ../output/bentham/shashankbest4senetgateddropouttwo/2023-09-30 16:12:56.237395/checkpoint_weights.hdf5\n",
      "551/551 [==============================] - 75s 136ms/step - loss: 11.0272 - val_loss: 11.0747 - lr: 0.0010\n",
      "Epoch 41/1000\n",
      "551/551 [==============================] - ETA: 0s - loss: 10.8663\n",
      "Epoch 41: val_loss did not improve from 11.07472\n",
      "551/551 [==============================] - 75s 137ms/step - loss: 10.8663 - val_loss: 11.3355 - lr: 0.0010\n",
      "Epoch 42/1000\n",
      "551/551 [==============================] - ETA: 0s - loss: 10.9641\n",
      "Epoch 42: val_loss did not improve from 11.07472\n",
      "551/551 [==============================] - 75s 135ms/step - loss: 10.9641 - val_loss: 11.2038 - lr: 0.0010\n",
      "Epoch 43/1000\n",
      "551/551 [==============================] - ETA: 0s - loss: 10.8827\n",
      "Epoch 43: val_loss improved from 11.07472 to 10.86792, saving model to ../output/bentham/shashankbest4senetgateddropouttwo/2023-09-30 16:12:56.237395/checkpoint_weights.hdf5\n",
      "551/551 [==============================] - 75s 135ms/step - loss: 10.8827 - val_loss: 10.8679 - lr: 0.0010\n",
      "Epoch 44/1000\n",
      "551/551 [==============================] - ETA: 0s - loss: 10.8352\n",
      "Epoch 44: val_loss did not improve from 10.86792\n",
      "551/551 [==============================] - 75s 135ms/step - loss: 10.8352 - val_loss: 11.0550 - lr: 0.0010\n",
      "Epoch 45/1000\n",
      "551/551 [==============================] - ETA: 0s - loss: 10.5852\n",
      "Epoch 45: val_loss did not improve from 10.86792\n",
      "551/551 [==============================] - 75s 136ms/step - loss: 10.5852 - val_loss: 11.1500 - lr: 0.0010\n",
      "Epoch 46/1000\n",
      "551/551 [==============================] - ETA: 0s - loss: 10.5332\n",
      "Epoch 46: val_loss did not improve from 10.86792\n",
      "551/551 [==============================] - 74s 135ms/step - loss: 10.5332 - val_loss: 11.2615 - lr: 0.0010\n",
      "Epoch 47/1000\n",
      "551/551 [==============================] - ETA: 0s - loss: 10.5491\n",
      "Epoch 47: val_loss improved from 10.86792 to 10.75355, saving model to ../output/bentham/shashankbest4senetgateddropouttwo/2023-09-30 16:12:56.237395/checkpoint_weights.hdf5\n",
      "551/551 [==============================] - 75s 136ms/step - loss: 10.5491 - val_loss: 10.7535 - lr: 0.0010\n",
      "Epoch 48/1000\n",
      "551/551 [==============================] - ETA: 0s - loss: 10.4404\n",
      "Epoch 48: val_loss improved from 10.75355 to 10.66174, saving model to ../output/bentham/shashankbest4senetgateddropouttwo/2023-09-30 16:12:56.237395/checkpoint_weights.hdf5\n",
      "551/551 [==============================] - 75s 136ms/step - loss: 10.4404 - val_loss: 10.6617 - lr: 0.0010\n",
      "Epoch 49/1000\n",
      "551/551 [==============================] - ETA: 0s - loss: 10.3181\n",
      "Epoch 49: val_loss improved from 10.66174 to 10.64745, saving model to ../output/bentham/shashankbest4senetgateddropouttwo/2023-09-30 16:12:56.237395/checkpoint_weights.hdf5\n",
      "551/551 [==============================] - 75s 136ms/step - loss: 10.3181 - val_loss: 10.6475 - lr: 0.0010\n",
      "Epoch 50/1000\n",
      "551/551 [==============================] - ETA: 0s - loss: 10.3213\n",
      "Epoch 50: val_loss did not improve from 10.64745\n",
      "551/551 [==============================] - 75s 136ms/step - loss: 10.3213 - val_loss: 10.8953 - lr: 0.0010\n",
      "Epoch 51/1000\n",
      "551/551 [==============================] - ETA: 0s - loss: 10.1842\n",
      "Epoch 51: val_loss did not improve from 10.64745\n",
      "551/551 [==============================] - 75s 137ms/step - loss: 10.1842 - val_loss: 10.7597 - lr: 0.0010\n",
      "Epoch 52/1000\n",
      "551/551 [==============================] - ETA: 0s - loss: 10.3014\n",
      "Epoch 52: val_loss improved from 10.64745 to 10.51738, saving model to ../output/bentham/shashankbest4senetgateddropouttwo/2023-09-30 16:12:56.237395/checkpoint_weights.hdf5\n",
      "551/551 [==============================] - 75s 136ms/step - loss: 10.3014 - val_loss: 10.5174 - lr: 0.0010\n",
      "Epoch 53/1000\n",
      "551/551 [==============================] - ETA: 0s - loss: 10.1684\n",
      "Epoch 53: val_loss did not improve from 10.51738\n",
      "551/551 [==============================] - 75s 136ms/step - loss: 10.1684 - val_loss: 10.6405 - lr: 0.0010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 54/1000\n",
      "551/551 [==============================] - ETA: 0s - loss: 10.0356\n",
      "Epoch 54: val_loss did not improve from 10.51738\n",
      "551/551 [==============================] - 75s 137ms/step - loss: 10.0356 - val_loss: 10.6880 - lr: 0.0010\n",
      "Epoch 55/1000\n",
      "551/551 [==============================] - ETA: 0s - loss: 10.0470\n",
      "Epoch 55: val_loss improved from 10.51738 to 10.50739, saving model to ../output/bentham/shashankbest4senetgateddropouttwo/2023-09-30 16:12:56.237395/checkpoint_weights.hdf5\n",
      "551/551 [==============================] - 75s 136ms/step - loss: 10.0470 - val_loss: 10.5074 - lr: 0.0010\n",
      "Epoch 56/1000\n",
      "551/551 [==============================] - ETA: 0s - loss: 9.9538\n",
      "Epoch 56: val_loss did not improve from 10.50739\n",
      "551/551 [==============================] - 75s 136ms/step - loss: 9.9538 - val_loss: 10.6523 - lr: 0.0010\n",
      "Epoch 57/1000\n",
      "551/551 [==============================] - ETA: 0s - loss: 9.8232\n",
      "Epoch 57: val_loss did not improve from 10.50739\n",
      "551/551 [==============================] - 75s 136ms/step - loss: 9.8232 - val_loss: 10.6418 - lr: 0.0010\n",
      "Epoch 58/1000\n",
      "551/551 [==============================] - ETA: 0s - loss: 9.8475\n",
      "Epoch 58: val_loss did not improve from 10.50739\n",
      "551/551 [==============================] - 75s 136ms/step - loss: 9.8475 - val_loss: 10.5519 - lr: 0.0010\n",
      "Epoch 59/1000\n",
      "551/551 [==============================] - ETA: 0s - loss: 9.7599\n",
      "Epoch 59: val_loss improved from 10.50739 to 10.33301, saving model to ../output/bentham/shashankbest4senetgateddropouttwo/2023-09-30 16:12:56.237395/checkpoint_weights.hdf5\n",
      "551/551 [==============================] - 75s 136ms/step - loss: 9.7599 - val_loss: 10.3330 - lr: 0.0010\n",
      "Epoch 60/1000\n",
      "551/551 [==============================] - ETA: 0s - loss: 9.8201\n",
      "Epoch 60: val_loss did not improve from 10.33301\n",
      "551/551 [==============================] - 75s 136ms/step - loss: 9.8201 - val_loss: 10.6512 - lr: 0.0010\n",
      "Epoch 61/1000\n",
      "551/551 [==============================] - ETA: 0s - loss: 9.5952\n",
      "Epoch 61: val_loss improved from 10.33301 to 9.98342, saving model to ../output/bentham/shashankbest4senetgateddropouttwo/2023-09-30 16:12:56.237395/checkpoint_weights.hdf5\n",
      "551/551 [==============================] - 75s 137ms/step - loss: 9.5952 - val_loss: 9.9834 - lr: 0.0010\n",
      "Epoch 62/1000\n",
      "551/551 [==============================] - ETA: 0s - loss: 9.6494\n",
      "Epoch 62: val_loss did not improve from 9.98342\n",
      "551/551 [==============================] - 75s 136ms/step - loss: 9.6494 - val_loss: 10.6020 - lr: 0.0010\n",
      "Epoch 63/1000\n",
      "551/551 [==============================] - ETA: 0s - loss: 9.6110\n",
      "Epoch 63: val_loss did not improve from 9.98342\n",
      "551/551 [==============================] - 75s 136ms/step - loss: 9.6110 - val_loss: 10.4524 - lr: 0.0010\n",
      "Epoch 64/1000\n",
      "551/551 [==============================] - ETA: 0s - loss: 9.5487\n",
      "Epoch 64: val_loss did not improve from 9.98342\n",
      "551/551 [==============================] - 75s 136ms/step - loss: 9.5487 - val_loss: 10.7324 - lr: 0.0010\n",
      "Epoch 65/1000\n",
      "551/551 [==============================] - ETA: 0s - loss: 9.5748\n",
      "Epoch 65: val_loss did not improve from 9.98342\n",
      "551/551 [==============================] - 75s 136ms/step - loss: 9.5748 - val_loss: 11.2920 - lr: 0.0010\n",
      "Epoch 66/1000\n",
      "551/551 [==============================] - ETA: 0s - loss: 9.5331\n",
      "Epoch 66: val_loss did not improve from 9.98342\n",
      "551/551 [==============================] - 75s 136ms/step - loss: 9.5331 - val_loss: 10.6598 - lr: 0.0010\n",
      "Epoch 67/1000\n",
      "551/551 [==============================] - ETA: 0s - loss: 9.4464\n",
      "Epoch 67: val_loss did not improve from 9.98342\n",
      "551/551 [==============================] - 75s 136ms/step - loss: 9.4464 - val_loss: 10.2137 - lr: 0.0010\n",
      "Epoch 68/1000\n",
      "551/551 [==============================] - ETA: 0s - loss: 9.3831\n",
      "Epoch 68: val_loss did not improve from 9.98342\n",
      "551/551 [==============================] - 75s 136ms/step - loss: 9.3831 - val_loss: 10.3756 - lr: 0.0010\n",
      "Epoch 69/1000\n",
      "551/551 [==============================] - ETA: 0s - loss: 9.3305\n",
      "Epoch 69: val_loss did not improve from 9.98342\n",
      "551/551 [==============================] - 75s 135ms/step - loss: 9.3305 - val_loss: 10.3688 - lr: 0.0010\n",
      "Epoch 70/1000\n",
      "551/551 [==============================] - ETA: 0s - loss: 9.3251\n",
      "Epoch 70: val_loss did not improve from 9.98342\n",
      "551/551 [==============================] - 74s 135ms/step - loss: 9.3251 - val_loss: 10.0234 - lr: 0.0010\n",
      "Epoch 71/1000\n",
      "551/551 [==============================] - ETA: 0s - loss: 9.2658\n",
      "Epoch 71: val_loss did not improve from 9.98342\n",
      "551/551 [==============================] - 75s 136ms/step - loss: 9.2658 - val_loss: 10.1498 - lr: 0.0010\n",
      "Epoch 72/1000\n",
      "551/551 [==============================] - ETA: 0s - loss: 9.2316\n",
      "Epoch 72: val_loss did not improve from 9.98342\n",
      "551/551 [==============================] - 75s 136ms/step - loss: 9.2316 - val_loss: 10.2256 - lr: 0.0010\n",
      "Epoch 73/1000\n",
      "551/551 [==============================] - ETA: 0s - loss: 9.2772\n",
      "Epoch 73: val_loss did not improve from 9.98342\n",
      "551/551 [==============================] - 75s 135ms/step - loss: 9.2772 - val_loss: 10.1938 - lr: 0.0010\n",
      "Epoch 74/1000\n",
      "551/551 [==============================] - ETA: 0s - loss: 9.2729\n",
      "Epoch 74: val_loss did not improve from 9.98342\n",
      "551/551 [==============================] - 75s 135ms/step - loss: 9.2729 - val_loss: 10.2188 - lr: 0.0010\n",
      "Epoch 75/1000\n",
      "551/551 [==============================] - ETA: 0s - loss: 9.0683\n",
      "Epoch 75: val_loss did not improve from 9.98342\n",
      "551/551 [==============================] - 75s 136ms/step - loss: 9.0683 - val_loss: 10.3821 - lr: 0.0010\n",
      "Epoch 76/1000\n",
      "551/551 [==============================] - ETA: 0s - loss: 9.1826\n",
      "Epoch 76: val_loss improved from 9.98342 to 9.77761, saving model to ../output/bentham/shashankbest4senetgateddropouttwo/2023-09-30 16:12:56.237395/checkpoint_weights.hdf5\n",
      "551/551 [==============================] - 75s 135ms/step - loss: 9.1826 - val_loss: 9.7776 - lr: 0.0010\n",
      "Epoch 77/1000\n",
      "551/551 [==============================] - ETA: 0s - loss: 9.0554\n",
      "Epoch 77: val_loss did not improve from 9.77761\n",
      "551/551 [==============================] - 74s 135ms/step - loss: 9.0554 - val_loss: 9.8842 - lr: 0.0010\n",
      "Epoch 78/1000\n",
      "551/551 [==============================] - ETA: 0s - loss: 9.0645\n",
      "Epoch 78: val_loss did not improve from 9.77761\n",
      "551/551 [==============================] - 75s 135ms/step - loss: 9.0645 - val_loss: 10.1846 - lr: 0.0010\n",
      "Epoch 79/1000\n",
      "551/551 [==============================] - ETA: 0s - loss: 9.0287\n",
      "Epoch 79: val_loss did not improve from 9.77761\n",
      "551/551 [==============================] - 75s 136ms/step - loss: 9.0287 - val_loss: 9.9165 - lr: 0.0010\n",
      "Epoch 80/1000\n",
      "551/551 [==============================] - ETA: 0s - loss: 8.9590\n",
      "Epoch 80: val_loss did not improve from 9.77761\n",
      "551/551 [==============================] - 75s 136ms/step - loss: 8.9590 - val_loss: 10.2070 - lr: 0.0010\n",
      "Epoch 81/1000\n",
      "551/551 [==============================] - ETA: 0s - loss: 9.0336\n",
      "Epoch 81: val_loss did not improve from 9.77761\n",
      "551/551 [==============================] - 75s 136ms/step - loss: 9.0336 - val_loss: 9.9961 - lr: 0.0010\n",
      "Epoch 82/1000\n",
      "551/551 [==============================] - ETA: 0s - loss: 8.9480\n",
      "Epoch 82: val_loss did not improve from 9.77761\n",
      "551/551 [==============================] - 74s 135ms/step - loss: 8.9480 - val_loss: 10.2367 - lr: 0.0010\n",
      "Epoch 83/1000\n",
      "551/551 [==============================] - ETA: 0s - loss: 8.8942\n",
      "Epoch 83: val_loss did not improve from 9.77761\n",
      "551/551 [==============================] - 75s 135ms/step - loss: 8.8942 - val_loss: 10.0518 - lr: 0.0010\n",
      "Epoch 84/1000\n",
      "551/551 [==============================] - ETA: 0s - loss: 8.8321\n",
      "Epoch 84: val_loss improved from 9.77761 to 9.75946, saving model to ../output/bentham/shashankbest4senetgateddropouttwo/2023-09-30 16:12:56.237395/checkpoint_weights.hdf5\n",
      "551/551 [==============================] - 75s 136ms/step - loss: 8.8321 - val_loss: 9.7595 - lr: 0.0010\n",
      "Epoch 85/1000\n",
      "551/551 [==============================] - ETA: 0s - loss: 8.9754\n",
      "Epoch 85: val_loss did not improve from 9.75946\n",
      "551/551 [==============================] - 75s 136ms/step - loss: 8.9754 - val_loss: 10.2052 - lr: 0.0010\n",
      "Epoch 86/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "551/551 [==============================] - ETA: 0s - loss: 8.8301\n",
      "Epoch 86: val_loss did not improve from 9.75946\n",
      "551/551 [==============================] - 75s 136ms/step - loss: 8.8301 - val_loss: 9.8582 - lr: 0.0010\n",
      "Epoch 87/1000\n",
      "551/551 [==============================] - ETA: 0s - loss: 8.8199\n",
      "Epoch 87: val_loss did not improve from 9.75946\n",
      "551/551 [==============================] - 75s 136ms/step - loss: 8.8199 - val_loss: 10.5246 - lr: 0.0010\n",
      "Epoch 88/1000\n",
      "551/551 [==============================] - ETA: 0s - loss: 8.8474\n",
      "Epoch 88: val_loss improved from 9.75946 to 9.57750, saving model to ../output/bentham/shashankbest4senetgateddropouttwo/2023-09-30 16:12:56.237395/checkpoint_weights.hdf5\n",
      "551/551 [==============================] - 75s 136ms/step - loss: 8.8474 - val_loss: 9.5775 - lr: 0.0010\n",
      "Epoch 89/1000\n",
      "551/551 [==============================] - ETA: 0s - loss: 8.6745\n",
      "Epoch 89: val_loss did not improve from 9.57750\n",
      "551/551 [==============================] - 75s 135ms/step - loss: 8.6745 - val_loss: 9.9709 - lr: 0.0010\n",
      "Epoch 90/1000\n",
      "551/551 [==============================] - ETA: 0s - loss: 9.0074\n",
      "Epoch 90: val_loss did not improve from 9.57750\n",
      "551/551 [==============================] - 75s 135ms/step - loss: 9.0074 - val_loss: 10.0020 - lr: 0.0010\n",
      "Epoch 91/1000\n",
      "551/551 [==============================] - ETA: 0s - loss: 8.6847\n",
      "Epoch 91: val_loss did not improve from 9.57750\n",
      "551/551 [==============================] - 75s 136ms/step - loss: 8.6847 - val_loss: 9.9791 - lr: 0.0010\n",
      "Epoch 92/1000\n",
      "551/551 [==============================] - ETA: 0s - loss: 8.6700\n",
      "Epoch 92: val_loss did not improve from 9.57750\n",
      "551/551 [==============================] - 75s 136ms/step - loss: 8.6700 - val_loss: 10.0766 - lr: 0.0010\n",
      "Epoch 93/1000\n",
      "551/551 [==============================] - ETA: 0s - loss: 8.6252\n",
      "Epoch 93: val_loss did not improve from 9.57750\n",
      "551/551 [==============================] - 75s 136ms/step - loss: 8.6252 - val_loss: 10.0054 - lr: 0.0010\n",
      "Epoch 94/1000\n",
      "551/551 [==============================] - ETA: 0s - loss: 8.6483\n",
      "Epoch 94: val_loss did not improve from 9.57750\n",
      "551/551 [==============================] - 75s 136ms/step - loss: 8.6483 - val_loss: 10.1955 - lr: 0.0010\n",
      "Epoch 95/1000\n",
      "551/551 [==============================] - ETA: 0s - loss: 8.5511\n",
      "Epoch 95: val_loss did not improve from 9.57750\n",
      "551/551 [==============================] - 75s 136ms/step - loss: 8.5511 - val_loss: 9.8289 - lr: 0.0010\n",
      "Epoch 96/1000\n",
      "551/551 [==============================] - ETA: 0s - loss: 8.4523\n",
      "Epoch 96: val_loss did not improve from 9.57750\n",
      "551/551 [==============================] - 75s 136ms/step - loss: 8.4523 - val_loss: 10.0541 - lr: 0.0010\n",
      "Epoch 97/1000\n",
      "551/551 [==============================] - ETA: 0s - loss: 8.6518\n",
      "Epoch 97: val_loss did not improve from 9.57750\n",
      "551/551 [==============================] - 75s 136ms/step - loss: 8.6518 - val_loss: 9.9690 - lr: 0.0010\n",
      "Epoch 98/1000\n",
      "551/551 [==============================] - ETA: 0s - loss: 8.5661\n",
      "Epoch 98: val_loss did not improve from 9.57750\n",
      "551/551 [==============================] - 75s 136ms/step - loss: 8.5661 - val_loss: 10.1171 - lr: 0.0010\n",
      "Epoch 99/1000\n",
      "551/551 [==============================] - ETA: 0s - loss: 8.4240\n",
      "Epoch 99: val_loss did not improve from 9.57750\n",
      "551/551 [==============================] - 75s 136ms/step - loss: 8.4240 - val_loss: 9.8355 - lr: 0.0010\n",
      "Epoch 100/1000\n",
      "551/551 [==============================] - ETA: 0s - loss: 8.6028\n",
      "Epoch 100: val_loss did not improve from 9.57750\n",
      "551/551 [==============================] - 75s 136ms/step - loss: 8.6028 - val_loss: 10.1946 - lr: 0.0010\n",
      "Epoch 101/1000\n",
      "551/551 [==============================] - ETA: 0s - loss: 8.4697\n",
      "Epoch 101: val_loss did not improve from 9.57750\n",
      "551/551 [==============================] - 76s 137ms/step - loss: 8.4697 - val_loss: 10.2273 - lr: 0.0010\n",
      "Epoch 102/1000\n",
      "551/551 [==============================] - ETA: 0s - loss: 8.4880\n",
      "Epoch 102: val_loss did not improve from 9.57750\n",
      "551/551 [==============================] - 75s 136ms/step - loss: 8.4880 - val_loss: 9.9177 - lr: 0.0010\n",
      "Epoch 103/1000\n",
      "551/551 [==============================] - ETA: 0s - loss: 8.5075\n",
      "Epoch 103: val_loss did not improve from 9.57750\n",
      "\n",
      "Epoch 103: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "551/551 [==============================] - 75s 136ms/step - loss: 8.5075 - val_loss: 9.9398 - lr: 0.0010\n",
      "Epoch 104/1000\n",
      "551/551 [==============================] - ETA: 0s - loss: 7.2581\n",
      "Epoch 104: val_loss improved from 9.57750 to 8.95370, saving model to ../output/bentham/shashankbest4senetgateddropouttwo/2023-09-30 16:12:56.237395/checkpoint_weights.hdf5\n",
      "551/551 [==============================] - 75s 137ms/step - loss: 7.2581 - val_loss: 8.9537 - lr: 2.0000e-04\n",
      "Epoch 105/1000\n",
      "551/551 [==============================] - ETA: 0s - loss: 6.7844\n",
      "Epoch 105: val_loss improved from 8.95370 to 8.85782, saving model to ../output/bentham/shashankbest4senetgateddropouttwo/2023-09-30 16:12:56.237395/checkpoint_weights.hdf5\n",
      "551/551 [==============================] - 75s 136ms/step - loss: 6.7844 - val_loss: 8.8578 - lr: 2.0000e-04\n",
      "Epoch 106/1000\n",
      "551/551 [==============================] - ETA: 0s - loss: 6.7094\n",
      "Epoch 106: val_loss did not improve from 8.85782\n",
      "551/551 [==============================] - 75s 136ms/step - loss: 6.7094 - val_loss: 8.9192 - lr: 2.0000e-04\n",
      "Epoch 107/1000\n",
      "551/551 [==============================] - ETA: 0s - loss: 6.5851\n",
      "Epoch 107: val_loss improved from 8.85782 to 8.84414, saving model to ../output/bentham/shashankbest4senetgateddropouttwo/2023-09-30 16:12:56.237395/checkpoint_weights.hdf5\n",
      "551/551 [==============================] - 75s 137ms/step - loss: 6.5851 - val_loss: 8.8441 - lr: 2.0000e-04\n",
      "Epoch 108/1000\n",
      "551/551 [==============================] - ETA: 0s - loss: 6.5356\n",
      "Epoch 108: val_loss improved from 8.84414 to 8.83218, saving model to ../output/bentham/shashankbest4senetgateddropouttwo/2023-09-30 16:12:56.237395/checkpoint_weights.hdf5\n",
      "551/551 [==============================] - 75s 136ms/step - loss: 6.5356 - val_loss: 8.8322 - lr: 2.0000e-04\n",
      "Epoch 109/1000\n",
      "551/551 [==============================] - ETA: 0s - loss: 6.4578\n",
      "Epoch 109: val_loss improved from 8.83218 to 8.70563, saving model to ../output/bentham/shashankbest4senetgateddropouttwo/2023-09-30 16:12:56.237395/checkpoint_weights.hdf5\n",
      "551/551 [==============================] - 75s 136ms/step - loss: 6.4578 - val_loss: 8.7056 - lr: 2.0000e-04\n",
      "Epoch 110/1000\n",
      "551/551 [==============================] - ETA: 0s - loss: 6.5222\n",
      "Epoch 110: val_loss did not improve from 8.70563\n",
      "551/551 [==============================] - 75s 136ms/step - loss: 6.5222 - val_loss: 8.7483 - lr: 2.0000e-04\n",
      "Epoch 111/1000\n",
      "551/551 [==============================] - ETA: 0s - loss: 6.4704\n",
      "Epoch 111: val_loss did not improve from 8.70563\n",
      "551/551 [==============================] - 76s 137ms/step - loss: 6.4704 - val_loss: 8.7887 - lr: 2.0000e-04\n",
      "Epoch 112/1000\n",
      "551/551 [==============================] - ETA: 0s - loss: 6.4120\n",
      "Epoch 112: val_loss did not improve from 8.70563\n",
      "551/551 [==============================] - 75s 136ms/step - loss: 6.4120 - val_loss: 8.8223 - lr: 2.0000e-04\n",
      "Epoch 113/1000\n",
      "551/551 [==============================] - ETA: 0s - loss: 6.5549\n",
      "Epoch 113: val_loss did not improve from 8.70563\n",
      "551/551 [==============================] - 75s 136ms/step - loss: 6.5549 - val_loss: 8.7319 - lr: 2.0000e-04\n",
      "Epoch 114/1000\n",
      "551/551 [==============================] - ETA: 0s - loss: 6.3650\n",
      "Epoch 114: val_loss did not improve from 8.70563\n",
      "551/551 [==============================] - 75s 136ms/step - loss: 6.3650 - val_loss: 8.7169 - lr: 2.0000e-04\n",
      "Epoch 115/1000\n",
      "551/551 [==============================] - ETA: 0s - loss: 6.2846\n",
      "Epoch 115: val_loss did not improve from 8.70563\n",
      "551/551 [==============================] - 75s 136ms/step - loss: 6.2846 - val_loss: 8.7898 - lr: 2.0000e-04\n",
      "Epoch 116/1000\n",
      "551/551 [==============================] - ETA: 0s - loss: 6.3713\n",
      "Epoch 116: val_loss did not improve from 8.70563\n",
      "551/551 [==============================] - 75s 136ms/step - loss: 6.3713 - val_loss: 8.7587 - lr: 2.0000e-04\n",
      "Epoch 117/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "551/551 [==============================] - ETA: 0s - loss: 6.3241\n",
      "Epoch 117: val_loss did not improve from 8.70563\n",
      "551/551 [==============================] - 75s 136ms/step - loss: 6.3241 - val_loss: 8.7975 - lr: 2.0000e-04\n",
      "Epoch 118/1000\n",
      "551/551 [==============================] - ETA: 0s - loss: 6.1954\n",
      "Epoch 118: val_loss did not improve from 8.70563\n",
      "551/551 [==============================] - 75s 136ms/step - loss: 6.1954 - val_loss: 8.8343 - lr: 2.0000e-04\n",
      "Epoch 119/1000\n",
      "551/551 [==============================] - ETA: 0s - loss: 6.2429\n",
      "Epoch 119: val_loss did not improve from 8.70563\n",
      "551/551 [==============================] - 75s 137ms/step - loss: 6.2429 - val_loss: 8.8167 - lr: 2.0000e-04\n",
      "Epoch 120/1000\n",
      "551/551 [==============================] - ETA: 0s - loss: 6.1791\n",
      "Epoch 120: val_loss did not improve from 8.70563\n",
      "551/551 [==============================] - 75s 136ms/step - loss: 6.1791 - val_loss: 8.7620 - lr: 2.0000e-04\n",
      "Epoch 121/1000\n",
      "551/551 [==============================] - ETA: 0s - loss: 6.1334\n",
      "Epoch 121: val_loss did not improve from 8.70563\n",
      "551/551 [==============================] - 76s 137ms/step - loss: 6.1334 - val_loss: 8.7928 - lr: 2.0000e-04\n",
      "Epoch 122/1000\n",
      "551/551 [==============================] - ETA: 0s - loss: 5.9539\n",
      "Epoch 122: val_loss did not improve from 8.70563\n",
      "551/551 [==============================] - 75s 136ms/step - loss: 5.9539 - val_loss: 8.8537 - lr: 2.0000e-04\n",
      "Epoch 123/1000\n",
      "551/551 [==============================] - ETA: 0s - loss: 6.1361\n",
      "Epoch 123: val_loss did not improve from 8.70563\n",
      "551/551 [==============================] - 75s 136ms/step - loss: 6.1361 - val_loss: 8.8043 - lr: 2.0000e-04\n",
      "Epoch 124/1000\n",
      "551/551 [==============================] - ETA: 0s - loss: 6.1055\n",
      "Epoch 124: val_loss did not improve from 8.70563\n",
      "\n",
      "Epoch 124: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
      "551/551 [==============================] - 75s 136ms/step - loss: 6.1055 - val_loss: 8.7953 - lr: 2.0000e-04\n",
      "Epoch 125/1000\n",
      "551/551 [==============================] - ETA: 0s - loss: 5.8879\n",
      "Epoch 125: val_loss improved from 8.70563 to 8.69067, saving model to ../output/bentham/shashankbest4senetgateddropouttwo/2023-09-30 16:12:56.237395/checkpoint_weights.hdf5\n",
      "551/551 [==============================] - 75s 136ms/step - loss: 5.8879 - val_loss: 8.6907 - lr: 4.0000e-05\n",
      "Epoch 126/1000\n",
      "551/551 [==============================] - ETA: 0s - loss: 5.7749\n",
      "Epoch 126: val_loss did not improve from 8.69067\n",
      "551/551 [==============================] - 75s 136ms/step - loss: 5.7749 - val_loss: 8.7145 - lr: 4.0000e-05\n",
      "Epoch 127/1000\n",
      "551/551 [==============================] - ETA: 0s - loss: 5.7631\n",
      "Epoch 127: val_loss did not improve from 8.69067\n",
      "551/551 [==============================] - 75s 136ms/step - loss: 5.7631 - val_loss: 8.7605 - lr: 4.0000e-05\n",
      "Epoch 128/1000\n",
      "551/551 [==============================] - ETA: 0s - loss: 5.8338\n",
      "Epoch 128: val_loss did not improve from 8.69067\n",
      "551/551 [==============================] - 75s 136ms/step - loss: 5.8338 - val_loss: 8.7571 - lr: 4.0000e-05\n",
      "Epoch 129/1000\n",
      "551/551 [==============================] - ETA: 0s - loss: 5.8126\n",
      "Epoch 129: val_loss did not improve from 8.69067\n",
      "551/551 [==============================] - 75s 136ms/step - loss: 5.8126 - val_loss: 8.6927 - lr: 4.0000e-05\n",
      "Epoch 130/1000\n",
      "551/551 [==============================] - ETA: 0s - loss: 5.8073\n",
      "Epoch 130: val_loss did not improve from 8.69067\n",
      "551/551 [==============================] - 75s 136ms/step - loss: 5.8073 - val_loss: 8.7394 - lr: 4.0000e-05\n",
      "Epoch 131/1000\n",
      "551/551 [==============================] - ETA: 0s - loss: 5.7706\n",
      "Epoch 131: val_loss did not improve from 8.69067\n",
      "551/551 [==============================] - 75s 137ms/step - loss: 5.7706 - val_loss: 8.7148 - lr: 4.0000e-05\n",
      "Epoch 132/1000\n",
      "551/551 [==============================] - ETA: 0s - loss: 5.6357\n",
      "Epoch 132: val_loss did not improve from 8.69067\n",
      "551/551 [==============================] - 75s 136ms/step - loss: 5.6357 - val_loss: 8.6988 - lr: 4.0000e-05\n",
      "Epoch 133/1000\n",
      "551/551 [==============================] - ETA: 0s - loss: 5.7014\n",
      "Epoch 133: val_loss did not improve from 8.69067\n",
      "551/551 [==============================] - 75s 136ms/step - loss: 5.7014 - val_loss: 8.6909 - lr: 4.0000e-05\n",
      "Epoch 134/1000\n",
      "551/551 [==============================] - ETA: 0s - loss: 5.8444\n",
      "Epoch 134: val_loss improved from 8.69067 to 8.67379, saving model to ../output/bentham/shashankbest4senetgateddropouttwo/2023-09-30 16:12:56.237395/checkpoint_weights.hdf5\n",
      "551/551 [==============================] - 75s 137ms/step - loss: 5.8444 - val_loss: 8.6738 - lr: 4.0000e-05\n",
      "Epoch 135/1000\n",
      "551/551 [==============================] - ETA: 0s - loss: 5.6565\n",
      "Epoch 135: val_loss improved from 8.67379 to 8.66260, saving model to ../output/bentham/shashankbest4senetgateddropouttwo/2023-09-30 16:12:56.237395/checkpoint_weights.hdf5\n",
      "551/551 [==============================] - 75s 137ms/step - loss: 5.6565 - val_loss: 8.6626 - lr: 4.0000e-05\n",
      "Epoch 136/1000\n",
      "551/551 [==============================] - ETA: 0s - loss: 5.7332\n",
      "Epoch 136: val_loss did not improve from 8.66260\n",
      "551/551 [==============================] - 75s 137ms/step - loss: 5.7332 - val_loss: 8.6727 - lr: 4.0000e-05\n",
      "Epoch 137/1000\n",
      "551/551 [==============================] - ETA: 0s - loss: 5.7165\n",
      "Epoch 137: val_loss did not improve from 8.66260\n",
      "551/551 [==============================] - 75s 136ms/step - loss: 5.7165 - val_loss: 8.7100 - lr: 4.0000e-05\n",
      "Epoch 138/1000\n",
      "551/551 [==============================] - ETA: 0s - loss: 5.7409\n",
      "Epoch 138: val_loss did not improve from 8.66260\n",
      "551/551 [==============================] - 75s 137ms/step - loss: 5.7409 - val_loss: 8.6965 - lr: 4.0000e-05\n",
      "Epoch 139/1000\n",
      "551/551 [==============================] - ETA: 0s - loss: 5.6786\n",
      "Epoch 139: val_loss did not improve from 8.66260\n",
      "551/551 [==============================] - 75s 136ms/step - loss: 5.6786 - val_loss: 8.6938 - lr: 4.0000e-05\n",
      "Epoch 140/1000\n",
      "551/551 [==============================] - ETA: 0s - loss: 5.6248\n",
      "Epoch 140: val_loss did not improve from 8.66260\n",
      "551/551 [==============================] - 75s 136ms/step - loss: 5.6248 - val_loss: 8.7326 - lr: 4.0000e-05\n",
      "Epoch 141/1000\n",
      "551/551 [==============================] - ETA: 0s - loss: 5.6221\n",
      "Epoch 141: val_loss did not improve from 8.66260\n",
      "551/551 [==============================] - 76s 137ms/step - loss: 5.6221 - val_loss: 8.7242 - lr: 4.0000e-05\n",
      "Epoch 142/1000\n",
      "551/551 [==============================] - ETA: 0s - loss: 5.7684\n",
      "Epoch 142: val_loss did not improve from 8.66260\n",
      "551/551 [==============================] - 75s 136ms/step - loss: 5.7684 - val_loss: 8.7105 - lr: 4.0000e-05\n",
      "Epoch 143/1000\n",
      "551/551 [==============================] - ETA: 0s - loss: 5.6922\n",
      "Epoch 143: val_loss did not improve from 8.66260\n",
      "551/551 [==============================] - 75s 136ms/step - loss: 5.6922 - val_loss: 8.6723 - lr: 4.0000e-05\n",
      "Epoch 144/1000\n",
      "551/551 [==============================] - ETA: 0s - loss: 5.7529\n",
      "Epoch 144: val_loss did not improve from 8.66260\n",
      "551/551 [==============================] - 75s 136ms/step - loss: 5.7529 - val_loss: 8.7243 - lr: 4.0000e-05\n",
      "Epoch 145/1000\n",
      "551/551 [==============================] - ETA: 0s - loss: 5.6942\n",
      "Epoch 145: val_loss did not improve from 8.66260\n",
      "551/551 [==============================] - 75s 136ms/step - loss: 5.6942 - val_loss: 8.7048 - lr: 4.0000e-05\n",
      "Epoch 146/1000\n",
      "551/551 [==============================] - ETA: 0s - loss: 5.6299\n",
      "Epoch 146: val_loss did not improve from 8.66260\n",
      "551/551 [==============================] - 75s 136ms/step - loss: 5.6299 - val_loss: 8.7357 - lr: 4.0000e-05\n",
      "Epoch 147/1000\n",
      "551/551 [==============================] - ETA: 0s - loss: 5.6824\n",
      "Epoch 147: val_loss did not improve from 8.66260\n",
      "551/551 [==============================] - 76s 137ms/step - loss: 5.6824 - val_loss: 8.7300 - lr: 4.0000e-05\n",
      "Epoch 148/1000\n",
      "551/551 [==============================] - ETA: 0s - loss: 5.6323\n",
      "Epoch 148: val_loss did not improve from 8.66260\n",
      "551/551 [==============================] - 75s 136ms/step - loss: 5.6323 - val_loss: 8.6746 - lr: 4.0000e-05\n",
      "Epoch 149/1000\n",
      "551/551 [==============================] - ETA: 0s - loss: 5.7292\n",
      "Epoch 149: val_loss did not improve from 8.66260\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "551/551 [==============================] - 75s 136ms/step - loss: 5.7292 - val_loss: 8.7193 - lr: 4.0000e-05\n",
      "Epoch 150/1000\n",
      "551/551 [==============================] - ETA: 0s - loss: 5.5706\n",
      "Epoch 150: val_loss did not improve from 8.66260\n",
      "\n",
      "Epoch 150: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.\n",
      "551/551 [==============================] - 75s 136ms/step - loss: 5.5706 - val_loss: 8.6766 - lr: 4.0000e-05\n",
      "Epoch 151/1000\n",
      "551/551 [==============================] - ETA: 0s - loss: 5.6083\n",
      "Epoch 151: val_loss did not improve from 8.66260\n",
      "551/551 [==============================] - 76s 137ms/step - loss: 5.6083 - val_loss: 8.6864 - lr: 8.0000e-06\n",
      "Epoch 152/1000\n",
      "551/551 [==============================] - ETA: 0s - loss: 5.5554\n",
      "Epoch 152: val_loss did not improve from 8.66260\n",
      "551/551 [==============================] - 75s 136ms/step - loss: 5.5554 - val_loss: 8.6922 - lr: 8.0000e-06\n",
      "Epoch 153/1000\n",
      "551/551 [==============================] - ETA: 0s - loss: 5.8400\n",
      "Epoch 153: val_loss did not improve from 8.66260\n",
      "551/551 [==============================] - 75s 136ms/step - loss: 5.8400 - val_loss: 8.7244 - lr: 8.0000e-06\n",
      "Epoch 154/1000\n",
      "551/551 [==============================] - ETA: 0s - loss: 5.5895\n",
      "Epoch 154: val_loss improved from 8.66260 to 8.63931, saving model to ../output/bentham/shashankbest4senetgateddropouttwo/2023-09-30 16:12:56.237395/checkpoint_weights.hdf5\n",
      "551/551 [==============================] - 75s 136ms/step - loss: 5.5895 - val_loss: 8.6393 - lr: 8.0000e-06\n",
      "Epoch 155/1000\n",
      "551/551 [==============================] - ETA: 0s - loss: 5.5295\n",
      "Epoch 155: val_loss did not improve from 8.63931\n",
      "551/551 [==============================] - 75s 136ms/step - loss: 5.5295 - val_loss: 8.7038 - lr: 8.0000e-06\n",
      "Epoch 156/1000\n",
      "551/551 [==============================] - ETA: 0s - loss: 5.5888\n",
      "Epoch 156: val_loss did not improve from 8.63931\n",
      "551/551 [==============================] - 75s 136ms/step - loss: 5.5888 - val_loss: 8.7126 - lr: 8.0000e-06\n",
      "Epoch 157/1000\n",
      "551/551 [==============================] - ETA: 0s - loss: 5.6058\n",
      "Epoch 157: val_loss did not improve from 8.63931\n",
      "551/551 [==============================] - 75s 136ms/step - loss: 5.6058 - val_loss: 8.6681 - lr: 8.0000e-06\n",
      "Epoch 158/1000\n",
      "551/551 [==============================] - ETA: 0s - loss: 5.5711\n",
      "Epoch 158: val_loss did not improve from 8.63931\n",
      "551/551 [==============================] - 75s 136ms/step - loss: 5.5711 - val_loss: 8.7114 - lr: 8.0000e-06\n",
      "Epoch 159/1000\n",
      "551/551 [==============================] - ETA: 0s - loss: 5.5770\n",
      "Epoch 159: val_loss did not improve from 8.63931\n",
      "551/551 [==============================] - 75s 136ms/step - loss: 5.5770 - val_loss: 8.6873 - lr: 8.0000e-06\n",
      "Epoch 160/1000\n",
      "436/551 [======================>.......] - ETA: 15s - loss: 5.6711"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    exec(open('IAM1.py').read())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
